// ignore_for_file: always_specify_types
// ignore_for_file: camel_case_types
// ignore_for_file: non_constant_identifier_names
// ignore_for_file: unused_import

// AUTO GENERATED FILE, DO NOT EDIT.
//
// Generated by `package:ffigen`.
// ignore_for_file: type=lint
@ffi.DefaultAsset('package:lcpp_ngin/src/bindings.dart')
library;

import 'dart:ffi' as ffi;

@ffi.Native<ffi.Void Function(ffi.Pointer<va_list>)>()
external void __va_start(
  ffi.Pointer<va_list> arg0,
);

@ffi.Native<ffi.Void Function()>()
external void __security_init_cookie();

@ffi.Native<ffi.Void Function(ffi.UintPtr)>()
external void __security_check_cookie(
  int _StackCookie,
);

@ffi.Native<ffi.Void Function(ffi.UintPtr)>()
external void __report_gsfailure(
  int _StackCookie,
);

@ffi.Native<ffi.UintPtr>()
external int __security_cookie;

@ffi.Native<ffi.Void Function()>()
external void _invalid_parameter_noinfo();

@ffi.Native<ffi.Void Function()>()
external void _invalid_parameter_noinfo_noreturn();

@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<ffi.WChar>,
    ffi.Pointer<ffi.WChar>,
    ffi.Pointer<ffi.WChar>,
    ffi.UnsignedInt,
    ffi.UintPtr,
  )
>()
external void _invoke_watson(
  ffi.Pointer<ffi.WChar> _Expression,
  ffi.Pointer<ffi.WChar> _FunctionName,
  ffi.Pointer<ffi.WChar> _FileName,
  int _LineNo,
  int _Reserved,
);

@ffi.Native<ffi.Pointer<ffi.Int> Function()>()
external ffi.Pointer<ffi.Int> _errno();

@ffi.Native<errno_t Function(ffi.Int)>()
external int _set_errno(
  int _Value,
);

@ffi.Native<errno_t Function(ffi.Pointer<ffi.Int>)>()
external int _get_errno(
  ffi.Pointer<ffi.Int> _Value,
);

@ffi.Native<ffi.UnsignedLong Function()>()
external int __threadid();

@ffi.Native<ffi.UintPtr Function()>()
external int __threadhandle();

@ffi.Native<ffi.Pointer<FILE> Function(ffi.UnsignedInt)>()
external ffi.Pointer<FILE> __acrt_iob_func(
  int _Ix,
);

@ffi.Native<wint_t Function(ffi.Pointer<FILE>)>()
external int fgetwc(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<wint_t Function()>()
external int _fgetwchar();

@ffi.Native<wint_t Function(ffi.WChar, ffi.Pointer<FILE>)>()
external int fputwc(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<wint_t Function(ffi.WChar)>()
external int _fputwchar(
  int _Character,
);

@ffi.Native<wint_t Function(ffi.Pointer<FILE>)>()
external int getwc(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<wint_t Function()>()
external int getwchar();

@ffi.Native<
  ffi.Pointer<ffi.WChar> Function(
    ffi.Pointer<ffi.WChar>,
    ffi.Int,
    ffi.Pointer<FILE>,
  )
>()
external ffi.Pointer<ffi.WChar> fgetws(
  ffi.Pointer<ffi.WChar> _Buffer,
  int _BufferCount,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ffi.WChar>, ffi.Pointer<FILE>)>()
external int fputws(
  ffi.Pointer<ffi.WChar> _Buffer,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Pointer<ffi.WChar> Function(ffi.Pointer<ffi.WChar>, ffi.Size)>()
external ffi.Pointer<ffi.WChar> _getws_s(
  ffi.Pointer<ffi.WChar> _Buffer,
  int _BufferCount,
);

@ffi.Native<wint_t Function(ffi.WChar, ffi.Pointer<FILE>)>()
external int putwc(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<wint_t Function(ffi.WChar)>()
external int putwchar(
  int _Character,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ffi.WChar>)>()
external int _putws(
  ffi.Pointer<ffi.WChar> _Buffer,
);

@ffi.Native<wint_t Function(wint_t, ffi.Pointer<FILE>)>()
external int ungetwc(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Pointer<FILE> Function(ffi.Int, ffi.Pointer<ffi.WChar>)>()
external ffi.Pointer<FILE> _wfdopen(
  int _FileHandle,
  ffi.Pointer<ffi.WChar> _Mode,
);

@ffi.Native<
  ffi.Pointer<FILE> Function(ffi.Pointer<ffi.WChar>, ffi.Pointer<ffi.WChar>)
>()
external ffi.Pointer<FILE> _wfopen(
  ffi.Pointer<ffi.WChar> _FileName,
  ffi.Pointer<ffi.WChar> _Mode,
);

@ffi.Native<
  errno_t Function(
    ffi.Pointer<ffi.Pointer<FILE>>,
    ffi.Pointer<ffi.WChar>,
    ffi.Pointer<ffi.WChar>,
  )
>()
external int _wfopen_s(
  ffi.Pointer<ffi.Pointer<FILE>> _Stream,
  ffi.Pointer<ffi.WChar> _FileName,
  ffi.Pointer<ffi.WChar> _Mode,
);

@ffi.Native<
  ffi.Pointer<FILE> Function(
    ffi.Pointer<ffi.WChar>,
    ffi.Pointer<ffi.WChar>,
    ffi.Pointer<FILE>,
  )
>()
external ffi.Pointer<FILE> _wfreopen(
  ffi.Pointer<ffi.WChar> _FileName,
  ffi.Pointer<ffi.WChar> _Mode,
  ffi.Pointer<FILE> _OldStream,
);

@ffi.Native<
  errno_t Function(
    ffi.Pointer<ffi.Pointer<FILE>>,
    ffi.Pointer<ffi.WChar>,
    ffi.Pointer<ffi.WChar>,
    ffi.Pointer<FILE>,
  )
>()
external int _wfreopen_s(
  ffi.Pointer<ffi.Pointer<FILE>> _Stream,
  ffi.Pointer<ffi.WChar> _FileName,
  ffi.Pointer<ffi.WChar> _Mode,
  ffi.Pointer<FILE> _OldStream,
);

@ffi.Native<
  ffi.Pointer<FILE> Function(
    ffi.Pointer<ffi.WChar>,
    ffi.Pointer<ffi.WChar>,
    ffi.Int,
  )
>()
external ffi.Pointer<FILE> _wfsopen(
  ffi.Pointer<ffi.WChar> _FileName,
  ffi.Pointer<ffi.WChar> _Mode,
  int _ShFlag,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ffi.WChar>)>()
external void _wperror(
  ffi.Pointer<ffi.WChar> _ErrorMessage,
);

@ffi.Native<
  ffi.Pointer<FILE> Function(ffi.Pointer<ffi.WChar>, ffi.Pointer<ffi.WChar>)
>()
external ffi.Pointer<FILE> _wpopen(
  ffi.Pointer<ffi.WChar> _Command,
  ffi.Pointer<ffi.WChar> _Mode,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ffi.WChar>)>()
external int _wremove(
  ffi.Pointer<ffi.WChar> _FileName,
);

@ffi.Native<
  ffi.Pointer<ffi.WChar> Function(
    ffi.Pointer<ffi.WChar>,
    ffi.Pointer<ffi.WChar>,
  )
>()
external ffi.Pointer<ffi.WChar> _wtempnam(
  ffi.Pointer<ffi.WChar> _Directory,
  ffi.Pointer<ffi.WChar> _FilePrefix,
);

@ffi.Native<errno_t Function(ffi.Pointer<ffi.WChar>, ffi.Size)>()
external int _wtmpnam_s(
  ffi.Pointer<ffi.WChar> _Buffer,
  int _BufferCount,
);

@ffi.Native<ffi.Pointer<ffi.WChar> Function(ffi.Pointer<ffi.WChar>)>()
external ffi.Pointer<ffi.WChar> _wtmpnam(
  ffi.Pointer<ffi.WChar> _Buffer,
);

@ffi.Native<wint_t Function(ffi.Pointer<FILE>)>()
external int _fgetwc_nolock(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<wint_t Function(ffi.WChar, ffi.Pointer<FILE>)>()
external int _fputwc_nolock(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<wint_t Function(ffi.Pointer<FILE>)>()
external int _getwc_nolock(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<wint_t Function(ffi.WChar, ffi.Pointer<FILE>)>()
external int _putwc_nolock(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<wint_t Function(wint_t, ffi.Pointer<FILE>)>()
external int _ungetwc_nolock(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
  ffi.Int Function(
    ffi.UnsignedLongLong,
    ffi.Pointer<FILE>,
    ffi.Pointer<ffi.WChar>,
    _locale_t,
    va_list,
  )
>()
external int __stdio_common_vfwprintf(
  int _Options,
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.WChar> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
  ffi.Int Function(
    ffi.UnsignedLongLong,
    ffi.Pointer<FILE>,
    ffi.Pointer<ffi.WChar>,
    _locale_t,
    va_list,
  )
>()
external int __stdio_common_vfwprintf_s(
  int _Options,
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.WChar> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
  ffi.Int Function(
    ffi.UnsignedLongLong,
    ffi.Pointer<FILE>,
    ffi.Pointer<ffi.WChar>,
    _locale_t,
    va_list,
  )
>()
external int __stdio_common_vfwprintf_p(
  int _Options,
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.WChar> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
  ffi.Int Function(
    ffi.UnsignedLongLong,
    ffi.Pointer<FILE>,
    ffi.Pointer<ffi.WChar>,
    _locale_t,
    va_list,
  )
>()
external int __stdio_common_vfwscanf(
  int _Options,
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.WChar> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
  ffi.Int Function(
    ffi.UnsignedLongLong,
    ffi.Pointer<ffi.WChar>,
    ffi.Size,
    ffi.Pointer<ffi.WChar>,
    _locale_t,
    va_list,
  )
>()
external int __stdio_common_vswprintf(
  int _Options,
  ffi.Pointer<ffi.WChar> _Buffer,
  int _BufferCount,
  ffi.Pointer<ffi.WChar> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
  ffi.Int Function(
    ffi.UnsignedLongLong,
    ffi.Pointer<ffi.WChar>,
    ffi.Size,
    ffi.Pointer<ffi.WChar>,
    _locale_t,
    va_list,
  )
>()
external int __stdio_common_vswprintf_s(
  int _Options,
  ffi.Pointer<ffi.WChar> _Buffer,
  int _BufferCount,
  ffi.Pointer<ffi.WChar> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
  ffi.Int Function(
    ffi.UnsignedLongLong,
    ffi.Pointer<ffi.WChar>,
    ffi.Size,
    ffi.Size,
    ffi.Pointer<ffi.WChar>,
    _locale_t,
    va_list,
  )
>()
external int __stdio_common_vsnwprintf_s(
  int _Options,
  ffi.Pointer<ffi.WChar> _Buffer,
  int _BufferCount,
  int _MaxCount,
  ffi.Pointer<ffi.WChar> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
  ffi.Int Function(
    ffi.UnsignedLongLong,
    ffi.Pointer<ffi.WChar>,
    ffi.Size,
    ffi.Pointer<ffi.WChar>,
    _locale_t,
    va_list,
  )
>()
external int __stdio_common_vswprintf_p(
  int _Options,
  ffi.Pointer<ffi.WChar> _Buffer,
  int _BufferCount,
  ffi.Pointer<ffi.WChar> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
  ffi.Int Function(
    ffi.UnsignedLongLong,
    ffi.Pointer<ffi.WChar>,
    ffi.Size,
    ffi.Pointer<ffi.WChar>,
    _locale_t,
    va_list,
  )
>()
external int __stdio_common_vswscanf(
  int _Options,
  ffi.Pointer<ffi.WChar> _Buffer,
  int _BufferCount,
  ffi.Pointer<ffi.WChar> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
  errno_t Function(
    ffi.Pointer<FILE>,
    ffi.Pointer<ffi.Pointer<ffi.Pointer<ffi.Char>>>,
    ffi.Pointer<ffi.Pointer<ffi.Pointer<ffi.Char>>>,
    ffi.Pointer<ffi.Pointer<ffi.Int>>,
  )
>()
external int _get_stream_buffer_pointers(
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.Pointer<ffi.Pointer<ffi.Char>>> _Base,
  ffi.Pointer<ffi.Pointer<ffi.Pointer<ffi.Char>>> _Pointer,
  ffi.Pointer<ffi.Pointer<ffi.Int>> _Count,
);

@ffi.Native<errno_t Function(ffi.Pointer<FILE>)>()
external int clearerr_s(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
  errno_t Function(
    ffi.Pointer<ffi.Pointer<FILE>>,
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<ffi.Char>,
  )
>()
external int fopen_s(
  ffi.Pointer<ffi.Pointer<FILE>> _Stream,
  ffi.Pointer<ffi.Char> _FileName,
  ffi.Pointer<ffi.Char> _Mode,
);

@ffi.Native<
  ffi.Size Function(
    ffi.Pointer<ffi.Void>,
    ffi.Size,
    ffi.Size,
    ffi.Size,
    ffi.Pointer<FILE>,
  )
>()
external int fread_s(
  ffi.Pointer<ffi.Void> _Buffer,
  int _BufferSize,
  int _ElementSize,
  int _ElementCount,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
  errno_t Function(
    ffi.Pointer<ffi.Pointer<FILE>>,
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<FILE>,
  )
>()
external int freopen_s(
  ffi.Pointer<ffi.Pointer<FILE>> _Stream,
  ffi.Pointer<ffi.Char> _FileName,
  ffi.Pointer<ffi.Char> _Mode,
  ffi.Pointer<FILE> _OldStream,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.Pointer<ffi.Char>, rsize_t)>()
external ffi.Pointer<ffi.Char> gets_s(
  ffi.Pointer<ffi.Char> _Buffer,
  int _Size,
);

@ffi.Native<errno_t Function(ffi.Pointer<ffi.Pointer<FILE>>)>()
external int tmpfile_s(
  ffi.Pointer<ffi.Pointer<FILE>> _Stream,
);

@ffi.Native<errno_t Function(ffi.Pointer<ffi.Char>, rsize_t)>()
external int tmpnam_s(
  ffi.Pointer<ffi.Char> _Buffer,
  int _Size,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<FILE>)>()
external void clearerr(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int fclose(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function()>()
external int _fcloseall();

@ffi.Native<ffi.Pointer<FILE> Function(ffi.Int, ffi.Pointer<ffi.Char>)>()
external ffi.Pointer<FILE> _fdopen(
  int _FileHandle,
  ffi.Pointer<ffi.Char> _Mode,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int feof(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int ferror(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int fflush(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int fgetc(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function()>()
external int _fgetchar();

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>, ffi.Pointer<fpos_t>)>()
external int fgetpos(
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<fpos_t> _Position,
);

@ffi.Native<
  ffi.Pointer<ffi.Char> Function(
    ffi.Pointer<ffi.Char>,
    ffi.Int,
    ffi.Pointer<FILE>,
  )
>()
external ffi.Pointer<ffi.Char> fgets(
  ffi.Pointer<ffi.Char> _Buffer,
  int _MaxCount,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int _fileno(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function()>()
external int _flushall();

@ffi.Native<
  ffi.Pointer<FILE> Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)
>()
external ffi.Pointer<FILE> fopen(
  ffi.Pointer<ffi.Char> _FileName,
  ffi.Pointer<ffi.Char> _Mode,
);

@ffi.Native<ffi.Int Function(ffi.Int, ffi.Pointer<FILE>)>()
external int fputc(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Int)>()
external int _fputchar(
  int _Character,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ffi.Char>, ffi.Pointer<FILE>)>()
external int fputs(
  ffi.Pointer<ffi.Char> _Buffer,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
  ffi.UnsignedLongLong Function(
    ffi.Pointer<ffi.Void>,
    ffi.Size,
    ffi.Size,
    ffi.Pointer<FILE>,
  )
>()
external int fread(
  ffi.Pointer<ffi.Void> _Buffer,
  int _ElementSize,
  int _ElementCount,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
  ffi.Pointer<FILE> Function(
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<FILE>,
  )
>()
external ffi.Pointer<FILE> freopen(
  ffi.Pointer<ffi.Char> _FileName,
  ffi.Pointer<ffi.Char> _Mode,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
  ffi.Pointer<FILE> Function(
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<ffi.Char>,
    ffi.Int,
  )
>()
external ffi.Pointer<FILE> _fsopen(
  ffi.Pointer<ffi.Char> _FileName,
  ffi.Pointer<ffi.Char> _Mode,
  int _ShFlag,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>, ffi.Pointer<fpos_t>)>()
external int fsetpos(
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<fpos_t> _Position,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>, ffi.Long, ffi.Int)>()
external int fseek(
  ffi.Pointer<FILE> _Stream,
  int _Offset,
  int _Origin,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>, ffi.LongLong, ffi.Int)>()
external int _fseeki64(
  ffi.Pointer<FILE> _Stream,
  int _Offset,
  int _Origin,
);

@ffi.Native<ffi.Long Function(ffi.Pointer<FILE>)>()
external int ftell(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.LongLong Function(ffi.Pointer<FILE>)>()
external int _ftelli64(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
  ffi.UnsignedLongLong Function(
    ffi.Pointer<ffi.Void>,
    ffi.Size,
    ffi.Size,
    ffi.Pointer<FILE>,
  )
>()
external int fwrite(
  ffi.Pointer<ffi.Void> _Buffer,
  int _ElementSize,
  int _ElementCount,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int getc(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function()>()
external int getchar();

@ffi.Native<ffi.Int Function()>()
external int _getmaxstdio();

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int _getw(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ffi.Char>)>()
external void perror(
  ffi.Pointer<ffi.Char> _ErrorMessage,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int _pclose(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
  ffi.Pointer<FILE> Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)
>()
external ffi.Pointer<FILE> _popen(
  ffi.Pointer<ffi.Char> _Command,
  ffi.Pointer<ffi.Char> _Mode,
);

@ffi.Native<ffi.Int Function(ffi.Int, ffi.Pointer<FILE>)>()
external int putc(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Int)>()
external int putchar(
  int _Character,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ffi.Char>)>()
external int puts(
  ffi.Pointer<ffi.Char> _Buffer,
);

@ffi.Native<ffi.Int Function(ffi.Int, ffi.Pointer<FILE>)>()
external int _putw(
  int _Word,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ffi.Char>)>()
external int remove(
  ffi.Pointer<ffi.Char> _FileName,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)>()
external int rename(
  ffi.Pointer<ffi.Char> _OldFileName,
  ffi.Pointer<ffi.Char> _NewFileName,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ffi.Char>)>()
external int _unlink(
  ffi.Pointer<ffi.Char> _FileName,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ffi.Char>)>()
external int unlink(
  ffi.Pointer<ffi.Char> _FileName,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<FILE>)>()
external void rewind(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function()>()
external int _rmtmp();

@ffi.Native<ffi.Void Function(ffi.Pointer<FILE>, ffi.Pointer<ffi.Char>)>()
external void setbuf(
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.Char> _Buffer,
);

@ffi.Native<ffi.Int Function(ffi.Int)>()
external int _setmaxstdio(
  int _Maximum,
);

@ffi.Native<
  ffi.Int Function(ffi.Pointer<FILE>, ffi.Pointer<ffi.Char>, ffi.Int, ffi.Size)
>()
external int setvbuf(
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.Char> _Buffer,
  int _Mode,
  int _Size,
);

@ffi.Native<
  ffi.Pointer<ffi.Char> Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)
>()
external ffi.Pointer<ffi.Char> _tempnam(
  ffi.Pointer<ffi.Char> _DirectoryName,
  ffi.Pointer<ffi.Char> _FilePrefix,
);

@ffi.Native<ffi.Pointer<FILE> Function()>()
external ffi.Pointer<FILE> tmpfile();

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.Pointer<ffi.Char>)>()
external ffi.Pointer<ffi.Char> tmpnam(
  ffi.Pointer<ffi.Char> _Buffer,
);

@ffi.Native<ffi.Int Function(ffi.Int, ffi.Pointer<FILE>)>()
external int ungetc(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<FILE>)>()
external void _lock_file(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<FILE>)>()
external void _unlock_file(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int _fclose_nolock(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int _fflush_nolock(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int _fgetc_nolock(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Int, ffi.Pointer<FILE>)>()
external int _fputc_nolock(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
  ffi.Size Function(
    ffi.Pointer<ffi.Void>,
    ffi.Size,
    ffi.Size,
    ffi.Pointer<FILE>,
  )
>()
external int _fread_nolock(
  ffi.Pointer<ffi.Void> _Buffer,
  int _ElementSize,
  int _ElementCount,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
  ffi.Size Function(
    ffi.Pointer<ffi.Void>,
    ffi.Size,
    ffi.Size,
    ffi.Size,
    ffi.Pointer<FILE>,
  )
>()
external int _fread_nolock_s(
  ffi.Pointer<ffi.Void> _Buffer,
  int _BufferSize,
  int _ElementSize,
  int _ElementCount,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>, ffi.Long, ffi.Int)>()
external int _fseek_nolock(
  ffi.Pointer<FILE> _Stream,
  int _Offset,
  int _Origin,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>, ffi.LongLong, ffi.Int)>()
external int _fseeki64_nolock(
  ffi.Pointer<FILE> _Stream,
  int _Offset,
  int _Origin,
);

@ffi.Native<ffi.Long Function(ffi.Pointer<FILE>)>()
external int _ftell_nolock(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.LongLong Function(ffi.Pointer<FILE>)>()
external int _ftelli64_nolock(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<
  ffi.Size Function(
    ffi.Pointer<ffi.Void>,
    ffi.Size,
    ffi.Size,
    ffi.Pointer<FILE>,
  )
>()
external int _fwrite_nolock(
  ffi.Pointer<ffi.Void> _Buffer,
  int _ElementSize,
  int _ElementCount,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int _getc_nolock(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Int, ffi.Pointer<FILE>)>()
external int _putc_nolock(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Int, ffi.Pointer<FILE>)>()
external int _ungetc_nolock(
  int _Character,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Pointer<ffi.Int> Function()>()
external ffi.Pointer<ffi.Int> __p__commode();

@ffi.Native<
  ffi.Int Function(
    ffi.UnsignedLongLong,
    ffi.Pointer<FILE>,
    ffi.Pointer<ffi.Char>,
    _locale_t,
    va_list,
  )
>()
external int __stdio_common_vfprintf(
  int _Options,
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.Char> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
  ffi.Int Function(
    ffi.UnsignedLongLong,
    ffi.Pointer<FILE>,
    ffi.Pointer<ffi.Char>,
    _locale_t,
    va_list,
  )
>()
external int __stdio_common_vfprintf_s(
  int _Options,
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.Char> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
  ffi.Int Function(
    ffi.UnsignedLongLong,
    ffi.Pointer<FILE>,
    ffi.Pointer<ffi.Char>,
    _locale_t,
    va_list,
  )
>()
external int __stdio_common_vfprintf_p(
  int _Options,
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.Char> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<ffi.Int Function(ffi.Int)>()
external int _set_printf_count_output(
  int _Value,
);

@ffi.Native<ffi.Int Function()>()
external int _get_printf_count_output();

@ffi.Native<
  ffi.Int Function(
    ffi.UnsignedLongLong,
    ffi.Pointer<FILE>,
    ffi.Pointer<ffi.Char>,
    _locale_t,
    va_list,
  )
>()
external int __stdio_common_vfscanf(
  int _Options,
  ffi.Pointer<FILE> _Stream,
  ffi.Pointer<ffi.Char> _Format,
  _locale_t _Locale,
  va_list _Arglist,
);

@ffi.Native<
  ffi.Int Function(
    ffi.UnsignedLongLong,
    ffi.Pointer<ffi.Char>,
    ffi.Size,
    ffi.Pointer<ffi.Char>,
    _locale_t,
    va_list,
  )
>()
external int __stdio_common_vsprintf(
  int _Options,
  ffi.Pointer<ffi.Char> _Buffer,
  int _BufferCount,
  ffi.Pointer<ffi.Char> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
  ffi.Int Function(
    ffi.UnsignedLongLong,
    ffi.Pointer<ffi.Char>,
    ffi.Size,
    ffi.Pointer<ffi.Char>,
    _locale_t,
    va_list,
  )
>()
external int __stdio_common_vsprintf_s(
  int _Options,
  ffi.Pointer<ffi.Char> _Buffer,
  int _BufferCount,
  ffi.Pointer<ffi.Char> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
  ffi.Int Function(
    ffi.UnsignedLongLong,
    ffi.Pointer<ffi.Char>,
    ffi.Size,
    ffi.Size,
    ffi.Pointer<ffi.Char>,
    _locale_t,
    va_list,
  )
>()
external int __stdio_common_vsnprintf_s(
  int _Options,
  ffi.Pointer<ffi.Char> _Buffer,
  int _BufferCount,
  int _MaxCount,
  ffi.Pointer<ffi.Char> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
  ffi.Int Function(
    ffi.UnsignedLongLong,
    ffi.Pointer<ffi.Char>,
    ffi.Size,
    ffi.Pointer<ffi.Char>,
    _locale_t,
    va_list,
  )
>()
external int __stdio_common_vsprintf_p(
  int _Options,
  ffi.Pointer<ffi.Char> _Buffer,
  int _BufferCount,
  ffi.Pointer<ffi.Char> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
  ffi.Int Function(
    ffi.UnsignedLongLong,
    ffi.Pointer<ffi.Char>,
    ffi.Size,
    ffi.Pointer<ffi.Char>,
    _locale_t,
    va_list,
  )
>()
external int __stdio_common_vsscanf(
  int _Options,
  ffi.Pointer<ffi.Char> _Buffer,
  int _BufferCount,
  ffi.Pointer<ffi.Char> _Format,
  _locale_t _Locale,
  va_list _ArgList,
);

@ffi.Native<
  ffi.Pointer<ffi.Char> Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)
>()
external ffi.Pointer<ffi.Char> tempnam(
  ffi.Pointer<ffi.Char> _Directory,
  ffi.Pointer<ffi.Char> _FilePrefix,
);

@ffi.Native<ffi.Int Function()>()
external int fcloseall();

@ffi.Native<ffi.Pointer<FILE> Function(ffi.Int, ffi.Pointer<ffi.Char>)>()
external ffi.Pointer<FILE> fdopen(
  int _FileHandle,
  ffi.Pointer<ffi.Char> _Format,
);

@ffi.Native<ffi.Int Function()>()
external int fgetchar();

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int fileno(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function()>()
external int flushall();

@ffi.Native<ffi.Int Function(ffi.Int)>()
external int fputchar(
  int _Ch,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<FILE>)>()
external int getw(
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function(ffi.Int, ffi.Pointer<FILE>)>()
external int putw(
  int _Ch,
  ffi.Pointer<FILE> _Stream,
);

@ffi.Native<ffi.Int Function()>()
external int rmtmp();

/// Set the abort callback (passing null will restore original abort functionality: printing a message to stdout)
/// Returns the old callback for chaining
@ffi.Native<ggml_abort_callback_t Function(ggml_abort_callback_t)>()
external ggml_abort_callback_t ggml_set_abort_callback(
  ggml_abort_callback_t callback,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<ffi.Char>, ffi.Int, ffi.Pointer<ffi.Char>)
>()
external void ggml_abort(
  ffi.Pointer<ffi.Char> file,
  int line,
  ffi.Pointer<ffi.Char> fmt,
);

/// get ggml_status name string
@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.Int)>(
  symbol: 'ggml_status_to_string',
)
external ffi.Pointer<ffi.Char> _ggml_status_to_string(
  int status,
);

ffi.Pointer<ffi.Char> ggml_status_to_string(
  ggml_status status,
) => _ggml_status_to_string(
  status.value,
);

@ffi.Native<ffi.Float Function(ggml_fp16_t)>()
external double ggml_fp16_to_fp32(
  int arg0,
);

@ffi.Native<ggml_fp16_t Function(ffi.Float)>()
external int ggml_fp32_to_fp16(
  double arg0,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<ggml_fp16_t>, ffi.Pointer<ffi.Float>, ffi.Int64)
>()
external void ggml_fp16_to_fp32_row(
  ffi.Pointer<ggml_fp16_t> arg0,
  ffi.Pointer<ffi.Float> arg1,
  int arg2,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<ffi.Float>, ffi.Pointer<ggml_fp16_t>, ffi.Int64)
>()
external void ggml_fp32_to_fp16_row(
  ffi.Pointer<ffi.Float> arg0,
  ffi.Pointer<ggml_fp16_t> arg1,
  int arg2,
);

@ffi.Native<ggml_bf16_t Function(ffi.Float)>()
external ggml_bf16_t ggml_fp32_to_bf16(
  double arg0,
);

@ffi.Native<ffi.Float Function(ggml_bf16_t)>()
external double ggml_bf16_to_fp32(
  ggml_bf16_t arg0,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<ggml_bf16_t>, ffi.Pointer<ffi.Float>, ffi.Int64)
>()
external void ggml_bf16_to_fp32_row(
  ffi.Pointer<ggml_bf16_t> arg0,
  ffi.Pointer<ffi.Float> arg1,
  int arg2,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<ffi.Float>, ffi.Pointer<ggml_bf16_t>, ffi.Int64)
>()
external void ggml_fp32_to_bf16_row_ref(
  ffi.Pointer<ffi.Float> arg0,
  ffi.Pointer<ggml_bf16_t> arg1,
  int arg2,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<ffi.Float>, ffi.Pointer<ggml_bf16_t>, ffi.Int64)
>()
external void ggml_fp32_to_bf16_row(
  ffi.Pointer<ffi.Float> arg0,
  ffi.Pointer<ggml_bf16_t> arg1,
  int arg2,
);

@ffi.Native<ffi.Size>()
external final int GGML_TENSOR_SIZE;

@ffi.Native<ffi.Bool Function(ggml_guid_t, ggml_guid_t)>()
external bool ggml_guid_matches(
  ggml_guid_t guid_a,
  ggml_guid_t guid_b,
);

/// misc
@ffi.Native<ffi.Pointer<ffi.Char> Function()>()
external ffi.Pointer<ffi.Char> ggml_version();

@ffi.Native<ffi.Pointer<ffi.Char> Function()>()
external ffi.Pointer<ffi.Char> ggml_commit();

@ffi.Native<ffi.Void Function()>()
external void ggml_time_init();

@ffi.Native<ffi.Int64 Function()>()
external int ggml_time_ms();

@ffi.Native<ffi.Int64 Function()>()
external int ggml_time_us();

@ffi.Native<ffi.Int64 Function()>()
external int ggml_cycles();

@ffi.Native<ffi.Int64 Function()>()
external int ggml_cycles_per_ms();

/// accepts a UTF-8 path, even on Windows
@ffi.Native<
  ffi.Pointer<FILE> Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)
>()
external ffi.Pointer<FILE> ggml_fopen(
  ffi.Pointer<ffi.Char> fname,
  ffi.Pointer<ffi.Char> mode,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_object>)>()
external void ggml_print_object(
  ffi.Pointer<ggml_object> obj,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_context>)>()
external void ggml_print_objects(
  ffi.Pointer<ggml_context> ctx,
);

@ffi.Native<ffi.Int64 Function(ffi.Pointer<ggml_tensor>)>()
external int ggml_nelements(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Int64 Function(ffi.Pointer<ggml_tensor>)>()
external int ggml_nrows(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Size Function(ffi.Pointer<ggml_tensor>)>()
external int ggml_nbytes(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Size Function(ffi.Pointer<ggml_tensor>)>()
external int ggml_nbytes_pad(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Int64 Function(ffi.UnsignedInt)>(symbol: 'ggml_blck_size')
external int _ggml_blck_size(
  int type,
);

int ggml_blck_size(
  ggml_type type,
) => _ggml_blck_size(
  type.value,
);

@ffi.Native<ffi.Size Function(ffi.UnsignedInt)>(symbol: 'ggml_type_size')
external int _ggml_type_size(
  int type,
);

int ggml_type_size(
  ggml_type type,
) => _ggml_type_size(
  type.value,
);

@ffi.Native<ffi.Size Function(ffi.UnsignedInt, ffi.Int64)>(
  symbol: 'ggml_row_size',
)
external int _ggml_row_size(
  int type,
  int ne,
);

int ggml_row_size(
  ggml_type type,
  int ne,
) => _ggml_row_size(
  type.value,
  ne,
);

@ffi.Native<ffi.Double Function(ffi.UnsignedInt)>(symbol: 'ggml_type_sizef')
external double _ggml_type_sizef(
  int type,
);

double ggml_type_sizef(
  ggml_type type,
) => _ggml_type_sizef(
  type.value,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.UnsignedInt)>(
  symbol: 'ggml_type_name',
)
external ffi.Pointer<ffi.Char> _ggml_type_name(
  int type,
);

ffi.Pointer<ffi.Char> ggml_type_name(
  ggml_type type,
) => _ggml_type_name(
  type.value,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.UnsignedInt)>(
  symbol: 'ggml_op_name',
)
external ffi.Pointer<ffi.Char> _ggml_op_name(
  int op,
);

ffi.Pointer<ffi.Char> ggml_op_name(
  ggml_op op,
) => _ggml_op_name(
  op.value,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.UnsignedInt)>(
  symbol: 'ggml_op_symbol',
)
external ffi.Pointer<ffi.Char> _ggml_op_symbol(
  int op,
);

ffi.Pointer<ffi.Char> ggml_op_symbol(
  ggml_op op,
) => _ggml_op_symbol(
  op.value,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.UnsignedInt)>(
  symbol: 'ggml_unary_op_name',
)
external ffi.Pointer<ffi.Char> _ggml_unary_op_name(
  int op,
);

ffi.Pointer<ffi.Char> ggml_unary_op_name(
  ggml_unary_op op,
) => _ggml_unary_op_name(
  op.value,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.UnsignedInt)>(
  symbol: 'ggml_glu_op_name',
)
external ffi.Pointer<ffi.Char> _ggml_glu_op_name(
  int op,
);

ffi.Pointer<ffi.Char> ggml_glu_op_name(
  ggml_glu_op op,
) => _ggml_glu_op_name(
  op.value,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ffi.Char> ggml_op_desc(
  ffi.Pointer<ggml_tensor> t,
);

@ffi.Native<ffi.Size Function(ffi.Pointer<ggml_tensor>)>()
external int ggml_element_size(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ffi.UnsignedInt)>(symbol: 'ggml_is_quantized')
external bool _ggml_is_quantized(
  int type,
);

bool ggml_is_quantized(
  ggml_type type,
) => _ggml_is_quantized(
  type.value,
);

/// TODO: temporary until model loading of ggml examples is refactored
@ffi.Native<ffi.UnsignedInt Function(ffi.Int)>(
  symbol: 'ggml_ftype_to_ggml_type',
)
external int _ggml_ftype_to_ggml_type(
  int ftype,
);

ggml_type ggml_ftype_to_ggml_type(
  ggml_ftype ftype,
) => ggml_type.fromValue(
  _ggml_ftype_to_ggml_type(
    ftype.value,
  ),
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_transposed(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_permuted(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_empty(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_scalar(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_vector(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_matrix(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_3d(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ggml_tensor>)>()
external int ggml_n_dims(
  ffi.Pointer<ggml_tensor> tensor,
);

/// returns whether the tensor elements can be iterated over with a flattened index (no gaps, no permutation)
@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_contiguous(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_contiguous_0(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_contiguous_1(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_contiguous_2(
  ffi.Pointer<ggml_tensor> tensor,
);

/// returns whether the tensor elements are allocated as one contiguous block of memory (no gaps, but permutation ok)
@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_contiguously_allocated(
  ffi.Pointer<ggml_tensor> tensor,
);

/// true for tensor that is stored in memory as CxWxHxN and has been permuted to WxHxCxN
@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_contiguous_channels(
  ffi.Pointer<ggml_tensor> tensor,
);

/// true if the elements in dimension 0 are contiguous, or there is just 1 block of elements
@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_tensor>)>()
external bool ggml_is_contiguous_rows(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<
  ffi.Bool Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)
>()
external bool ggml_are_same_shape(
  ffi.Pointer<ggml_tensor> t0,
  ffi.Pointer<ggml_tensor> t1,
);

@ffi.Native<
  ffi.Bool Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)
>()
external bool ggml_are_same_stride(
  ffi.Pointer<ggml_tensor> t0,
  ffi.Pointer<ggml_tensor> t1,
);

@ffi.Native<
  ffi.Bool Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)
>()
external bool ggml_can_repeat(
  ffi.Pointer<ggml_tensor> t0,
  ffi.Pointer<ggml_tensor> t1,
);

/// use this to compute the memory overhead of a tensor
@ffi.Native<ffi.Size Function()>()
external int ggml_tensor_overhead();

@ffi.Native<
  ffi.Bool Function(ffi.UnsignedInt, ffi.Pointer<ffi.Void>, ffi.Size)
>(symbol: 'ggml_validate_row_data')
external bool _ggml_validate_row_data(
  int type,
  ffi.Pointer<ffi.Void> data,
  int nbytes,
);

bool ggml_validate_row_data(
  ggml_type type,
  ffi.Pointer<ffi.Void> data,
  int nbytes,
) => _ggml_validate_row_data(
  type.value,
  data,
  nbytes,
);

/// main
@ffi.Native<ffi.Pointer<ggml_context> Function(ggml_init_params)>()
external ffi.Pointer<ggml_context> ggml_init(
  ggml_init_params params,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_context>)>()
external void ggml_reset(
  ffi.Pointer<ggml_context> ctx,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_context>)>()
external void ggml_free(
  ffi.Pointer<ggml_context> ctx,
);

@ffi.Native<ffi.Size Function(ffi.Pointer<ggml_context>)>()
external int ggml_used_mem(
  ffi.Pointer<ggml_context> ctx,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<ggml_context>)>()
external bool ggml_get_no_alloc(
  ffi.Pointer<ggml_context> ctx,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_context>, ffi.Bool)>()
external void ggml_set_no_alloc(
  ffi.Pointer<ggml_context> ctx,
  bool no_alloc,
);

@ffi.Native<ffi.Pointer<ffi.Void> Function(ffi.Pointer<ggml_context>)>()
external ffi.Pointer<ffi.Void> ggml_get_mem_buffer(
  ffi.Pointer<ggml_context> ctx,
);

@ffi.Native<ffi.Size Function(ffi.Pointer<ggml_context>)>()
external int ggml_get_mem_size(
  ffi.Pointer<ggml_context> ctx,
);

@ffi.Native<ffi.Size Function(ffi.Pointer<ggml_context>)>()
external int ggml_get_max_tensor_size(
  ffi.Pointer<ggml_context> ctx,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.UnsignedInt,
    ffi.Int,
    ffi.Pointer<ffi.Int64>,
  )
>(symbol: 'ggml_new_tensor')
external ffi.Pointer<ggml_tensor> _ggml_new_tensor(
  ffi.Pointer<ggml_context> ctx,
  int type,
  int n_dims,
  ffi.Pointer<ffi.Int64> ne,
);

ffi.Pointer<ggml_tensor> ggml_new_tensor(
  ffi.Pointer<ggml_context> ctx,
  ggml_type type,
  int n_dims,
  ffi.Pointer<ffi.Int64> ne,
) => _ggml_new_tensor(
  ctx,
  type.value,
  n_dims,
  ne,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.UnsignedInt,
    ffi.Int64,
  )
>(symbol: 'ggml_new_tensor_1d')
external ffi.Pointer<ggml_tensor> _ggml_new_tensor_1d(
  ffi.Pointer<ggml_context> ctx,
  int type,
  int ne0,
);

ffi.Pointer<ggml_tensor> ggml_new_tensor_1d(
  ffi.Pointer<ggml_context> ctx,
  ggml_type type,
  int ne0,
) => _ggml_new_tensor_1d(
  ctx,
  type.value,
  ne0,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.UnsignedInt,
    ffi.Int64,
    ffi.Int64,
  )
>(symbol: 'ggml_new_tensor_2d')
external ffi.Pointer<ggml_tensor> _ggml_new_tensor_2d(
  ffi.Pointer<ggml_context> ctx,
  int type,
  int ne0,
  int ne1,
);

ffi.Pointer<ggml_tensor> ggml_new_tensor_2d(
  ffi.Pointer<ggml_context> ctx,
  ggml_type type,
  int ne0,
  int ne1,
) => _ggml_new_tensor_2d(
  ctx,
  type.value,
  ne0,
  ne1,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.UnsignedInt,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
  )
>(symbol: 'ggml_new_tensor_3d')
external ffi.Pointer<ggml_tensor> _ggml_new_tensor_3d(
  ffi.Pointer<ggml_context> ctx,
  int type,
  int ne0,
  int ne1,
  int ne2,
);

ffi.Pointer<ggml_tensor> ggml_new_tensor_3d(
  ffi.Pointer<ggml_context> ctx,
  ggml_type type,
  int ne0,
  int ne1,
  int ne2,
) => _ggml_new_tensor_3d(
  ctx,
  type.value,
  ne0,
  ne1,
  ne2,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.UnsignedInt,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
  )
>(symbol: 'ggml_new_tensor_4d')
external ffi.Pointer<ggml_tensor> _ggml_new_tensor_4d(
  ffi.Pointer<ggml_context> ctx,
  int type,
  int ne0,
  int ne1,
  int ne2,
  int ne3,
);

ffi.Pointer<ggml_tensor> ggml_new_tensor_4d(
  ffi.Pointer<ggml_context> ctx,
  ggml_type type,
  int ne0,
  int ne1,
  int ne2,
  int ne3,
) => _ggml_new_tensor_4d(
  ctx,
  type.value,
  ne0,
  ne1,
  ne2,
  ne3,
);

@ffi.Native<
  ffi.Pointer<ffi.Void> Function(ffi.Pointer<ggml_context>, ffi.Size)
>()
external ffi.Pointer<ffi.Void> ggml_new_buffer(
  ffi.Pointer<ggml_context> ctx,
  int nbytes,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_dup_tensor(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> src,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_view_tensor(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> src,
);

/// Context tensor enumeration and lookup
@ffi.Native<ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>)>()
external ffi.Pointer<ggml_tensor> ggml_get_first_tensor(
  ffi.Pointer<ggml_context> ctx,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_get_next_tensor(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ffi.Char>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_get_tensor(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ffi.Char> name,
);

/// Converts a flat index into coordinates
@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<ggml_tensor>,
    ffi.Int64,
    ffi.Pointer<ffi.Int64>,
    ffi.Pointer<ffi.Int64>,
    ffi.Pointer<ffi.Int64>,
    ffi.Pointer<ffi.Int64>,
  )
>()
external void ggml_unravel_index(
  ffi.Pointer<ggml_tensor> tensor,
  int i,
  ffi.Pointer<ffi.Int64> i0,
  ffi.Pointer<ffi.Int64> i1,
  ffi.Pointer<ffi.Int64> i2,
  ffi.Pointer<ffi.Int64> i3,
);

@ffi.Native<ffi.UnsignedInt Function(ffi.Pointer<ggml_tensor>)>(
  symbol: 'ggml_get_unary_op',
)
external int _ggml_get_unary_op(
  ffi.Pointer<ggml_tensor> tensor,
);

ggml_unary_op ggml_get_unary_op(
  ffi.Pointer<ggml_tensor> tensor,
) => ggml_unary_op.fromValue(
  _ggml_get_unary_op(
    tensor,
  ),
);

@ffi.Native<ffi.UnsignedInt Function(ffi.Pointer<ggml_tensor>)>(
  symbol: 'ggml_get_glu_op',
)
external int _ggml_get_glu_op(
  ffi.Pointer<ggml_tensor> tensor,
);

ggml_glu_op ggml_get_glu_op(
  ffi.Pointer<ggml_tensor> tensor,
) => ggml_glu_op.fromValue(
  _ggml_get_glu_op(
    tensor,
  ),
);

@ffi.Native<ffi.Pointer<ffi.Void> Function(ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ffi.Void> ggml_get_data(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Pointer<ffi.Float> Function(ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ffi.Float> ggml_get_data_f32(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ffi.Char> ggml_get_name(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ffi.Char>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_set_name(
  ffi.Pointer<ggml_tensor> tensor,
  ffi.Pointer<ffi.Char> name,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ffi.Char>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_format_name(
  ffi.Pointer<ggml_tensor> tensor,
  ffi.Pointer<ffi.Char> fmt,
);

/// Tensor flags
@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_tensor>)>()
external void ggml_set_input(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_tensor>)>()
external void ggml_set_output(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_tensor>)>()
external void ggml_set_param(
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_tensor>)>()
external void ggml_set_loss(
  ffi.Pointer<ggml_tensor> tensor,
);

/// operations on tensors with backpropagation
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_dup(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// in-place, returns view(a)
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_dup_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_add(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_add_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.UnsignedInt,
  )
>(symbol: 'ggml_add_cast')
external ffi.Pointer<ggml_tensor> _ggml_add_cast(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int type,
);

ffi.Pointer<ggml_tensor> ggml_add_cast(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ggml_type type,
) => _ggml_add_cast(
  ctx,
  a,
  b,
  type.value,
);

/// dst[i0, i1, i2] = a[i0, i1, i2] + b[i0, ids[i1, i2]]
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_add_id(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> ids,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_add1(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_add1_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

/// dst = a
/// view(dst, nb1, nb2, nb3, offset) += b
/// return dst
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Size,
    ffi.Size,
    ffi.Size,
    ffi.Size,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_acc(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int nb1,
  int nb2,
  int nb3,
  int offset,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Size,
    ffi.Size,
    ffi.Size,
    ffi.Size,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_acc_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int nb1,
  int nb2,
  int nb3,
  int offset,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_sub(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_sub_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_mul(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_mul_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_div(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_div_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_sqr(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_sqr_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_sqrt(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_sqrt_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_log(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_log_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_expm1(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_expm1_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_softplus(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_softplus_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_sin(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_sin_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_cos(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_cos_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// return scalar
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_sum(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// sums along rows, with input shape [a,b,c,d] return shape [1,b,c,d]
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_sum_rows(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_cumsum(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// mean along rows
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_mean(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// argmax along rows
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_argmax(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// count number of equal elements in a and b
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_count_equal(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

/// if a is the same shape as b, and a is not parameter, return a
/// otherwise, return a new tensor: repeat(a) to fit in b
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_repeat(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

/// repeat a to the specified shape
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_repeat_4d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
  int ne2,
  int ne3,
);

/// sums repetitions in a into shape of b
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_repeat_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

/// concat a and b along dim
/// used in stable-diffusion
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_concat(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int dim,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_abs(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_abs_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_sgn(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_sgn_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_neg(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_neg_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_step(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_step_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_tanh(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_tanh_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_elu(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_elu_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_relu(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
    ffi.Bool,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_leaky_relu(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double negative_slope,
  bool inplace,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_relu_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_sigmoid(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_sigmoid_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_gelu(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_gelu_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// GELU using erf (error function) when possible
/// some backends may fallback to approximation based on Abramowitz and Stegun formula
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_gelu_erf(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_gelu_erf_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_gelu_quick(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_gelu_quick_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_silu(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_silu_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// a - x
/// b - dy
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_silu_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

/// hardswish(x) = x * relu6(x + 3) / 6
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_hardswish(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// hardsigmoid(x) = relu6(x + 3) / 6
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_hardsigmoid(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_exp(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_exp_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_floor(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_floor_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_ceil(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_ceil_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_round(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_round_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// Truncates the fractional part of each element in the tensor (towards zero).
/// For example: trunc(3.7) = 3.0, trunc(-2.9) = -2.0
/// Similar to std::trunc in C/C++.
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_trunc(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_trunc_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// xIELU activation function
/// x = x * (c_a(alpha_n) + c_b(alpha_p, beta) * sigmoid(beta * x)) + eps * (x > 0)
/// where c_a = softplus and c_b(a, b) = softplus(a) + b are constraining functions
/// that constrain the positive and negative source alpha values respectively
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_xielu(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double alpha_n,
  double alpha_p,
  double beta,
  double eps,
);

/// gated linear unit ops
/// A: n columns, r rows,
/// result is n / 2 columns, r rows,
/// expects gate in second half of row, unless swapped is true
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.UnsignedInt,
    ffi.Bool,
  )
>(symbol: 'ggml_glu')
external ffi.Pointer<ggml_tensor> _ggml_glu(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int op,
  bool swapped,
);

ffi.Pointer<ggml_tensor> ggml_glu(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_glu_op op,
  bool swapped,
) => _ggml_glu(
  ctx,
  a,
  op.value,
  swapped,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_reglu(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_reglu_swapped(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_geglu(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_geglu_swapped(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_swiglu(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_swiglu_swapped(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_geglu_erf(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_geglu_erf_swapped(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_geglu_quick(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_geglu_quick_swapped(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// A: n columns, r rows,
/// B: n columns, r rows,
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.UnsignedInt,
  )
>(symbol: 'ggml_glu_split')
external ffi.Pointer<ggml_tensor> _ggml_glu_split(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int op,
);

ffi.Pointer<ggml_tensor> ggml_glu_split(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ggml_glu_op op,
) => _ggml_glu_split(
  ctx,
  a,
  b,
  op.value,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_reglu_split(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_geglu_split(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_swiglu_split(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_geglu_erf_split(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_geglu_quick_split(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_swiglu_oai(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  double alpha,
  double limit,
);

/// normalize along rows
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_norm(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double eps,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_norm_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double eps,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_rms_norm(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double eps,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_rms_norm_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double eps,
);

/// group normalize along ne0*ne1*n_groups
/// used in stable-diffusion
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_group_norm(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int n_groups,
  double eps,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_group_norm_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int n_groups,
  double eps,
);

/// l2 normalize along rows
/// used in rwkv v7
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_l2_norm(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double eps,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_l2_norm_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double eps,
);

/// a - x
/// b - dy
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_rms_norm_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  double eps,
);

/// A: k columns, n rows => [ne03, ne02, n, k]
/// B: k columns, m rows  (i.e. we transpose it internally) => [ne03 * x, ne02 * y, m, k]
/// result is n columns, m rows => [ne03 * x, ne02 * y, m, n]
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_mul_mat(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

/// change the precision of a matrix multiplication
/// set to GGML_PREC_F32 for higher precision (useful for phi-2)
@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.UnsignedInt)>(
  symbol: 'ggml_mul_mat_set_prec',
)
external void _ggml_mul_mat_set_prec(
  ffi.Pointer<ggml_tensor> a,
  int prec,
);

void ggml_mul_mat_set_prec(
  ffi.Pointer<ggml_tensor> a,
  ggml_prec prec,
) => _ggml_mul_mat_set_prec(
  a,
  prec.value,
);

/// indirect matrix multiplication
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_mul_mat_id(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> as,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> ids,
);

/// A: m columns, n rows,
/// B: p columns, n rows,
/// result is m columns, p rows
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_out_prod(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

/// operations on tensors without backpropagation
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_scale(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double s,
);

/// in-place, returns view(a)
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_scale_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double s,
);

/// x = s * a + b
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_scale_bias(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double s,
  double b,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_scale_bias_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double s,
  double b,
);

/// b -> view(a,offset,nb1,nb2,3), return modified a
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Size,
    ffi.Size,
    ffi.Size,
    ffi.Size,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_set(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int nb1,
  int nb2,
  int nb3,
  int offset,
);

/// b -> view(a,offset,nb1,nb2,3), return view(a)
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Size,
    ffi.Size,
    ffi.Size,
    ffi.Size,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_set_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int nb1,
  int nb2,
  int nb3,
  int offset,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Size,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_set_1d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int offset,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Size,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_set_1d_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int offset,
);

/// b -> view(a,offset,nb1,nb2,3), return modified a
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Size,
    ffi.Size,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_set_2d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int nb1,
  int offset,
);

/// b -> view(a,offset,nb1,nb2,3), return view(a)
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Size,
    ffi.Size,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_set_2d_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int nb1,
  int offset,
);

/// a -> b, return view(b)
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_cpy(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

/// note: casting from f32 to i32 will discard the fractional part
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.UnsignedInt,
  )
>(symbol: 'ggml_cast')
external ffi.Pointer<ggml_tensor> _ggml_cast(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int type,
);

ffi.Pointer<ggml_tensor> ggml_cast(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_type type,
) => _ggml_cast(
  ctx,
  a,
  type.value,
);

/// make contiguous
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_cont(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// make contiguous, with new shape
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int64,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_cont_1d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int64,
    ffi.Int64,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_cont_2d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_cont_3d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
  int ne2,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_cont_4d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
  int ne2,
  int ne3,
);

/// return view(a), b specifies the new shape
/// TODO: when we start computing gradient, make a copy instead of view
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_reshape(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

/// return view(a)
/// TODO: when we start computing gradient, make a copy instead of view
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int64,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_reshape_1d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int64,
    ffi.Int64,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_reshape_2d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
);

/// return view(a)
/// TODO: when we start computing gradient, make a copy instead of view
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_reshape_3d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
  int ne2,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_reshape_4d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
  int ne2,
  int ne3,
);

/// offset in bytes
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int64,
    ffi.Size,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_view_1d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int offset,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int64,
    ffi.Int64,
    ffi.Size,
    ffi.Size,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_view_2d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
  int nb1,
  int offset,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
    ffi.Size,
    ffi.Size,
    ffi.Size,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_view_3d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
  int ne2,
  int nb1,
  int nb2,
  int offset,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
    ffi.Size,
    ffi.Size,
    ffi.Size,
    ffi.Size,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_view_4d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
  int ne2,
  int ne3,
  int nb1,
  int nb2,
  int nb3,
  int offset,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_permute(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int axis0,
  int axis1,
  int axis2,
  int axis3,
);

/// alias for ggml_permute(ctx, a, 1, 0, 2, 3)
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_transpose(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// supports 4D a:
/// a     [n_embd, ne1, ne2, ne3]
/// b I32 [n_rows, ne2, ne3, 1]
///
/// return [n_embd, n_rows, ne2, ne3]
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_get_rows(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_get_rows_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
);

/// a TD  [n_embd, ne1,    ne2,    ne3]
/// b TS  [n_embd, n_rows, ne02,   ne03] | ne02 == ne2, ne03 == ne3
/// c I64 [n_rows, ne11,   ne12,   1]    | c[i] in [0, ne1)
///
/// undefined behavior if destination rows overlap
///
/// broadcast:
/// ne2 % ne11 == 0
/// ne3 % ne12 == 0
///
/// return view(a)
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_set_rows(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_diag(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// set elements above the diagonal to -INF
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_diag_mask_inf(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int n_past,
);

/// in-place, returns view(a)
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_diag_mask_inf_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int n_past,
);

/// set elements above the diagonal to 0
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_diag_mask_zero(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int n_past,
);

/// in-place, returns view(a)
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_diag_mask_zero_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int n_past,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_soft_max(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// in-place, returns view(a)
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_soft_max_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
);

/// a    [ne0, ne01, ne02, ne03]
/// mask [ne0, ne11, ne12, ne13] | ne11 >= ne01, F16 or F32, optional
///
/// broadcast:
/// ne02 % ne12 == 0
/// ne03 % ne13 == 0
///
/// fused soft_max(a*scale + mask*(ALiBi slope))
/// max_bias = 0.0f for no ALiBi
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_soft_max_ext(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> mask,
  double scale,
  double max_bias,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_soft_max_ext_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> mask,
  double scale,
  double max_bias,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)
>()
external void ggml_soft_max_add_sinks(
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> sinks,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_soft_max_ext_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  double scale,
  double max_bias,
);

/// in-place, returns view(a)
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_soft_max_ext_back_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  double scale,
  double max_bias,
);

/// rotary position embedding
/// if (mode & 1) - skip n_past elements (NOT SUPPORTED)
/// if (mode & GGML_ROPE_TYPE_NEOX) - GPT-NeoX style
///
/// b is an int32 vector with size a->ne[2], it contains the positions
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_rope(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int n_dims,
  int mode,
);

/// in-place, returns view(a)
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_rope_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int n_dims,
  int mode,
);

/// custom RoPE
/// c is freq factors (e.g. phi3-128k), (optional)
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_rope_ext(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
  int n_dims,
  int mode,
  int n_ctx_orig,
  double freq_base,
  double freq_scale,
  double ext_factor,
  double attn_factor,
  double beta_fast,
  double beta_slow,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Pointer<ffi.Int>,
    ffi.Int,
    ffi.Int,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_rope_multi(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
  int n_dims,
  ffi.Pointer<ffi.Int> sections,
  int mode,
  int n_ctx_orig,
  double freq_base,
  double freq_scale,
  double ext_factor,
  double attn_factor,
  double beta_fast,
  double beta_slow,
);

/// in-place, returns view(a)
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_rope_ext_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
  int n_dims,
  int mode,
  int n_ctx_orig,
  double freq_base,
  double freq_scale,
  double ext_factor,
  double attn_factor,
  double beta_fast,
  double beta_slow,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Pointer<ffi.Int>,
    ffi.Int,
    ffi.Int,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_rope_multi_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
  int n_dims,
  ffi.Pointer<ffi.Int> sections,
  int mode,
  int n_ctx_orig,
  double freq_base,
  double freq_scale,
  double ext_factor,
  double attn_factor,
  double beta_fast,
  double beta_slow,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_rope_custom(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int n_dims,
  int mode,
  int n_ctx_orig,
  double freq_base,
  double freq_scale,
  double ext_factor,
  double attn_factor,
  double beta_fast,
  double beta_slow,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_rope_custom_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int n_dims,
  int mode,
  int n_ctx_orig,
  double freq_base,
  double freq_scale,
  double ext_factor,
  double attn_factor,
  double beta_fast,
  double beta_slow,
);

/// compute correction dims for YaRN RoPE scaling
@ffi.Native<
  ffi.Void Function(
    ffi.Int,
    ffi.Int,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Pointer<ffi.Float>,
  )
>()
external void ggml_rope_yarn_corr_dims(
  int n_dims,
  int n_ctx_orig,
  double freq_base,
  double beta_fast,
  double beta_slow,
  ffi.Pointer<ffi.Float> dims,
);

/// rotary position embedding backward, i.e compute dx from dy
/// a - dy
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_rope_ext_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
  int n_dims,
  int mode,
  int n_ctx_orig,
  double freq_base,
  double freq_scale,
  double ext_factor,
  double attn_factor,
  double beta_fast,
  double beta_slow,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Pointer<ffi.Int>,
    ffi.Int,
    ffi.Int,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_rope_multi_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
  int n_dims,
  ffi.Pointer<ffi.Int> sections,
  int mode,
  int n_ctx_orig,
  double freq_base,
  double freq_scale,
  double ext_factor,
  double attn_factor,
  double beta_fast,
  double beta_slow,
);

/// clamp
/// in-place, returns view(a)
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_clamp(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double min,
  double max,
);

/// im2col
/// converts data into a format that effectively results in a convolution when combined with matrix multiplication
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Bool,
    ffi.UnsignedInt,
  )
>(symbol: 'ggml_im2col')
external ffi.Pointer<ggml_tensor> _ggml_im2col(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int s0,
  int s1,
  int p0,
  int p1,
  int d0,
  int d1,
  bool is_2D,
  int dst_type,
);

ffi.Pointer<ggml_tensor> ggml_im2col(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int s0,
  int s1,
  int p0,
  int p1,
  int d0,
  int d1,
  bool is_2D,
  ggml_type dst_type,
) => _ggml_im2col(
  ctx,
  a,
  b,
  s0,
  s1,
  p0,
  p1,
  d0,
  d1,
  is_2D,
  dst_type.value,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ffi.Int64>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Bool,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_im2col_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ffi.Int64> ne,
  int s0,
  int s1,
  int p0,
  int p1,
  int d0,
  int d1,
  bool is_2D,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_conv_1d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int s0,
  int p0,
  int d0,
);

/// conv_1d with padding = half
/// alias for ggml_conv_1d(a, b, s, a->ne[0]/2, d)
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_conv_1d_ph(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int s,
  int d,
);

/// depthwise
/// TODO: this is very likely wrong for some cases! - needs more testing
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_conv_1d_dw(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int s0,
  int p0,
  int d0,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_conv_1d_dw_ph(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int s0,
  int d0,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_conv_transpose_1d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int s0,
  int p0,
  int d0,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_conv_2d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int s0,
  int s1,
  int p0,
  int p1,
  int d0,
  int d1,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int64,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.UnsignedInt,
  )
>(symbol: 'ggml_im2col_3d')
external ffi.Pointer<ggml_tensor> _ggml_im2col_3d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int IC,
  int s0,
  int s1,
  int s2,
  int p0,
  int p1,
  int p2,
  int d0,
  int d1,
  int d2,
  int dst_type,
);

ffi.Pointer<ggml_tensor> ggml_im2col_3d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int IC,
  int s0,
  int s1,
  int s2,
  int p0,
  int p1,
  int p2,
  int d0,
  int d1,
  int d2,
  ggml_type dst_type,
) => _ggml_im2col_3d(
  ctx,
  a,
  b,
  IC,
  s0,
  s1,
  s2,
  p0,
  p1,
  p2,
  d0,
  d1,
  d2,
  dst_type.value,
);

/// a: [OC*IC, KD, KH, KW]
/// b: [N*IC, ID, IH, IW]
/// result: [N*OC, OD, OH, OW]
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int64,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_conv_3d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int IC,
  int s0,
  int s1,
  int s2,
  int p0,
  int p1,
  int p2,
  int d0,
  int d1,
  int d2,
);

/// kernel size is a->ne[0] x a->ne[1]
/// stride is equal to kernel size
/// padding is zero
/// example:
/// a:     16   16    3  768
/// b:   1024 1024    3    1
/// res:   64   64  768    1
/// used in sam
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_conv_2d_sk_p0(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

/// kernel size is a->ne[0] x a->ne[1]
/// stride is 1
/// padding is half
/// example:
/// a:      3    3    256  256
/// b:     64   64    256    1
/// res:   64   64    256    1
/// used in sam
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_conv_2d_s1_ph(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

/// depthwise (via im2col and mul_mat)
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_conv_2d_dw(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int s0,
  int s1,
  int p0,
  int p1,
  int d0,
  int d1,
);

/// Depthwise 2D convolution
/// may be faster than ggml_conv_2d_dw, but not available in all backends
/// a:   KW    KH    1    C    convolution kernel
/// b:   W     H     C    N    input data
/// res: W_out H_out C    N
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_conv_2d_dw_direct(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int stride0,
  int stride1,
  int pad0,
  int pad1,
  int dilation0,
  int dilation1,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_conv_transpose_2d_p0(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int stride,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_conv_2d_direct(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int s0,
  int s1,
  int p0,
  int p1,
  int d0,
  int d1,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_conv_3d_direct(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  int s0,
  int s1,
  int s2,
  int p0,
  int p1,
  int p2,
  int d0,
  int d1,
  int d2,
  int n_channels,
  int n_batch,
  int n_channels_out,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.UnsignedInt,
    ffi.Int,
    ffi.Int,
    ffi.Int,
  )
>(symbol: 'ggml_pool_1d')
external ffi.Pointer<ggml_tensor> _ggml_pool_1d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int op,
  int k0,
  int s0,
  int p0,
);

ffi.Pointer<ggml_tensor> ggml_pool_1d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_op_pool op,
  int k0,
  int s0,
  int p0,
) => _ggml_pool_1d(
  ctx,
  a,
  op.value,
  k0,
  s0,
  p0,
);

/// the result will have 2*p0 padding for the first dimension
/// and 2*p1 padding for the second dimension
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.UnsignedInt,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Float,
    ffi.Float,
  )
>(symbol: 'ggml_pool_2d')
external ffi.Pointer<ggml_tensor> _ggml_pool_2d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int op,
  int k0,
  int k1,
  int s0,
  int s1,
  double p0,
  double p1,
);

ffi.Pointer<ggml_tensor> ggml_pool_2d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_op_pool op,
  int k0,
  int k1,
  int s0,
  int s1,
  double p0,
  double p1,
) => _ggml_pool_2d(
  ctx,
  a,
  op.value,
  k0,
  k1,
  s0,
  s1,
  p0,
  p1,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.UnsignedInt,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Float,
    ffi.Float,
  )
>(symbol: 'ggml_pool_2d_back')
external ffi.Pointer<ggml_tensor> _ggml_pool_2d_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> af,
  int op,
  int k0,
  int k1,
  int s0,
  int s1,
  double p0,
  double p1,
);

ffi.Pointer<ggml_tensor> ggml_pool_2d_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> af,
  ggml_op_pool op,
  int k0,
  int k1,
  int s0,
  int s1,
  double p0,
  double p1,
) => _ggml_pool_2d_back(
  ctx,
  a,
  af,
  op.value,
  k0,
  k1,
  s0,
  s1,
  p0,
  p1,
);

/// interpolate
/// multiplies ne0 and ne1 by scale factor
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.UnsignedInt,
  )
>(symbol: 'ggml_upscale')
external ffi.Pointer<ggml_tensor> _ggml_upscale(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int scale_factor,
  int mode,
);

ffi.Pointer<ggml_tensor> ggml_upscale(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int scale_factor,
  ggml_scale_mode mode,
) => _ggml_upscale(
  ctx,
  a,
  scale_factor,
  mode.value,
);

/// interpolate
/// interpolate scale to specified dimensions
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.UnsignedInt,
  )
>(symbol: 'ggml_upscale_ext')
external ffi.Pointer<ggml_tensor> _ggml_upscale_ext(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
  int ne2,
  int ne3,
  int mode,
);

ffi.Pointer<ggml_tensor> ggml_upscale_ext(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
  int ne2,
  int ne3,
  ggml_scale_mode mode,
) => _ggml_upscale_ext(
  ctx,
  a,
  ne0,
  ne1,
  ne2,
  ne3,
  mode.value,
);

/// Up- or downsamples the input to the specified size.
/// 2D scale modes (eg. bilinear) are applied to the first two dimensions.
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
    ffi.Uint32,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_interpolate(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int ne0,
  int ne1,
  int ne2,
  int ne3,
  int mode,
);

/// pad each dimension with zeros: [x, ..., x] -> [x, ..., x, 0, ..., 0]
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_pad(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int p0,
  int p1,
  int p2,
  int p3,
);

/// pad each dimension with values on the other side of the torus (looping around)
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_pad_circular(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int p0,
  int p1,
  int p2,
  int p3,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_pad_ext(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int lp0,
  int rp0,
  int lp1,
  int rp1,
  int lp2,
  int rp2,
  int lp3,
  int rp3,
);

/// pad each dimension with values on the other side of the torus (looping around)
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_pad_ext_circular(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int lp0,
  int rp0,
  int lp1,
  int rp1,
  int lp2,
  int rp2,
  int lp3,
  int rp3,
);

/// pad each dimension with reflection: [a, b, c, d] -> [b, a, b, c, d, c]
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_pad_reflect_1d(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int p0,
  int p1,
);

/// Move tensor elements by an offset given for each dimension. Elements that
/// are shifted beyond the last position are wrapped around to the beginning.
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_roll(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int shift0,
  int shift1,
  int shift2,
  int shift3,
);

/// Convert matrix into a triangular one (upper, strict upper, lower or strict lower) by writing
/// zeroes everywhere outside the masked area
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.UnsignedInt,
  )
>(symbol: 'ggml_tri')
external ffi.Pointer<ggml_tensor> _ggml_tri(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int type,
);

ffi.Pointer<ggml_tensor> ggml_tri(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_tri_type type,
) => _ggml_tri(
  ctx,
  a,
  type.value,
);

/// Fill tensor a with constant c
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_fill(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double c,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_fill_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  double c,
);

/// Ref: https://github.com/CompVis/stable-diffusion/blob/main/ldm/modules/diffusionmodules/util.py#L151
/// timesteps: [N,]
/// return: [N, dim]
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_timestep_embedding(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> timesteps,
  int dim,
  int max_period,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.UnsignedInt,
  )
>(symbol: 'ggml_argsort')
external ffi.Pointer<ggml_tensor> _ggml_argsort(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int order,
);

ffi.Pointer<ggml_tensor> ggml_argsort(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_sort_order order,
) => _ggml_argsort(
  ctx,
  a,
  order.value,
);

/// similar to ggml_top_k but implemented as `argsort` + `view`
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_argsort_top_k(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int k,
);

/// top k elements per row
/// note: the resulting top k indices are in no particular order
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_top_k(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int k,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Float,
    ffi.Float,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_arange(
  ffi.Pointer<ggml_context> ctx,
  double start,
  double stop,
  double step,
);

/// q:    [n_embd_k, n_batch, n_head,    ne3 ]
/// k:    [n_embd_k, n_kv,    n_head_kv, ne3 ]
/// v:    [n_embd_v, n_kv,    n_head_kv, ne3 ] !! not transposed !!
/// mask: [n_kv,     n_batch, ne32,      ne33]
/// res:  [n_embd_v, n_head,  n_batch,   ne3 ] !! permuted !!
///
/// broadcast:
/// n_head % n_head_kv == 0
/// n_head % ne32      == 0
/// ne3    % ne33      == 0
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
    ffi.Float,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_flash_attn_ext(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> q,
  ffi.Pointer<ggml_tensor> k,
  ffi.Pointer<ggml_tensor> v,
  ffi.Pointer<ggml_tensor> mask,
  double scale,
  double max_bias,
  double logit_softcap,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.UnsignedInt)>(
  symbol: 'ggml_flash_attn_ext_set_prec',
)
external void _ggml_flash_attn_ext_set_prec(
  ffi.Pointer<ggml_tensor> a,
  int prec,
);

void ggml_flash_attn_ext_set_prec(
  ffi.Pointer<ggml_tensor> a,
  ggml_prec prec,
) => _ggml_flash_attn_ext_set_prec(
  a,
  prec.value,
);

@ffi.Native<ffi.UnsignedInt Function(ffi.Pointer<ggml_tensor>)>(
  symbol: 'ggml_flash_attn_ext_get_prec',
)
external int _ggml_flash_attn_ext_get_prec(
  ffi.Pointer<ggml_tensor> a,
);

ggml_prec ggml_flash_attn_ext_get_prec(
  ffi.Pointer<ggml_tensor> a,
) => ggml_prec.fromValue(
  _ggml_flash_attn_ext_get_prec(
    a,
  ),
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)
>()
external void ggml_flash_attn_ext_add_sinks(
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> sinks,
);

/// TODO: needs to be adapted to ggml_flash_attn_ext
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Bool,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_flash_attn_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> q,
  ffi.Pointer<ggml_tensor> k,
  ffi.Pointer<ggml_tensor> v,
  ffi.Pointer<ggml_tensor> d,
  bool masked,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_ssm_conv(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> sx,
  ffi.Pointer<ggml_tensor> c,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_ssm_scan(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> s,
  ffi.Pointer<ggml_tensor> x,
  ffi.Pointer<ggml_tensor> dt,
  ffi.Pointer<ggml_tensor> A,
  ffi.Pointer<ggml_tensor> B,
  ffi.Pointer<ggml_tensor> C,
  ffi.Pointer<ggml_tensor> ids,
);

/// partition into non-overlapping windows with padding if needed
/// example:
/// a:   768   64   64    1
/// w:    14
/// res: 768   14   14    25
/// used in sam
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_win_part(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int w,
);

/// reverse of ggml_win_part
/// used in sam
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_win_unpart(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int w0,
  int h0,
  int w,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.UnsignedInt,
  )
>(symbol: 'ggml_unary')
external ffi.Pointer<ggml_tensor> _ggml_unary(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int op,
);

ffi.Pointer<ggml_tensor> ggml_unary(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_unary_op op,
) => _ggml_unary(
  ctx,
  a,
  op.value,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.UnsignedInt,
  )
>(symbol: 'ggml_unary_inplace')
external ffi.Pointer<ggml_tensor> _ggml_unary_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int op,
);

ffi.Pointer<ggml_tensor> ggml_unary_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_unary_op op,
) => _ggml_unary_inplace(
  ctx,
  a,
  op.value,
);

/// used in sam
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_get_rel_pos(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  int qh,
  int kh,
);

/// used in sam
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_add_rel_pos(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> pw,
  ffi.Pointer<ggml_tensor> ph,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_add_rel_pos_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> pw,
  ffi.Pointer<ggml_tensor> ph,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_rwkv_wkv6(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> k,
  ffi.Pointer<ggml_tensor> v,
  ffi.Pointer<ggml_tensor> r,
  ffi.Pointer<ggml_tensor> tf,
  ffi.Pointer<ggml_tensor> td,
  ffi.Pointer<ggml_tensor> state,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Float,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_gated_linear_attn(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> k,
  ffi.Pointer<ggml_tensor> v,
  ffi.Pointer<ggml_tensor> q,
  ffi.Pointer<ggml_tensor> g,
  ffi.Pointer<ggml_tensor> state,
  double scale,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_rwkv_wkv7(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> r,
  ffi.Pointer<ggml_tensor> w,
  ffi.Pointer<ggml_tensor> k,
  ffi.Pointer<ggml_tensor> v,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> state,
);

/// Solves a specific equation of the form Ax=B, where A is a triangular matrix
/// without zeroes on the diagonal (i.e. invertible).
/// B can have any number of columns, but must have the same number of rows as A
/// If A is [n, n] and B is [n, m], then the result will be [n, m] as well
/// Has O(n^3) complexity (unlike most matrix ops out there), so use on cases
/// where n > 100 sparingly, pre-chunk if necessary.
///
/// If left = false, solves xA=B instead
/// If lower = false, assumes upper triangular instead
/// If uni = true, assumes diagonal of A to be all ones (will override actual values)
///
/// TODO: currently only lower, right, non-unitriangular variant is implemented
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Bool,
    ffi.Bool,
    ffi.Bool,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_solve_tri(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  bool left,
  bool lower,
  bool uni,
);

/// n_tasks == GGML_N_TASKS_MAX means to use max number of tasks
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ggml_custom1_op_t,
    ffi.Int,
    ffi.Pointer<ffi.Void>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_map_custom1(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_custom1_op_t fun,
  int n_tasks,
  ffi.Pointer<ffi.Void> userdata,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ggml_custom1_op_t,
    ffi.Int,
    ffi.Pointer<ffi.Void>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_map_custom1_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ggml_custom1_op_t fun,
  int n_tasks,
  ffi.Pointer<ffi.Void> userdata,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ggml_custom2_op_t,
    ffi.Int,
    ffi.Pointer<ffi.Void>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_map_custom2(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ggml_custom2_op_t fun,
  int n_tasks,
  ffi.Pointer<ffi.Void> userdata,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ggml_custom2_op_t,
    ffi.Int,
    ffi.Pointer<ffi.Void>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_map_custom2_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ggml_custom2_op_t fun,
  int n_tasks,
  ffi.Pointer<ffi.Void> userdata,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ggml_custom3_op_t,
    ffi.Int,
    ffi.Pointer<ffi.Void>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_map_custom3(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
  ggml_custom3_op_t fun,
  int n_tasks,
  ffi.Pointer<ffi.Void> userdata,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ggml_custom3_op_t,
    ffi.Int,
    ffi.Pointer<ffi.Void>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_map_custom3_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
  ggml_custom3_op_t fun,
  int n_tasks,
  ffi.Pointer<ffi.Void> userdata,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.UnsignedInt,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
    ffi.Pointer<ffi.Pointer<ggml_tensor>>,
    ffi.Int,
    ggml_custom_op_t,
    ffi.Int,
    ffi.Pointer<ffi.Void>,
  )
>(symbol: 'ggml_custom_4d')
external ffi.Pointer<ggml_tensor> _ggml_custom_4d(
  ffi.Pointer<ggml_context> ctx,
  int type,
  int ne0,
  int ne1,
  int ne2,
  int ne3,
  ffi.Pointer<ffi.Pointer<ggml_tensor>> args,
  int n_args,
  ggml_custom_op_t fun,
  int n_tasks,
  ffi.Pointer<ffi.Void> userdata,
);

ffi.Pointer<ggml_tensor> ggml_custom_4d(
  ffi.Pointer<ggml_context> ctx,
  ggml_type type,
  int ne0,
  int ne1,
  int ne2,
  int ne3,
  ffi.Pointer<ffi.Pointer<ggml_tensor>> args,
  int n_args,
  ggml_custom_op_t fun,
  int n_tasks,
  ffi.Pointer<ffi.Void> userdata,
) => _ggml_custom_4d(
  ctx,
  type.value,
  ne0,
  ne1,
  ne2,
  ne3,
  args,
  n_args,
  fun,
  n_tasks,
  userdata,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ffi.Pointer<ggml_tensor>>,
    ffi.Int,
    ggml_custom_op_t,
    ffi.Int,
    ffi.Pointer<ffi.Void>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_custom_inplace(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ffi.Pointer<ggml_tensor>> args,
  int n_args,
  ggml_custom_op_t fun,
  int n_tasks,
  ffi.Pointer<ffi.Void> userdata,
);

/// loss function
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_cross_entropy_loss(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_cross_entropy_loss_back(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> b,
  ffi.Pointer<ggml_tensor> c,
);

/// AdamW optimizer step
/// Paper: https://arxiv.org/pdf/1711.05101v3.pdf
/// PyTorch: https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_opt_step_adamw(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> grad,
  ffi.Pointer<ggml_tensor> m,
  ffi.Pointer<ggml_tensor> v,
  ffi.Pointer<ggml_tensor> adamw_params,
);

/// stochastic gradient descent step (with weight decay)
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_opt_step_sgd(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_tensor> a,
  ffi.Pointer<ggml_tensor> grad,
  ffi.Pointer<ggml_tensor> sgd_params,
);

/// automatic differentiation
@ffi.Native<
  ffi.Void Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_tensor>)
>()
external void ggml_build_forward_expand(
  ffi.Pointer<ggml_cgraph> cgraph,
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_cgraph>,
    ffi.Pointer<ffi.Pointer<ggml_tensor>>,
  )
>()
external void ggml_build_backward_expand(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_cgraph> cgraph,
  ffi.Pointer<ffi.Pointer<ggml_tensor>> grad_accs,
);

/// graph allocation in a context
@ffi.Native<ffi.Pointer<ggml_cgraph> Function(ffi.Pointer<ggml_context>)>()
external ffi.Pointer<ggml_cgraph> ggml_new_graph(
  ffi.Pointer<ggml_context> ctx,
);

@ffi.Native<
  ffi.Pointer<ggml_cgraph> Function(
    ffi.Pointer<ggml_context>,
    ffi.Size,
    ffi.Bool,
  )
>()
external ffi.Pointer<ggml_cgraph> ggml_new_graph_custom(
  ffi.Pointer<ggml_context> ctx,
  int size,
  bool grads,
);

@ffi.Native<
  ffi.Pointer<ggml_cgraph> Function(
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_cgraph>,
    ffi.Bool,
  )
>()
external ffi.Pointer<ggml_cgraph> ggml_graph_dup(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_cgraph> cgraph,
  bool force_grads,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_cgraph>)
>()
external void ggml_graph_cpy(
  ffi.Pointer<ggml_cgraph> src,
  ffi.Pointer<ggml_cgraph> dst,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_cgraph>)>()
external void ggml_graph_reset(
  ffi.Pointer<ggml_cgraph> cgraph,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_cgraph>)>()
external void ggml_graph_clear(
  ffi.Pointer<ggml_cgraph> cgraph,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ggml_cgraph>)>()
external int ggml_graph_size(
  ffi.Pointer<ggml_cgraph> cgraph,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_cgraph>, ffi.Int)
>()
external ffi.Pointer<ggml_tensor> ggml_graph_node(
  ffi.Pointer<ggml_cgraph> cgraph,
  int i,
);

@ffi.Native<
  ffi.Pointer<ffi.Pointer<ggml_tensor>> Function(ffi.Pointer<ggml_cgraph>)
>()
external ffi.Pointer<ffi.Pointer<ggml_tensor>> ggml_graph_nodes(
  ffi.Pointer<ggml_cgraph> cgraph,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ggml_cgraph>)>()
external int ggml_graph_n_nodes(
  ffi.Pointer<ggml_cgraph> cgraph,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_tensor>)
>()
external void ggml_graph_add_node(
  ffi.Pointer<ggml_cgraph> cgraph,
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Size Function()>()
external int ggml_graph_overhead();

@ffi.Native<ffi.Size Function(ffi.Size, ffi.Bool)>()
external int ggml_graph_overhead_custom(
  int size,
  bool grads,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_cgraph>,
    ffi.Pointer<ffi.Char>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_graph_get_tensor(
  ffi.Pointer<ggml_cgraph> cgraph,
  ffi.Pointer<ffi.Char> name,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_cgraph>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_graph_get_grad(
  ffi.Pointer<ggml_cgraph> cgraph,
  ffi.Pointer<ggml_tensor> node,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ffi.Pointer<ggml_cgraph>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_graph_get_grad_acc(
  ffi.Pointer<ggml_cgraph> cgraph,
  ffi.Pointer<ggml_tensor> node,
);

/// print info and performance information for the graph
@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_cgraph>)>()
external void ggml_graph_print(
  ffi.Pointer<ggml_cgraph> cgraph,
);

/// dump the graph into a file using the dot format
@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<ggml_cgraph>,
    ffi.Pointer<ggml_cgraph>,
    ffi.Pointer<ffi.Char>,
  )
>()
external void ggml_graph_dump_dot(
  ffi.Pointer<ggml_cgraph> gb,
  ffi.Pointer<ggml_cgraph> gf,
  ffi.Pointer<ffi.Char> filename,
);

/// Set callback for all future logging events.
/// If this is not called, or NULL is supplied, everything is output on stderr.
@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<ggml_log_callback>,
    ffi.Pointer<ffi.Pointer<ffi.Void>>,
  )
>()
external void ggml_log_get(
  ffi.Pointer<ggml_log_callback> log_callback,
  ffi.Pointer<ffi.Pointer<ffi.Void>> user_data,
);

@ffi.Native<ffi.Void Function(ggml_log_callback, ffi.Pointer<ffi.Void>)>()
external void ggml_log_set(
  ggml_log_callback log_callback,
  ffi.Pointer<ffi.Void> user_data,
);

@ffi.Native<ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_tensor>)>()
external ffi.Pointer<ggml_tensor> ggml_set_zero(
  ffi.Pointer<ggml_tensor> tensor,
);

/// - ggml_quantize_init can be called multiple times with the same type
/// it will only initialize the quantization tables for the first call or after ggml_quantize_free
/// automatically called by ggml_quantize_chunk for convenience
///
/// - ggml_quantize_free will free any memory allocated by ggml_quantize_init
/// call this at the end of the program to avoid memory leaks
///
/// note: these are thread-safe
@ffi.Native<ffi.Void Function(ffi.UnsignedInt)>(symbol: 'ggml_quantize_init')
external void _ggml_quantize_init(
  int type,
);

void ggml_quantize_init(
  ggml_type type,
) => _ggml_quantize_init(
  type.value,
);

@ffi.Native<ffi.Void Function()>()
external void ggml_quantize_free();

/// some quantization type cannot be used without an importance matrix
@ffi.Native<ffi.Bool Function(ffi.UnsignedInt)>(
  symbol: 'ggml_quantize_requires_imatrix',
)
external bool _ggml_quantize_requires_imatrix(
  int type,
);

bool ggml_quantize_requires_imatrix(
  ggml_type type,
) => _ggml_quantize_requires_imatrix(
  type.value,
);

/// calls ggml_quantize_init internally (i.e. can allocate memory)
@ffi.Native<
  ffi.Size Function(
    ffi.UnsignedInt,
    ffi.Pointer<ffi.Float>,
    ffi.Pointer<ffi.Void>,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
    ffi.Pointer<ffi.Float>,
  )
>(symbol: 'ggml_quantize_chunk')
external int _ggml_quantize_chunk(
  int type,
  ffi.Pointer<ffi.Float> src,
  ffi.Pointer<ffi.Void> dst,
  int start,
  int nrows,
  int n_per_row,
  ffi.Pointer<ffi.Float> imatrix,
);

int ggml_quantize_chunk(
  ggml_type type,
  ffi.Pointer<ffi.Float> src,
  ffi.Pointer<ffi.Void> dst,
  int start,
  int nrows,
  int n_per_row,
  ffi.Pointer<ffi.Float> imatrix,
) => _ggml_quantize_chunk(
  type.value,
  src,
  dst,
  start,
  nrows,
  n_per_row,
  imatrix,
);

@ffi.Native<ffi.Pointer<ggml_type_traits> Function(ffi.UnsignedInt)>(
  symbol: 'ggml_get_type_traits',
)
external ffi.Pointer<ggml_type_traits> _ggml_get_type_traits(
  int type,
);

ffi.Pointer<ggml_type_traits> ggml_get_type_traits(
  ggml_type type,
) => _ggml_get_type_traits(
  type.value,
);

@ffi.Native<ggml_threadpool_params Function(ffi.Int)>()
external ggml_threadpool_params ggml_threadpool_params_default(
  int n_threads,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_threadpool_params>, ffi.Int)>()
external void ggml_threadpool_params_init(
  ffi.Pointer<ggml_threadpool_params> p,
  int n_threads,
);

@ffi.Native<
  ffi.Bool Function(
    ffi.Pointer<ggml_threadpool_params>,
    ffi.Pointer<ggml_threadpool_params>,
  )
>()
external bool ggml_threadpool_params_match(
  ffi.Pointer<ggml_threadpool_params> p0,
  ffi.Pointer<ggml_threadpool_params> p1,
);

@ffi.Native<ggml_tallocr Function(ggml_backend_buffer_t)>()
external ggml_tallocr ggml_tallocr_new(
  ggml_backend_buffer_t buffer,
);

@ffi.Native<
  ffi.Int Function(ffi.Pointer<ggml_tallocr>, ffi.Pointer<ggml_tensor>)
>(symbol: 'ggml_tallocr_alloc')
external int _ggml_tallocr_alloc(
  ffi.Pointer<ggml_tallocr> talloc,
  ffi.Pointer<ggml_tensor> tensor,
);

ggml_status ggml_tallocr_alloc(
  ffi.Pointer<ggml_tallocr> talloc,
  ffi.Pointer<ggml_tensor> tensor,
) => ggml_status.fromValue(
  _ggml_tallocr_alloc(
    talloc,
    tensor,
  ),
);

@ffi.Native<ggml_gallocr_t Function(ggml_backend_buffer_type_t)>()
external ggml_gallocr_t ggml_gallocr_new(
  ggml_backend_buffer_type_t buft,
);

@ffi.Native<
  ggml_gallocr_t Function(ffi.Pointer<ggml_backend_buffer_type_t>, ffi.Int)
>()
external ggml_gallocr_t ggml_gallocr_new_n(
  ffi.Pointer<ggml_backend_buffer_type_t> bufts,
  int n_bufs,
);

@ffi.Native<ffi.Void Function(ggml_gallocr_t)>()
external void ggml_gallocr_free(
  ggml_gallocr_t galloc,
);

/// pre-allocate buffers from a measure graph - does not allocate or modify the graph
/// call with a worst-case graph to avoid buffer reallocations
/// not strictly required for single buffer usage: ggml_gallocr_alloc_graph will reallocate the buffers automatically if needed
/// returns false if the buffer allocation failed
/// ggml_gallocr_resrve_n_size writes the buffer sizes per galloc buffer that would be allocated by ggml_gallocr_reserve_n to sizes
@ffi.Native<ffi.Bool Function(ggml_gallocr_t, ffi.Pointer<ggml_cgraph>)>()
external bool ggml_gallocr_reserve(
  ggml_gallocr_t galloc,
  ffi.Pointer<ggml_cgraph> graph,
);

@ffi.Native<
  ffi.Void Function(
    ggml_gallocr_t,
    ffi.Pointer<ggml_cgraph>,
    ffi.Pointer<ffi.Int>,
    ffi.Pointer<ffi.Int>,
    ffi.Pointer<ffi.Size>,
  )
>()
external void ggml_gallocr_reserve_n_size(
  ggml_gallocr_t galloc,
  ffi.Pointer<ggml_cgraph> graph,
  ffi.Pointer<ffi.Int> node_buffer_ids,
  ffi.Pointer<ffi.Int> leaf_buffer_ids,
  ffi.Pointer<ffi.Size> sizes,
);

@ffi.Native<
  ffi.Bool Function(
    ggml_gallocr_t,
    ffi.Pointer<ggml_cgraph>,
    ffi.Pointer<ffi.Int>,
    ffi.Pointer<ffi.Int>,
  )
>()
external bool ggml_gallocr_reserve_n(
  ggml_gallocr_t galloc,
  ffi.Pointer<ggml_cgraph> graph,
  ffi.Pointer<ffi.Int> node_buffer_ids,
  ffi.Pointer<ffi.Int> leaf_buffer_ids,
);

/// automatic reallocation if the topology changes when using a single buffer
/// returns false if using multiple buffers and a re-allocation is needed (call ggml_gallocr_reserve_n first to set the node buffers)
@ffi.Native<ffi.Bool Function(ggml_gallocr_t, ffi.Pointer<ggml_cgraph>)>()
external bool ggml_gallocr_alloc_graph(
  ggml_gallocr_t galloc,
  ffi.Pointer<ggml_cgraph> graph,
);

@ffi.Native<ffi.Size Function(ggml_gallocr_t, ffi.Int)>()
external int ggml_gallocr_get_buffer_size(
  ggml_gallocr_t galloc,
  int buffer_id,
);

/// Utils
/// Create a buffer and allocate all the tensors in a ggml_context
/// ggml_backend_alloc_ctx_tensors_from_buft_size returns the size of the buffer that would be allocated by ggml_backend_alloc_ctx_tensors_from_buft
@ffi.Native<
  ffi.Size Function(ffi.Pointer<ggml_context>, ggml_backend_buffer_type_t)
>()
external int ggml_backend_alloc_ctx_tensors_from_buft_size(
  ffi.Pointer<ggml_context> ctx,
  ggml_backend_buffer_type_t buft,
);

@ffi.Native<
  ffi.Pointer<ggml_backend_buffer> Function(
    ffi.Pointer<ggml_context>,
    ggml_backend_buffer_type_t,
  )
>()
external ffi.Pointer<ggml_backend_buffer>
ggml_backend_alloc_ctx_tensors_from_buft(
  ffi.Pointer<ggml_context> ctx,
  ggml_backend_buffer_type_t buft,
);

@ffi.Native<
  ffi.Pointer<ggml_backend_buffer> Function(
    ffi.Pointer<ggml_context>,
    ggml_backend_t,
  )
>()
external ffi.Pointer<ggml_backend_buffer> ggml_backend_alloc_ctx_tensors(
  ffi.Pointer<ggml_context> ctx,
  ggml_backend_t backend,
);

/// Backend buffer type
@ffi.Native<ffi.Pointer<ffi.Char> Function(ggml_backend_buffer_type_t)>()
external ffi.Pointer<ffi.Char> ggml_backend_buft_name(
  ggml_backend_buffer_type_t buft,
);

@ffi.Native<
  ggml_backend_buffer_t Function(ggml_backend_buffer_type_t, ffi.Size)
>()
external ggml_backend_buffer_t ggml_backend_buft_alloc_buffer(
  ggml_backend_buffer_type_t buft,
  int size,
);

@ffi.Native<ffi.Size Function(ggml_backend_buffer_type_t)>()
external int ggml_backend_buft_get_alignment(
  ggml_backend_buffer_type_t buft,
);

@ffi.Native<ffi.Size Function(ggml_backend_buffer_type_t)>()
external int ggml_backend_buft_get_max_size(
  ggml_backend_buffer_type_t buft,
);

@ffi.Native<
  ffi.Size Function(ggml_backend_buffer_type_t, ffi.Pointer<ggml_tensor>)
>()
external int ggml_backend_buft_get_alloc_size(
  ggml_backend_buffer_type_t buft,
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Bool Function(ggml_backend_buffer_type_t)>()
external bool ggml_backend_buft_is_host(
  ggml_backend_buffer_type_t buft,
);

@ffi.Native<ggml_backend_dev_t Function(ggml_backend_buffer_type_t)>()
external ggml_backend_dev_t ggml_backend_buft_get_device(
  ggml_backend_buffer_type_t buft,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ggml_backend_buffer_t)>()
external ffi.Pointer<ffi.Char> ggml_backend_buffer_name(
  ggml_backend_buffer_t buffer,
);

@ffi.Native<ffi.Void Function(ggml_backend_buffer_t)>()
external void ggml_backend_buffer_free(
  ggml_backend_buffer_t buffer,
);

@ffi.Native<ffi.Pointer<ffi.Void> Function(ggml_backend_buffer_t)>()
external ffi.Pointer<ffi.Void> ggml_backend_buffer_get_base(
  ggml_backend_buffer_t buffer,
);

@ffi.Native<ffi.Size Function(ggml_backend_buffer_t)>()
external int ggml_backend_buffer_get_size(
  ggml_backend_buffer_t buffer,
);

@ffi.Native<ffi.Int Function(ggml_backend_buffer_t, ffi.Pointer<ggml_tensor>)>(
  symbol: 'ggml_backend_buffer_init_tensor',
)
external int _ggml_backend_buffer_init_tensor(
  ggml_backend_buffer_t buffer,
  ffi.Pointer<ggml_tensor> tensor,
);

ggml_status ggml_backend_buffer_init_tensor(
  ggml_backend_buffer_t buffer,
  ffi.Pointer<ggml_tensor> tensor,
) => ggml_status.fromValue(
  _ggml_backend_buffer_init_tensor(
    buffer,
    tensor,
  ),
);

@ffi.Native<ffi.Size Function(ggml_backend_buffer_t)>()
external int ggml_backend_buffer_get_alignment(
  ggml_backend_buffer_t buffer,
);

@ffi.Native<ffi.Size Function(ggml_backend_buffer_t)>()
external int ggml_backend_buffer_get_max_size(
  ggml_backend_buffer_t buffer,
);

@ffi.Native<
  ffi.Size Function(ggml_backend_buffer_t, ffi.Pointer<ggml_tensor>)
>()
external int ggml_backend_buffer_get_alloc_size(
  ggml_backend_buffer_t buffer,
  ffi.Pointer<ggml_tensor> tensor,
);

@ffi.Native<ffi.Void Function(ggml_backend_buffer_t, ffi.Uint8)>()
external void ggml_backend_buffer_clear(
  ggml_backend_buffer_t buffer,
  int value,
);

@ffi.Native<ffi.Bool Function(ggml_backend_buffer_t)>()
external bool ggml_backend_buffer_is_host(
  ggml_backend_buffer_t buffer,
);

@ffi.Native<ffi.Void Function(ggml_backend_buffer_t, ffi.UnsignedInt)>(
  symbol: 'ggml_backend_buffer_set_usage',
)
external void _ggml_backend_buffer_set_usage(
  ggml_backend_buffer_t buffer,
  int usage,
);

void ggml_backend_buffer_set_usage(
  ggml_backend_buffer_t buffer,
  ggml_backend_buffer_usage usage,
) => _ggml_backend_buffer_set_usage(
  buffer,
  usage.value,
);

@ffi.Native<ffi.UnsignedInt Function(ggml_backend_buffer_t)>(
  symbol: 'ggml_backend_buffer_get_usage',
)
external int _ggml_backend_buffer_get_usage(
  ggml_backend_buffer_t buffer,
);

ggml_backend_buffer_usage ggml_backend_buffer_get_usage(
  ggml_backend_buffer_t buffer,
) => ggml_backend_buffer_usage.fromValue(
  _ggml_backend_buffer_get_usage(
    buffer,
  ),
);

@ffi.Native<ggml_backend_buffer_type_t Function(ggml_backend_buffer_t)>()
external ggml_backend_buffer_type_t ggml_backend_buffer_get_type(
  ggml_backend_buffer_t buffer,
);

@ffi.Native<ffi.Void Function(ggml_backend_buffer_t)>()
external void ggml_backend_buffer_reset(
  ggml_backend_buffer_t buffer,
);

/// tensor copy between different backends
@ffi.Native<
  ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ggml_tensor>)
>()
external void ggml_backend_tensor_copy(
  ffi.Pointer<ggml_tensor> src,
  ffi.Pointer<ggml_tensor> dst,
);

/// Backend (stream)
@ffi.Native<ggml_guid_t Function(ggml_backend_t)>()
external ggml_guid_t ggml_backend_guid(
  ggml_backend_t backend,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ggml_backend_t)>()
external ffi.Pointer<ffi.Char> ggml_backend_name(
  ggml_backend_t backend,
);

@ffi.Native<ffi.Void Function(ggml_backend_t)>()
external void ggml_backend_free(
  ggml_backend_t backend,
);

@ffi.Native<ggml_backend_buffer_type_t Function(ggml_backend_t)>()
external ggml_backend_buffer_type_t ggml_backend_get_default_buffer_type(
  ggml_backend_t backend,
);

@ffi.Native<ggml_backend_buffer_t Function(ggml_backend_t, ffi.Size)>()
external ggml_backend_buffer_t ggml_backend_alloc_buffer(
  ggml_backend_t backend,
  int size,
);

@ffi.Native<ffi.Size Function(ggml_backend_t)>()
external int ggml_backend_get_alignment(
  ggml_backend_t backend,
);

@ffi.Native<ffi.Size Function(ggml_backend_t)>()
external int ggml_backend_get_max_size(
  ggml_backend_t backend,
);

@ffi.Native<
  ffi.Void Function(
    ggml_backend_t,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ffi.Void>,
    ffi.Size,
    ffi.Size,
  )
>()
external void ggml_backend_tensor_set_async(
  ggml_backend_t backend,
  ffi.Pointer<ggml_tensor> tensor,
  ffi.Pointer<ffi.Void> data,
  int offset,
  int size,
);

@ffi.Native<
  ffi.Void Function(
    ggml_backend_t,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ffi.Void>,
    ffi.Size,
    ffi.Size,
  )
>()
external void ggml_backend_tensor_get_async(
  ggml_backend_t backend,
  ffi.Pointer<ggml_tensor> tensor,
  ffi.Pointer<ffi.Void> data,
  int offset,
  int size,
);

/// "offset" refers to the offset in tensor->data for setting/getting data
@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ffi.Void>,
    ffi.Size,
    ffi.Size,
  )
>()
external void ggml_backend_tensor_set(
  ffi.Pointer<ggml_tensor> tensor,
  ffi.Pointer<ffi.Void> data,
  int offset,
  int size,
);

@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ffi.Void>,
    ffi.Size,
    ffi.Size,
  )
>()
external void ggml_backend_tensor_get(
  ffi.Pointer<ggml_tensor> tensor,
  ffi.Pointer<ffi.Void> data,
  int offset,
  int size,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Uint8, ffi.Size, ffi.Size)
>()
external void ggml_backend_tensor_memset(
  ffi.Pointer<ggml_tensor> tensor,
  int value,
  int offset,
  int size,
);

@ffi.Native<ffi.Void Function(ggml_backend_t)>()
external void ggml_backend_synchronize(
  ggml_backend_t backend,
);

@ffi.Native<
  ggml_backend_graph_plan_t Function(ggml_backend_t, ffi.Pointer<ggml_cgraph>)
>()
external ggml_backend_graph_plan_t ggml_backend_graph_plan_create(
  ggml_backend_t backend,
  ffi.Pointer<ggml_cgraph> cgraph,
);

@ffi.Native<ffi.Void Function(ggml_backend_t, ggml_backend_graph_plan_t)>()
external void ggml_backend_graph_plan_free(
  ggml_backend_t backend,
  ggml_backend_graph_plan_t plan,
);

@ffi.Native<ffi.Int Function(ggml_backend_t, ggml_backend_graph_plan_t)>(
  symbol: 'ggml_backend_graph_plan_compute',
)
external int _ggml_backend_graph_plan_compute(
  ggml_backend_t backend,
  ggml_backend_graph_plan_t plan,
);

ggml_status ggml_backend_graph_plan_compute(
  ggml_backend_t backend,
  ggml_backend_graph_plan_t plan,
) => ggml_status.fromValue(
  _ggml_backend_graph_plan_compute(
    backend,
    plan,
  ),
);

@ffi.Native<ffi.Int Function(ggml_backend_t, ffi.Pointer<ggml_cgraph>)>(
  symbol: 'ggml_backend_graph_compute',
)
external int _ggml_backend_graph_compute(
  ggml_backend_t backend,
  ffi.Pointer<ggml_cgraph> cgraph,
);

ggml_status ggml_backend_graph_compute(
  ggml_backend_t backend,
  ffi.Pointer<ggml_cgraph> cgraph,
) => ggml_status.fromValue(
  _ggml_backend_graph_compute(
    backend,
    cgraph,
  ),
);

@ffi.Native<ffi.Int Function(ggml_backend_t, ffi.Pointer<ggml_cgraph>)>(
  symbol: 'ggml_backend_graph_compute_async',
)
external int _ggml_backend_graph_compute_async(
  ggml_backend_t backend,
  ffi.Pointer<ggml_cgraph> cgraph,
);

ggml_status ggml_backend_graph_compute_async(
  ggml_backend_t backend,
  ffi.Pointer<ggml_cgraph> cgraph,
) => ggml_status.fromValue(
  _ggml_backend_graph_compute_async(
    backend,
    cgraph,
  ),
);

/// NOTE: will be removed, use device version instead
@ffi.Native<ffi.Bool Function(ggml_backend_t, ffi.Pointer<ggml_tensor>)>()
external bool ggml_backend_supports_op(
  ggml_backend_t backend,
  ffi.Pointer<ggml_tensor> op,
);

@ffi.Native<ffi.Bool Function(ggml_backend_t, ggml_backend_buffer_type_t)>()
external bool ggml_backend_supports_buft(
  ggml_backend_t backend,
  ggml_backend_buffer_type_t buft,
);

@ffi.Native<ffi.Bool Function(ggml_backend_t, ffi.Pointer<ggml_tensor>)>()
external bool ggml_backend_offload_op(
  ggml_backend_t backend,
  ffi.Pointer<ggml_tensor> op,
);

/// asynchronous copy
/// the copy is performed after all the currently queued operations in backend_src
/// backend_dst will wait for the copy to complete before performing other operations
/// automatic fallback to sync copy if async is not supported
@ffi.Native<
  ffi.Void Function(
    ggml_backend_t,
    ggml_backend_t,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external void ggml_backend_tensor_copy_async(
  ggml_backend_t backend_src,
  ggml_backend_t backend_dst,
  ffi.Pointer<ggml_tensor> src,
  ffi.Pointer<ggml_tensor> dst,
);

@ffi.Native<ggml_backend_dev_t Function(ggml_backend_t)>()
external ggml_backend_dev_t ggml_backend_get_device(
  ggml_backend_t backend,
);

/// Events
@ffi.Native<ggml_backend_event_t Function(ggml_backend_dev_t)>()
external ggml_backend_event_t ggml_backend_event_new(
  ggml_backend_dev_t device,
);

@ffi.Native<ffi.Void Function(ggml_backend_event_t)>()
external void ggml_backend_event_free(
  ggml_backend_event_t event,
);

@ffi.Native<ffi.Void Function(ggml_backend_event_t, ggml_backend_t)>()
external void ggml_backend_event_record(
  ggml_backend_event_t event,
  ggml_backend_t backend,
);

@ffi.Native<ffi.Void Function(ggml_backend_event_t)>()
external void ggml_backend_event_synchronize(
  ggml_backend_event_t event,
);

@ffi.Native<ffi.Void Function(ggml_backend_t, ggml_backend_event_t)>()
external void ggml_backend_event_wait(
  ggml_backend_t backend,
  ggml_backend_event_t event,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ggml_backend_dev_t)>()
external ffi.Pointer<ffi.Char> ggml_backend_dev_name(
  ggml_backend_dev_t device,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ggml_backend_dev_t)>()
external ffi.Pointer<ffi.Char> ggml_backend_dev_description(
  ggml_backend_dev_t device,
);

@ffi.Native<
  ffi.Void Function(
    ggml_backend_dev_t,
    ffi.Pointer<ffi.Size>,
    ffi.Pointer<ffi.Size>,
  )
>()
external void ggml_backend_dev_memory(
  ggml_backend_dev_t device,
  ffi.Pointer<ffi.Size> free,
  ffi.Pointer<ffi.Size> total,
);

@ffi.Native<ffi.UnsignedInt Function(ggml_backend_dev_t)>(
  symbol: 'ggml_backend_dev_type',
)
external int _ggml_backend_dev_type$1(
  ggml_backend_dev_t device,
);

ggml_backend_dev_type ggml_backend_dev_type$1(
  ggml_backend_dev_t device,
) => ggml_backend_dev_type.fromValue(
  _ggml_backend_dev_type$1(
    device,
  ),
);

@ffi.Native<
  ffi.Void Function(ggml_backend_dev_t, ffi.Pointer<ggml_backend_dev_props>)
>()
external void ggml_backend_dev_get_props(
  ggml_backend_dev_t device,
  ffi.Pointer<ggml_backend_dev_props> props,
);

@ffi.Native<ggml_backend_reg_t Function(ggml_backend_dev_t)>()
external ggml_backend_reg_t ggml_backend_dev_backend_reg(
  ggml_backend_dev_t device,
);

@ffi.Native<
  ggml_backend_t Function(ggml_backend_dev_t, ffi.Pointer<ffi.Char>)
>()
external ggml_backend_t ggml_backend_dev_init(
  ggml_backend_dev_t device,
  ffi.Pointer<ffi.Char> params,
);

@ffi.Native<ggml_backend_buffer_type_t Function(ggml_backend_dev_t)>()
external ggml_backend_buffer_type_t ggml_backend_dev_buffer_type(
  ggml_backend_dev_t device,
);

@ffi.Native<ggml_backend_buffer_type_t Function(ggml_backend_dev_t)>()
external ggml_backend_buffer_type_t ggml_backend_dev_host_buffer_type(
  ggml_backend_dev_t device,
);

@ffi.Native<
  ggml_backend_buffer_t Function(
    ggml_backend_dev_t,
    ffi.Pointer<ffi.Void>,
    ffi.Size,
    ffi.Size,
  )
>()
external ggml_backend_buffer_t ggml_backend_dev_buffer_from_host_ptr(
  ggml_backend_dev_t device,
  ffi.Pointer<ffi.Void> ptr,
  int size,
  int max_tensor_size,
);

@ffi.Native<ffi.Bool Function(ggml_backend_dev_t, ffi.Pointer<ggml_tensor>)>()
external bool ggml_backend_dev_supports_op(
  ggml_backend_dev_t device,
  ffi.Pointer<ggml_tensor> op,
);

@ffi.Native<ffi.Bool Function(ggml_backend_dev_t, ggml_backend_buffer_type_t)>()
external bool ggml_backend_dev_supports_buft(
  ggml_backend_dev_t device,
  ggml_backend_buffer_type_t buft,
);

@ffi.Native<ffi.Bool Function(ggml_backend_dev_t, ffi.Pointer<ggml_tensor>)>()
external bool ggml_backend_dev_offload_op(
  ggml_backend_dev_t device,
  ffi.Pointer<ggml_tensor> op,
);

/// Backend (reg)
@ffi.Native<ffi.Pointer<ffi.Char> Function(ggml_backend_reg_t)>()
external ffi.Pointer<ffi.Char> ggml_backend_reg_name(
  ggml_backend_reg_t reg,
);

@ffi.Native<ffi.Size Function(ggml_backend_reg_t)>()
external int ggml_backend_reg_dev_count(
  ggml_backend_reg_t reg,
);

@ffi.Native<ggml_backend_dev_t Function(ggml_backend_reg_t, ffi.Size)>()
external ggml_backend_dev_t ggml_backend_reg_dev_get(
  ggml_backend_reg_t reg,
  int index,
);

@ffi.Native<
  ffi.Pointer<ffi.Void> Function(ggml_backend_reg_t, ffi.Pointer<ffi.Char>)
>()
external ffi.Pointer<ffi.Void> ggml_backend_reg_get_proc_address(
  ggml_backend_reg_t reg,
  ffi.Pointer<ffi.Char> name,
);

/// Backend registry
@ffi.Native<ffi.Void Function(ggml_backend_reg_t)>()
external void ggml_backend_register(
  ggml_backend_reg_t reg,
);

@ffi.Native<ffi.Void Function(ggml_backend_dev_t)>()
external void ggml_backend_device_register(
  ggml_backend_dev_t device,
);

/// Backend (reg) enumeration
@ffi.Native<ffi.Size Function()>()
external int ggml_backend_reg_count();

@ffi.Native<ggml_backend_reg_t Function(ffi.Size)>()
external ggml_backend_reg_t ggml_backend_reg_get(
  int index,
);

@ffi.Native<ggml_backend_reg_t Function(ffi.Pointer<ffi.Char>)>()
external ggml_backend_reg_t ggml_backend_reg_by_name(
  ffi.Pointer<ffi.Char> name,
);

/// Device enumeration
@ffi.Native<ffi.Size Function()>()
external int ggml_backend_dev_count();

@ffi.Native<ggml_backend_dev_t Function(ffi.Size)>()
external ggml_backend_dev_t ggml_backend_dev_get(
  int index,
);

@ffi.Native<ggml_backend_dev_t Function(ffi.Pointer<ffi.Char>)>()
external ggml_backend_dev_t ggml_backend_dev_by_name(
  ffi.Pointer<ffi.Char> name,
);

@ffi.Native<ggml_backend_dev_t Function(ffi.UnsignedInt)>(
  symbol: 'ggml_backend_dev_by_type',
)
external ggml_backend_dev_t _ggml_backend_dev_by_type(
  int type,
);

ggml_backend_dev_t ggml_backend_dev_by_type(
  ggml_backend_dev_type type,
) => _ggml_backend_dev_by_type(
  type.value,
);

/// Direct backend (stream) initialization
/// = ggml_backend_dev_init(ggml_backend_dev_by_name(name), params)
@ffi.Native<
  ggml_backend_t Function(ffi.Pointer<ffi.Char>, ffi.Pointer<ffi.Char>)
>()
external ggml_backend_t ggml_backend_init_by_name(
  ffi.Pointer<ffi.Char> name,
  ffi.Pointer<ffi.Char> params,
);

/// = ggml_backend_dev_init(ggml_backend_dev_by_type(type), params)
@ffi.Native<ggml_backend_t Function(ffi.UnsignedInt, ffi.Pointer<ffi.Char>)>(
  symbol: 'ggml_backend_init_by_type',
)
external ggml_backend_t _ggml_backend_init_by_type(
  int type,
  ffi.Pointer<ffi.Char> params,
);

ggml_backend_t ggml_backend_init_by_type(
  ggml_backend_dev_type type,
  ffi.Pointer<ffi.Char> params,
) => _ggml_backend_init_by_type(
  type.value,
  params,
);

/// = ggml_backend_dev_init(ggml_backend_dev_by_type(GPU) OR ggml_backend_dev_by_type(CPU), NULL)
@ffi.Native<ggml_backend_t Function()>()
external ggml_backend_t ggml_backend_init_best();

/// Load a backend from a dynamic library and register it
@ffi.Native<ggml_backend_reg_t Function(ffi.Pointer<ffi.Char>)>()
external ggml_backend_reg_t ggml_backend_load(
  ffi.Pointer<ffi.Char> path,
);

/// Unload a backend if loaded dynamically and unregister it
@ffi.Native<ffi.Void Function(ggml_backend_reg_t)>()
external void ggml_backend_unload(
  ggml_backend_reg_t reg,
);

/// Load all known backends from dynamic libraries
@ffi.Native<ffi.Void Function()>()
external void ggml_backend_load_all();

@ffi.Native<ffi.Void Function(ffi.Pointer<ffi.Char>)>()
external void ggml_backend_load_all_from_path(
  ffi.Pointer<ffi.Char> dir_path,
);

/// Initialize a backend scheduler, backends with low index are given priority over backends with high index
@ffi.Native<
  ggml_backend_sched_t Function(
    ffi.Pointer<ggml_backend_t>,
    ffi.Pointer<ggml_backend_buffer_type_t>,
    ffi.Int,
    ffi.Size,
    ffi.Bool,
    ffi.Bool,
  )
>()
external ggml_backend_sched_t ggml_backend_sched_new(
  ffi.Pointer<ggml_backend_t> backends,
  ffi.Pointer<ggml_backend_buffer_type_t> bufts,
  int n_backends,
  int graph_size,
  bool parallel,
  bool op_offload,
);

@ffi.Native<ffi.Void Function(ggml_backend_sched_t)>()
external void ggml_backend_sched_free(
  ggml_backend_sched_t sched,
);

/// Initialize backend buffers from a measure graph
@ffi.Native<
  ffi.Void Function(
    ggml_backend_sched_t,
    ffi.Pointer<ggml_cgraph>,
    ffi.Pointer<ffi.Size>,
  )
>()
external void ggml_backend_sched_reserve_size(
  ggml_backend_sched_t sched,
  ffi.Pointer<ggml_cgraph> measure_graph,
  ffi.Pointer<ffi.Size> sizes,
);

@ffi.Native<ffi.Bool Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)>()
external bool ggml_backend_sched_reserve(
  ggml_backend_sched_t sched,
  ffi.Pointer<ggml_cgraph> measure_graph,
);

@ffi.Native<ffi.Int Function(ggml_backend_sched_t)>()
external int ggml_backend_sched_get_n_backends(
  ggml_backend_sched_t sched,
);

@ffi.Native<ggml_backend_t Function(ggml_backend_sched_t, ffi.Int)>()
external ggml_backend_t ggml_backend_sched_get_backend(
  ggml_backend_sched_t sched,
  int i,
);

/// Get the number of splits of the last graph
@ffi.Native<ffi.Int Function(ggml_backend_sched_t)>()
external int ggml_backend_sched_get_n_splits(
  ggml_backend_sched_t sched,
);

@ffi.Native<ffi.Int Function(ggml_backend_sched_t)>()
external int ggml_backend_sched_get_n_copies(
  ggml_backend_sched_t sched,
);

@ffi.Native<
  ggml_backend_buffer_type_t Function(ggml_backend_sched_t, ggml_backend_t)
>()
external ggml_backend_buffer_type_t ggml_backend_sched_get_buffer_type(
  ggml_backend_sched_t sched,
  ggml_backend_t backend,
);

@ffi.Native<ffi.Size Function(ggml_backend_sched_t, ggml_backend_t)>()
external int ggml_backend_sched_get_buffer_size(
  ggml_backend_sched_t sched,
  ggml_backend_t backend,
);

@ffi.Native<
  ffi.Void Function(
    ggml_backend_sched_t,
    ffi.Pointer<ggml_tensor>,
    ggml_backend_t,
  )
>()
external void ggml_backend_sched_set_tensor_backend(
  ggml_backend_sched_t sched,
  ffi.Pointer<ggml_tensor> node,
  ggml_backend_t backend,
);

@ffi.Native<
  ggml_backend_t Function(ggml_backend_sched_t, ffi.Pointer<ggml_tensor>)
>()
external ggml_backend_t ggml_backend_sched_get_tensor_backend(
  ggml_backend_sched_t sched,
  ffi.Pointer<ggml_tensor> node,
);

/// Split graph without allocating it
@ffi.Native<ffi.Void Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)>()
external void ggml_backend_sched_split_graph(
  ggml_backend_sched_t sched,
  ffi.Pointer<ggml_cgraph> graph,
);

/// Allocate and compute graph on the backend scheduler
@ffi.Native<ffi.Bool Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)>()
external bool ggml_backend_sched_alloc_graph(
  ggml_backend_sched_t sched,
  ffi.Pointer<ggml_cgraph> graph,
);

@ffi.Native<ffi.Int Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)>(
  symbol: 'ggml_backend_sched_graph_compute',
)
external int _ggml_backend_sched_graph_compute(
  ggml_backend_sched_t sched,
  ffi.Pointer<ggml_cgraph> graph,
);

ggml_status ggml_backend_sched_graph_compute(
  ggml_backend_sched_t sched,
  ffi.Pointer<ggml_cgraph> graph,
) => ggml_status.fromValue(
  _ggml_backend_sched_graph_compute(
    sched,
    graph,
  ),
);

@ffi.Native<ffi.Int Function(ggml_backend_sched_t, ffi.Pointer<ggml_cgraph>)>(
  symbol: 'ggml_backend_sched_graph_compute_async',
)
external int _ggml_backend_sched_graph_compute_async(
  ggml_backend_sched_t sched,
  ffi.Pointer<ggml_cgraph> graph,
);

ggml_status ggml_backend_sched_graph_compute_async(
  ggml_backend_sched_t sched,
  ffi.Pointer<ggml_cgraph> graph,
) => ggml_status.fromValue(
  _ggml_backend_sched_graph_compute_async(
    sched,
    graph,
  ),
);

@ffi.Native<ffi.Void Function(ggml_backend_sched_t)>()
external void ggml_backend_sched_synchronize(
  ggml_backend_sched_t sched,
);

/// Reset all assignments and allocators - must be called before changing the node backends or allocating a new graph.
/// This in effect deallocates all tensors that were previously allocated and leaves them with dangling pointers.
/// The correct way to use this API is to discard the deallocated tensors and create new ones.
@ffi.Native<ffi.Void Function(ggml_backend_sched_t)>()
external void ggml_backend_sched_reset(
  ggml_backend_sched_t sched,
);

/// Set a callback to be called for each resulting node during graph compute
@ffi.Native<
  ffi.Void Function(
    ggml_backend_sched_t,
    ggml_backend_sched_eval_callback,
    ffi.Pointer<ffi.Void>,
  )
>()
external void ggml_backend_sched_set_eval_callback(
  ggml_backend_sched_t sched,
  ggml_backend_sched_eval_callback callback,
  ffi.Pointer<ffi.Void> user_data,
);

/// Copy a graph to a different backend
@ffi.Native<
  ggml_backend_graph_copy$1 Function(ggml_backend_t, ffi.Pointer<ggml_cgraph>)
>()
external ggml_backend_graph_copy$1 ggml_backend_graph_copy(
  ggml_backend_t backend,
  ffi.Pointer<ggml_cgraph> graph,
);

@ffi.Native<ffi.Void Function(ggml_backend_graph_copy$1)>()
external void ggml_backend_graph_copy_free(
  ggml_backend_graph_copy$1 copy,
);

/// Compare the output of two backends
@ffi.Native<
  ffi.Bool Function(
    ggml_backend_t,
    ggml_backend_t,
    ffi.Pointer<ggml_cgraph>,
    ggml_backend_eval_callback,
    ffi.Pointer<ffi.Void>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external bool ggml_backend_compare_graph_backend(
  ggml_backend_t backend1,
  ggml_backend_t backend2,
  ffi.Pointer<ggml_cgraph> graph,
  ggml_backend_eval_callback callback,
  ffi.Pointer<ffi.Void> user_data,
  ffi.Pointer<ggml_tensor> test_node,
);

/// Tensor initialization
@ffi.Native<
  ffi.Int Function(
    ggml_backend_buffer_t,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ffi.Void>,
  )
>(symbol: 'ggml_backend_tensor_alloc')
external int _ggml_backend_tensor_alloc(
  ggml_backend_buffer_t buffer,
  ffi.Pointer<ggml_tensor> tensor,
  ffi.Pointer<ffi.Void> addr,
);

ggml_status ggml_backend_tensor_alloc(
  ggml_backend_buffer_t buffer,
  ffi.Pointer<ggml_tensor> tensor,
  ffi.Pointer<ffi.Void> addr,
) => ggml_status.fromValue(
  _ggml_backend_tensor_alloc(
    buffer,
    tensor,
    addr,
  ),
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ggml_tensor>)>(
  symbol: 'ggml_backend_view_init',
)
external int _ggml_backend_view_init(
  ffi.Pointer<ggml_tensor> tensor,
);

ggml_status ggml_backend_view_init(
  ffi.Pointer<ggml_tensor> tensor,
) => ggml_status.fromValue(
  _ggml_backend_view_init(
    tensor,
  ),
);

/// CPU buffer types are always available
@ffi.Native<ggml_backend_buffer_t Function(ffi.Pointer<ffi.Void>, ffi.Size)>()
external ggml_backend_buffer_t ggml_backend_cpu_buffer_from_ptr(
  ffi.Pointer<ffi.Void> ptr,
  int size,
);

@ffi.Native<ggml_backend_buffer_type_t Function()>()
external ggml_backend_buffer_type_t ggml_backend_cpu_buffer_type();

@ffi.Native<ffi.Void Function(ffi.UnsignedInt)>(symbol: 'ggml_numa_init')
external void _ggml_numa_init(
  int numa,
);

void ggml_numa_init(
  ggml_numa_strategy numa,
) => _ggml_numa_init(
  numa.value,
);

@ffi.Native<ffi.Bool Function()>()
external bool ggml_is_numa();

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>, ffi.Int32)
>()
external ffi.Pointer<ggml_tensor> ggml_new_i32(
  ffi.Pointer<ggml_context> ctx,
  int value,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_context>, ffi.Float)
>()
external ffi.Pointer<ggml_tensor> ggml_new_f32(
  ffi.Pointer<ggml_context> ctx,
  double value,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_tensor>, ffi.Int32)
>()
external ffi.Pointer<ggml_tensor> ggml_set_i32(
  ffi.Pointer<ggml_tensor> tensor,
  int value,
);

@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(ffi.Pointer<ggml_tensor>, ffi.Float)
>()
external ffi.Pointer<ggml_tensor> ggml_set_f32(
  ffi.Pointer<ggml_tensor> tensor,
  double value,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<ggml_tensor>, ffi.Int)>()
external int ggml_get_i32_1d(
  ffi.Pointer<ggml_tensor> tensor,
  int i,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Int32)>()
external void ggml_set_i32_1d(
  ffi.Pointer<ggml_tensor> tensor,
  int i,
  int value,
);

@ffi.Native<
  ffi.Int32 Function(
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
  )
>()
external int ggml_get_i32_nd(
  ffi.Pointer<ggml_tensor> tensor,
  int i0,
  int i1,
  int i2,
  int i3,
);

@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int32,
  )
>()
external void ggml_set_i32_nd(
  ffi.Pointer<ggml_tensor> tensor,
  int i0,
  int i1,
  int i2,
  int i3,
  int value,
);

@ffi.Native<ffi.Float Function(ffi.Pointer<ggml_tensor>, ffi.Int)>()
external double ggml_get_f32_1d(
  ffi.Pointer<ggml_tensor> tensor,
  int i,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_tensor>, ffi.Int, ffi.Float)>()
external void ggml_set_f32_1d(
  ffi.Pointer<ggml_tensor> tensor,
  int i,
  double value,
);

@ffi.Native<
  ffi.Float Function(
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
  )
>()
external double ggml_get_f32_nd(
  ffi.Pointer<ggml_tensor> tensor,
  int i0,
  int i1,
  int i2,
  int i3,
);

@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<ggml_tensor>,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Int,
    ffi.Float,
  )
>()
external void ggml_set_f32_nd(
  ffi.Pointer<ggml_tensor> tensor,
  int i0,
  int i1,
  int i2,
  int i3,
  double value,
);

@ffi.Native<
  ffi.Pointer<ggml_threadpool> Function(ffi.Pointer<ggml_threadpool_params>)
>()
external ffi.Pointer<ggml_threadpool> ggml_threadpool_new(
  ffi.Pointer<ggml_threadpool_params> params,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_threadpool>)>()
external void ggml_threadpool_free(
  ffi.Pointer<ggml_threadpool> threadpool,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<ggml_threadpool>)>()
external int ggml_threadpool_get_n_threads(
  ffi.Pointer<ggml_threadpool> threadpool,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_threadpool>)>()
external void ggml_threadpool_pause(
  ffi.Pointer<ggml_threadpool> threadpool,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<ggml_threadpool>)>()
external void ggml_threadpool_resume(
  ffi.Pointer<ggml_threadpool> threadpool,
);

/// ggml_graph_plan() has to be called before ggml_graph_compute()
/// when plan.work_size > 0, caller must allocate memory for plan.work_data
@ffi.Native<
  ggml_cplan Function(
    ffi.Pointer<ggml_cgraph>,
    ffi.Int,
    ffi.Pointer<ggml_threadpool>,
  )
>()
external ggml_cplan ggml_graph_plan(
  ffi.Pointer<ggml_cgraph> cgraph,
  int n_threads,
  ffi.Pointer<ggml_threadpool> threadpool,
);

@ffi.Native<
  ffi.Int Function(ffi.Pointer<ggml_cgraph>, ffi.Pointer<ggml_cplan>)
>(symbol: 'ggml_graph_compute')
external int _ggml_graph_compute(
  ffi.Pointer<ggml_cgraph> cgraph,
  ffi.Pointer<ggml_cplan> cplan,
);

ggml_status ggml_graph_compute(
  ffi.Pointer<ggml_cgraph> cgraph,
  ffi.Pointer<ggml_cplan> cplan,
) => ggml_status.fromValue(
  _ggml_graph_compute(
    cgraph,
    cplan,
  ),
);

/// same as ggml_graph_compute() but the work data is allocated as a part of the context
/// note: the drawback of this API is that you must have ensured that the context has enough memory for the work data
@ffi.Native<
  ffi.Int Function(ffi.Pointer<ggml_context>, ffi.Pointer<ggml_cgraph>, ffi.Int)
>(symbol: 'ggml_graph_compute_with_ctx')
external int _ggml_graph_compute_with_ctx(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_cgraph> cgraph,
  int n_threads,
);

ggml_status ggml_graph_compute_with_ctx(
  ffi.Pointer<ggml_context> ctx,
  ffi.Pointer<ggml_cgraph> cgraph,
  int n_threads,
) => ggml_status.fromValue(
  _ggml_graph_compute_with_ctx(
    ctx,
    cgraph,
    n_threads,
  ),
);

/// x86
@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_sse3();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_ssse3();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_avx();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_avx_vnni();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_avx2();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_bmi2();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_f16c();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_fma();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_avx512();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_avx512_vbmi();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_avx512_vnni();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_avx512_bf16();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_amx_int8();

/// ARM
@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_neon();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_arm_fma();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_fp16_va();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_dotprod();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_matmul_int8();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_sve();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_get_sve_cnt();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_sme();

/// other
@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_riscv_v();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_get_rvv_vlen();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_vsx();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_vxe();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_wasm_simd();

@ffi.Native<ffi.Int Function()>()
external int ggml_cpu_has_llamafile();

@ffi.Native<ffi.Pointer<ggml_type_traits_cpu> Function(ffi.UnsignedInt)>(
  symbol: 'ggml_get_type_traits_cpu',
)
external ffi.Pointer<ggml_type_traits_cpu> _ggml_get_type_traits_cpu(
  int type,
);

ffi.Pointer<ggml_type_traits_cpu> ggml_get_type_traits_cpu(
  ggml_type type,
) => _ggml_get_type_traits_cpu(
  type.value,
);

@ffi.Native<ffi.Void Function()>()
external void ggml_cpu_init();

/// CPU backend
@ffi.Native<ggml_backend_t Function()>()
external ggml_backend_t ggml_backend_cpu_init();

@ffi.Native<ffi.Bool Function(ggml_backend_t)>()
external bool ggml_backend_is_cpu(
  ggml_backend_t backend,
);

@ffi.Native<ffi.Void Function(ggml_backend_t, ffi.Int)>()
external void ggml_backend_cpu_set_n_threads(
  ggml_backend_t backend_cpu,
  int n_threads,
);

@ffi.Native<ffi.Void Function(ggml_backend_t, ggml_threadpool_t)>()
external void ggml_backend_cpu_set_threadpool(
  ggml_backend_t backend_cpu,
  ggml_threadpool_t threadpool,
);

@ffi.Native<
  ffi.Void Function(ggml_backend_t, ggml_abort_callback, ffi.Pointer<ffi.Void>)
>()
external void ggml_backend_cpu_set_abort_callback(
  ggml_backend_t backend_cpu,
  ggml_abort_callback abort_callback,
  ffi.Pointer<ffi.Void> abort_callback_data,
);

@ffi.Native<ggml_backend_reg_t Function()>()
external ggml_backend_reg_t ggml_backend_cpu_reg();

@ffi.Native<
  ffi.Void Function(ffi.Pointer<ffi.Float>, ffi.Pointer<ffi.Float>, ffi.Int64)
>()
external void ggml_cpu_fp32_to_fp32(
  ffi.Pointer<ffi.Float> arg0,
  ffi.Pointer<ffi.Float> arg1,
  int arg2,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<ffi.Float>, ffi.Pointer<ffi.Int32>, ffi.Int64)
>()
external void ggml_cpu_fp32_to_i32(
  ffi.Pointer<ffi.Float> arg0,
  ffi.Pointer<ffi.Int32> arg1,
  int arg2,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<ffi.Float>, ffi.Pointer<ggml_fp16_t>, ffi.Int64)
>()
external void ggml_cpu_fp32_to_fp16(
  ffi.Pointer<ffi.Float> arg0,
  ffi.Pointer<ggml_fp16_t> arg1,
  int arg2,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<ggml_fp16_t>, ffi.Pointer<ffi.Float>, ffi.Int64)
>()
external void ggml_cpu_fp16_to_fp32(
  ffi.Pointer<ggml_fp16_t> arg0,
  ffi.Pointer<ffi.Float> arg1,
  int arg2,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<ffi.Float>, ffi.Pointer<ggml_bf16_t>, ffi.Int64)
>()
external void ggml_cpu_fp32_to_bf16(
  ffi.Pointer<ffi.Float> arg0,
  ffi.Pointer<ggml_bf16_t> arg1,
  int arg2,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<ggml_bf16_t>, ffi.Pointer<ffi.Float>, ffi.Int64)
>()
external void ggml_cpu_bf16_to_fp32(
  ffi.Pointer<ggml_bf16_t> arg0,
  ffi.Pointer<ffi.Float> arg1,
  int arg2,
);

/// ====== Dataset ======
@ffi.Native<
  ggml_opt_dataset_t Function(
    ffi.UnsignedInt,
    ffi.UnsignedInt,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
  )
>(symbol: 'ggml_opt_dataset_init')
external ggml_opt_dataset_t _ggml_opt_dataset_init(
  int type_data,
  int type_label,
  int ne_datapoint,
  int ne_label,
  int ndata,
  int ndata_shard,
);

ggml_opt_dataset_t ggml_opt_dataset_init(
  ggml_type type_data,
  ggml_type type_label,
  int ne_datapoint,
  int ne_label,
  int ndata,
  int ndata_shard,
) => _ggml_opt_dataset_init(
  type_data.value,
  type_label.value,
  ne_datapoint,
  ne_label,
  ndata,
  ndata_shard,
);

@ffi.Native<ffi.Void Function(ggml_opt_dataset_t)>()
external void ggml_opt_dataset_free(
  ggml_opt_dataset_t dataset,
);

/// get underlying tensors that store the data
@ffi.Native<ffi.Int64 Function(ggml_opt_dataset_t)>()
external int ggml_opt_dataset_ndata(
  ggml_opt_dataset_t dataset,
);

@ffi.Native<ffi.Pointer<ggml_tensor> Function(ggml_opt_dataset_t)>()
external ffi.Pointer<ggml_tensor> ggml_opt_dataset_data(
  ggml_opt_dataset_t dataset,
);

@ffi.Native<ffi.Pointer<ggml_tensor> Function(ggml_opt_dataset_t)>()
external ffi.Pointer<ggml_tensor> ggml_opt_dataset_labels(
  ggml_opt_dataset_t dataset,
);

/// shuffle idata first datapoints from dataset with RNG from opt_ctx, shuffle all datapoints if idata is negative
@ffi.Native<
  ffi.Void Function(ggml_opt_context_t, ggml_opt_dataset_t, ffi.Int64)
>()
external void ggml_opt_dataset_shuffle(
  ggml_opt_context_t opt_ctx,
  ggml_opt_dataset_t dataset,
  int idata,
);

/// get batch at position ibatch from dataset and copy the data to data_batch and labels_batch
@ffi.Native<
  ffi.Void Function(
    ggml_opt_dataset_t,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ffi.Int64,
  )
>()
external void ggml_opt_dataset_get_batch(
  ggml_opt_dataset_t dataset,
  ffi.Pointer<ggml_tensor> data_batch,
  ffi.Pointer<ggml_tensor> labels_batch,
  int ibatch,
);

@ffi.Native<
  ffi.Void Function(
    ggml_opt_dataset_t,
    ffi.Pointer<ffi.Void>,
    ffi.Size,
    ffi.Pointer<ffi.Void>,
    ffi.Int64,
  )
>()
external void ggml_opt_dataset_get_batch_host(
  ggml_opt_dataset_t dataset,
  ffi.Pointer<ffi.Void> data_batch,
  int nb_data_batch,
  ffi.Pointer<ffi.Void> labels_batch,
  int ibatch,
);

/// returns the default optimizer params (constant, hard-coded values)
/// userdata is not used
@ffi.Native<ggml_opt_optimizer_params Function(ffi.Pointer<ffi.Void>)>()
external ggml_opt_optimizer_params ggml_opt_get_default_optimizer_params(
  ffi.Pointer<ffi.Void> userdata,
);

/// casts userdata to ggml_opt_optimizer_params and returns it
@ffi.Native<ggml_opt_optimizer_params Function(ffi.Pointer<ffi.Void>)>()
external ggml_opt_optimizer_params ggml_opt_get_constant_optimizer_params(
  ffi.Pointer<ffi.Void> userdata,
);

/// get parameters for an optimization context with defaults set where possible
/// parameters for which no sensible defaults exist are supplied as arguments to this function
@ffi.Native<ggml_opt_params Function(ggml_backend_sched_t, ffi.UnsignedInt)>(
  symbol: 'ggml_opt_default_params',
)
external ggml_opt_params _ggml_opt_default_params(
  ggml_backend_sched_t backend_sched,
  int loss_type,
);

ggml_opt_params ggml_opt_default_params(
  ggml_backend_sched_t backend_sched,
  ggml_opt_loss_type loss_type,
) => _ggml_opt_default_params(
  backend_sched,
  loss_type.value,
);

@ffi.Native<ggml_opt_context_t Function(ggml_opt_params)>()
external ggml_opt_context_t ggml_opt_init(
  ggml_opt_params params,
);

@ffi.Native<ffi.Void Function(ggml_opt_context_t)>()
external void ggml_opt_free(
  ggml_opt_context_t opt_ctx,
);

/// set gradients to zero, initilize loss, and optionally reset the optimizer
@ffi.Native<ffi.Void Function(ggml_opt_context_t, ffi.Bool)>()
external void ggml_opt_reset(
  ggml_opt_context_t opt_ctx,
  bool optimizer,
);

@ffi.Native<ffi.Bool Function(ggml_opt_context_t)>()
external bool ggml_opt_static_graphs(
  ggml_opt_context_t opt_ctx,
);

/// get underlying tensors that store data
/// if not using static graphs these pointers become invalid with the next call to ggml_opt_alloc
@ffi.Native<ffi.Pointer<ggml_tensor> Function(ggml_opt_context_t)>()
external ffi.Pointer<ggml_tensor> ggml_opt_inputs(
  ggml_opt_context_t opt_ctx,
);

@ffi.Native<ffi.Pointer<ggml_tensor> Function(ggml_opt_context_t)>()
external ffi.Pointer<ggml_tensor> ggml_opt_outputs(
  ggml_opt_context_t opt_ctx,
);

@ffi.Native<ffi.Pointer<ggml_tensor> Function(ggml_opt_context_t)>()
external ffi.Pointer<ggml_tensor> ggml_opt_labels(
  ggml_opt_context_t opt_ctx,
);

@ffi.Native<ffi.Pointer<ggml_tensor> Function(ggml_opt_context_t)>()
external ffi.Pointer<ggml_tensor> ggml_opt_loss(
  ggml_opt_context_t opt_ctx,
);

@ffi.Native<ffi.Pointer<ggml_tensor> Function(ggml_opt_context_t)>()
external ffi.Pointer<ggml_tensor> ggml_opt_pred(
  ggml_opt_context_t opt_ctx,
);

@ffi.Native<ffi.Pointer<ggml_tensor> Function(ggml_opt_context_t)>()
external ffi.Pointer<ggml_tensor> ggml_opt_ncorrect(
  ggml_opt_context_t opt_ctx,
);

/// get the gradient accumulator for a node from the forward graph
@ffi.Native<
  ffi.Pointer<ggml_tensor> Function(
    ggml_opt_context_t,
    ffi.Pointer<ggml_tensor>,
  )
>()
external ffi.Pointer<ggml_tensor> ggml_opt_grad_acc(
  ggml_opt_context_t opt_ctx,
  ffi.Pointer<ggml_tensor> node,
);

@ffi.Native<ffi.UnsignedInt Function(ggml_opt_context_t)>(
  symbol: 'ggml_opt_context_optimizer_type',
)
external int _ggml_opt_context_optimizer_type(
  ggml_opt_context_t arg0,
);

ggml_opt_optimizer_type ggml_opt_context_optimizer_type(
  ggml_opt_context_t arg0,
) => ggml_opt_optimizer_type.fromValue(
  _ggml_opt_context_optimizer_type(
    arg0,
  ),
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.UnsignedInt)>(
  symbol: 'ggml_opt_optimizer_name',
)
external ffi.Pointer<ffi.Char> _ggml_opt_optimizer_name(
  int arg0,
);

ffi.Pointer<ffi.Char> ggml_opt_optimizer_name(
  ggml_opt_optimizer_type arg0,
) => _ggml_opt_optimizer_name(
  arg0.value,
);

/// ====== Optimization Result ======
@ffi.Native<ggml_opt_result_t Function()>()
external ggml_opt_result_t ggml_opt_result_init();

@ffi.Native<ffi.Void Function(ggml_opt_result_t)>()
external void ggml_opt_result_free(
  ggml_opt_result_t result,
);

@ffi.Native<ffi.Void Function(ggml_opt_result_t)>()
external void ggml_opt_result_reset(
  ggml_opt_result_t result,
);

/// get data from result, uncertainties are optional and can be ignored by passing NULL
@ffi.Native<ffi.Void Function(ggml_opt_result_t, ffi.Pointer<ffi.Int64>)>()
external void ggml_opt_result_ndata(
  ggml_opt_result_t result,
  ffi.Pointer<ffi.Int64> ndata,
);

@ffi.Native<
  ffi.Void Function(
    ggml_opt_result_t,
    ffi.Pointer<ffi.Double>,
    ffi.Pointer<ffi.Double>,
  )
>()
external void ggml_opt_result_loss(
  ggml_opt_result_t result,
  ffi.Pointer<ffi.Double> loss,
  ffi.Pointer<ffi.Double> unc,
);

@ffi.Native<ffi.Void Function(ggml_opt_result_t, ffi.Pointer<ffi.Int32>)>()
external void ggml_opt_result_pred(
  ggml_opt_result_t result,
  ffi.Pointer<ffi.Int32> pred,
);

@ffi.Native<
  ffi.Void Function(
    ggml_opt_result_t,
    ffi.Pointer<ffi.Double>,
    ffi.Pointer<ffi.Double>,
  )
>()
external void ggml_opt_result_accuracy(
  ggml_opt_result_t result,
  ffi.Pointer<ffi.Double> accuracy,
  ffi.Pointer<ffi.Double> unc,
);

/// if not using static graphs, this function must be called prior to ggml_opt_alloc
@ffi.Native<
  ffi.Void Function(
    ggml_opt_context_t,
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_cgraph>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
  )
>()
external void ggml_opt_prepare_alloc(
  ggml_opt_context_t opt_ctx,
  ffi.Pointer<ggml_context> ctx_compute,
  ffi.Pointer<ggml_cgraph> gf,
  ffi.Pointer<ggml_tensor> inputs,
  ffi.Pointer<ggml_tensor> outputs,
);

/// allocate the next graph for evaluation, either forward or forward + backward
/// must be called exactly once prior to calling ggml_opt_eval
@ffi.Native<ffi.Void Function(ggml_opt_context_t, ffi.Bool)>()
external void ggml_opt_alloc(
  ggml_opt_context_t opt_ctx,
  bool backward,
);

/// do forward pass, increment result if not NULL, do backward pass if allocated
@ffi.Native<ffi.Void Function(ggml_opt_context_t, ggml_opt_result_t)>()
external void ggml_opt_eval(
  ggml_opt_context_t opt_ctx,
  ggml_opt_result_t result,
);

/// do training on front of dataset, do evaluation only on back of dataset
@ffi.Native<
  ffi.Void Function(
    ggml_opt_context_t,
    ggml_opt_dataset_t,
    ggml_opt_result_t,
    ggml_opt_result_t,
    ffi.Int64,
    ggml_opt_epoch_callback,
    ggml_opt_epoch_callback,
  )
>()
external void ggml_opt_epoch(
  ggml_opt_context_t opt_ctx,
  ggml_opt_dataset_t dataset,
  ggml_opt_result_t result_train,
  ggml_opt_result_t result_eval,
  int idata_split,
  ggml_opt_epoch_callback callback_train,
  ggml_opt_epoch_callback callback_eval,
);

/// callback that prints a progress bar on stderr
@ffi.Native<
  ffi.Void Function(
    ffi.Bool,
    ggml_opt_context_t,
    ggml_opt_dataset_t,
    ggml_opt_result_t,
    ffi.Int64,
    ffi.Int64,
    ffi.Int64,
  )
>()
external void ggml_opt_epoch_callback_progress_bar(
  bool train,
  ggml_opt_context_t opt_ctx,
  ggml_opt_dataset_t dataset,
  ggml_opt_result_t result,
  int ibatch,
  int ibatch_max,
  int t_start_us,
);

/// fit model defined by inputs and outputs to dataset
@ffi.Native<
  ffi.Void Function(
    ggml_backend_sched_t,
    ffi.Pointer<ggml_context>,
    ffi.Pointer<ggml_tensor>,
    ffi.Pointer<ggml_tensor>,
    ggml_opt_dataset_t,
    ffi.UnsignedInt,
    ffi.UnsignedInt,
    ggml_opt_get_optimizer_params,
    ffi.Int64,
    ffi.Int64,
    ffi.Float,
    ffi.Bool,
  )
>(symbol: 'ggml_opt_fit')
external void _ggml_opt_fit(
  ggml_backend_sched_t backend_sched,
  ffi.Pointer<ggml_context> ctx_compute,
  ffi.Pointer<ggml_tensor> inputs,
  ffi.Pointer<ggml_tensor> outputs,
  ggml_opt_dataset_t dataset,
  int loss_type,
  int optimizer,
  ggml_opt_get_optimizer_params get_opt_pars,
  int nepoch,
  int nbatch_logical,
  double val_split,
  bool silent,
);

void ggml_opt_fit(
  ggml_backend_sched_t backend_sched,
  ffi.Pointer<ggml_context> ctx_compute,
  ffi.Pointer<ggml_tensor> inputs,
  ffi.Pointer<ggml_tensor> outputs,
  ggml_opt_dataset_t dataset,
  ggml_opt_loss_type loss_type,
  ggml_opt_optimizer_type optimizer,
  ggml_opt_get_optimizer_params get_opt_pars,
  int nepoch,
  int nbatch_logical,
  double val_split,
  bool silent,
) => _ggml_opt_fit(
  backend_sched,
  ctx_compute,
  inputs,
  outputs,
  dataset,
  loss_type.value,
  optimizer.value,
  get_opt_pars,
  nepoch,
  nbatch_logical,
  val_split,
  silent,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.Int)>(
  symbol: 'llama_flash_attn_type_name',
)
external ffi.Pointer<ffi.Char> _llama_flash_attn_type_name(
  int flash_attn_type,
);

ffi.Pointer<ffi.Char> llama_flash_attn_type_name(
  llama_flash_attn_type flash_attn_type,
) => _llama_flash_attn_type_name(
  flash_attn_type.value,
);

/// Helpers for getting default parameters
/// TODO: update API to start accepting pointers to params structs (https://github.com/ggml-org/llama.cpp/discussions/9172)
@ffi.Native<llama_model_params Function()>()
external llama_model_params llama_model_default_params();

@ffi.Native<llama_context_params Function()>()
external llama_context_params llama_context_default_params();

@ffi.Native<llama_sampler_chain_params Function()>()
external llama_sampler_chain_params llama_sampler_chain_default_params();

@ffi.Native<llama_model_quantize_params Function()>()
external llama_model_quantize_params llama_model_quantize_default_params();

/// Initialize the llama + ggml backend
/// If numa is true, use NUMA optimizations
/// Call once at the start of the program
@ffi.Native<ffi.Void Function()>()
external void llama_backend_init();

/// Call once at the end of the program - currently only used for MPI
@ffi.Native<ffi.Void Function()>()
external void llama_backend_free();

/// optional:
@ffi.Native<ffi.Void Function(ffi.UnsignedInt)>(symbol: 'llama_numa_init')
external void _llama_numa_init(
  int numa,
);

void llama_numa_init(
  ggml_numa_strategy numa,
) => _llama_numa_init(
  numa.value,
);

/// Optional: an auto threadpool gets created in ggml if not passed explicitly
@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<llama_context>,
    ggml_threadpool_t,
    ggml_threadpool_t,
  )
>()
external void llama_attach_threadpool(
  ffi.Pointer<llama_context> ctx,
  ggml_threadpool_t threadpool,
  ggml_threadpool_t threadpool_batch,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>()
external void llama_detach_threadpool(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<
  ffi.Pointer<llama_model> Function(ffi.Pointer<ffi.Char>, llama_model_params)
>()
external ffi.Pointer<llama_model> llama_load_model_from_file(
  ffi.Pointer<ffi.Char> path_model,
  llama_model_params params,
);

/// Load the model from a file
/// If the file is split into multiple parts, the file name must follow this pattern: <name>-%05d-of-%05d.gguf
/// If the split file name does not follow this pattern, use llama_model_load_from_splits
@ffi.Native<
  ffi.Pointer<llama_model> Function(ffi.Pointer<ffi.Char>, llama_model_params)
>()
external ffi.Pointer<llama_model> llama_model_load_from_file(
  ffi.Pointer<ffi.Char> path_model,
  llama_model_params params,
);

/// Load the model from multiple splits (support custom naming scheme)
/// The paths must be in the correct order
@ffi.Native<
  ffi.Pointer<llama_model> Function(
    ffi.Pointer<ffi.Pointer<ffi.Char>>,
    ffi.Size,
    llama_model_params,
  )
>()
external ffi.Pointer<llama_model> llama_model_load_from_splits(
  ffi.Pointer<ffi.Pointer<ffi.Char>> paths,
  int n_paths,
  llama_model_params params,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>)
>()
external void llama_model_save_to_file(
  ffi.Pointer<llama_model> model,
  ffi.Pointer<ffi.Char> path_model,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_model>)>()
external void llama_free_model(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_model>)>()
external void llama_model_free(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<
  ffi.Pointer<llama_context> Function(
    ffi.Pointer<llama_model>,
    llama_context_params,
  )
>()
external ffi.Pointer<llama_context> llama_init_from_model(
  ffi.Pointer<llama_model> model,
  llama_context_params params,
);

@ffi.Native<
  ffi.Pointer<llama_context> Function(
    ffi.Pointer<llama_model>,
    llama_context_params,
  )
>()
external ffi.Pointer<llama_context> llama_new_context_with_model(
  ffi.Pointer<llama_model> model,
  llama_context_params params,
);

/// Frees all allocated memory
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>()
external void llama_free(
  ffi.Pointer<llama_context> ctx,
);

/// fits mparams and cparams to free device memory (assumes system memory is unlimited)
/// returns true if the parameters could be successfully modified to fit device memory
/// this function is NOT thread safe because it modifies the global llama logger state
@ffi.Native<
  ffi.Bool Function(
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<llama_model_params>,
    ffi.Pointer<llama_context_params>,
    ffi.Pointer<ffi.Float>,
    ffi.Pointer<llama_model_tensor_buft_override>,
    ffi.Size,
    ffi.Uint32,
    ffi.UnsignedInt,
  )
>(symbol: 'llama_params_fit')
external bool _llama_params_fit(
  ffi.Pointer<ffi.Char> path_model,
  ffi.Pointer<llama_model_params> mparams,
  ffi.Pointer<llama_context_params> cparams,
  ffi.Pointer<ffi.Float> tensor_split,
  ffi.Pointer<llama_model_tensor_buft_override> tensor_buft_overrides,
  int margin,
  int n_ctx_min,
  int log_level,
);

bool llama_params_fit(
  ffi.Pointer<ffi.Char> path_model,
  ffi.Pointer<llama_model_params> mparams,
  ffi.Pointer<llama_context_params> cparams,
  ffi.Pointer<ffi.Float> tensor_split,
  ffi.Pointer<llama_model_tensor_buft_override> tensor_buft_overrides,
  int margin,
  int n_ctx_min,
  ggml_log_level log_level,
) => _llama_params_fit(
  path_model,
  mparams,
  cparams,
  tensor_split,
  tensor_buft_overrides,
  margin,
  n_ctx_min,
  log_level.value,
);

@ffi.Native<ffi.Int64 Function()>()
external int llama_time_us();

@ffi.Native<ffi.Size Function()>()
external int llama_max_devices();

@ffi.Native<ffi.Size Function()>()
external int llama_max_parallel_sequences();

@ffi.Native<ffi.Size Function()>()
external int llama_max_tensor_buft_overrides();

@ffi.Native<ffi.Bool Function()>()
external bool llama_supports_mmap();

@ffi.Native<ffi.Bool Function()>()
external bool llama_supports_mlock();

@ffi.Native<ffi.Bool Function()>()
external bool llama_supports_gpu_offload();

@ffi.Native<ffi.Bool Function()>()
external bool llama_supports_rpc();

/// NOTE: After creating a llama_context, it is recommended to query the actual values using these functions
/// In some cases the requested values via llama_context_params may differ from the actual values used by the context
/// ref: https://github.com/ggml-org/llama.cpp/pull/17046#discussion_r2503085732
@ffi.Native<ffi.Uint32 Function(ffi.Pointer<llama_context>)>()
external int llama_n_ctx(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Uint32 Function(ffi.Pointer<llama_context>)>()
external int llama_n_ctx_seq(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Uint32 Function(ffi.Pointer<llama_context>)>()
external int llama_n_batch(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Uint32 Function(ffi.Pointer<llama_context>)>()
external int llama_n_ubatch(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Uint32 Function(ffi.Pointer<llama_context>)>()
external int llama_n_seq_max(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_n_ctx_train(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_n_embd(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_n_layer(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_n_head(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_vocab>)>()
external int llama_n_vocab(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<ffi.Pointer<llama_model> Function(ffi.Pointer<llama_context>)>()
external ffi.Pointer<llama_model> llama_get_model(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<llama_memory_t Function(ffi.Pointer<llama_context>)>()
external llama_memory_t llama_get_memory(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<llama_context>)>(
  symbol: 'llama_pooling_type',
)
external int _llama_pooling_type$1(
  ffi.Pointer<llama_context> ctx,
);

llama_pooling_type llama_pooling_type$1(
  ffi.Pointer<llama_context> ctx,
) => llama_pooling_type.fromValue(
  _llama_pooling_type$1(
    ctx,
  ),
);

@ffi.Native<ffi.Pointer<llama_vocab> Function(ffi.Pointer<llama_model>)>()
external ffi.Pointer<llama_vocab> llama_model_get_vocab(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<llama_model>)>(
  symbol: 'llama_model_rope_type',
)
external int _llama_model_rope_type(
  ffi.Pointer<llama_model> model,
);

llama_rope_type llama_model_rope_type(
  ffi.Pointer<llama_model> model,
) => llama_rope_type.fromValue(
  _llama_model_rope_type(
    model,
  ),
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_model_n_ctx_train(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_model_n_embd(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_model_n_embd_inp(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_model_n_layer(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_model_n_head(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_model_n_head_kv(
  ffi.Pointer<llama_model> model,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_model_n_swa(
  ffi.Pointer<llama_model> model,
);

/// Get the model's RoPE frequency scaling factor
@ffi.Native<ffi.Float Function(ffi.Pointer<llama_model>)>()
external double llama_model_rope_freq_scale_train(
  ffi.Pointer<llama_model> model,
);

/// Returns the number of classifier outputs (only valid for classifier models)
/// Undefined behavior for non-classifier models
@ffi.Native<ffi.Uint32 Function(ffi.Pointer<llama_model>)>()
external int llama_model_n_cls_out(
  ffi.Pointer<llama_model> model,
);

/// Returns label of classifier output by index (<n_cls_out). Returns nullptr if no label provided
@ffi.Native<
  ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_model>, ffi.Uint32)
>()
external ffi.Pointer<ffi.Char> llama_model_cls_label(
  ffi.Pointer<llama_model> model,
  int i,
);

@ffi.Native<ffi.UnsignedInt Function(ffi.Pointer<llama_vocab>)>(
  symbol: 'llama_vocab_type',
)
external int _llama_vocab_type$1(
  ffi.Pointer<llama_vocab> vocab,
);

llama_vocab_type llama_vocab_type$1(
  ffi.Pointer<llama_vocab> vocab,
) => llama_vocab_type.fromValue(
  _llama_vocab_type$1(
    vocab,
  ),
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_n_tokens(
  ffi.Pointer<llama_vocab> vocab,
);

/// Get metadata value as a string by key name
@ffi.Native<
  ffi.Int32 Function(
    ffi.Pointer<llama_model>,
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<ffi.Char>,
    ffi.Size,
  )
>()
external int llama_model_meta_val_str(
  ffi.Pointer<llama_model> model,
  ffi.Pointer<ffi.Char> key,
  ffi.Pointer<ffi.Char> buf,
  int buf_size,
);

/// Get the number of metadata key/value pairs
@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_model>)>()
external int llama_model_meta_count(
  ffi.Pointer<llama_model> model,
);

/// Get sampling metadata key name. Returns nullptr if the key is invalid
@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.UnsignedInt)>(
  symbol: 'llama_model_meta_key_str',
)
external ffi.Pointer<ffi.Char> _llama_model_meta_key_str(
  int key,
);

ffi.Pointer<ffi.Char> llama_model_meta_key_str(
  llama_model_meta_key key,
) => _llama_model_meta_key_str(
  key.value,
);

/// Get metadata key name by index
@ffi.Native<
  ffi.Int32 Function(
    ffi.Pointer<llama_model>,
    ffi.Int32,
    ffi.Pointer<ffi.Char>,
    ffi.Size,
  )
>()
external int llama_model_meta_key_by_index(
  ffi.Pointer<llama_model> model,
  int i,
  ffi.Pointer<ffi.Char> buf,
  int buf_size,
);

/// Get metadata value as a string by index
@ffi.Native<
  ffi.Int32 Function(
    ffi.Pointer<llama_model>,
    ffi.Int32,
    ffi.Pointer<ffi.Char>,
    ffi.Size,
  )
>()
external int llama_model_meta_val_str_by_index(
  ffi.Pointer<llama_model> model,
  int i,
  ffi.Pointer<ffi.Char> buf,
  int buf_size,
);

/// Get a string describing the model type
@ffi.Native<
  ffi.Int32 Function(ffi.Pointer<llama_model>, ffi.Pointer<ffi.Char>, ffi.Size)
>()
external int llama_model_desc(
  ffi.Pointer<llama_model> model,
  ffi.Pointer<ffi.Char> buf,
  int buf_size,
);

/// Returns the total size of all the tensors in the model in bytes
@ffi.Native<ffi.Uint64 Function(ffi.Pointer<llama_model>)>()
external int llama_model_size(
  ffi.Pointer<llama_model> model,
);

/// Get the default chat template. Returns nullptr if not available
/// If name is NULL, returns the default chat template
@ffi.Native<
  ffi.Pointer<ffi.Char> Function(
    ffi.Pointer<llama_model>,
    ffi.Pointer<ffi.Char>,
  )
>()
external ffi.Pointer<ffi.Char> llama_model_chat_template(
  ffi.Pointer<llama_model> model,
  ffi.Pointer<ffi.Char> name,
);

/// Returns the total number of parameters in the model
@ffi.Native<ffi.Uint64 Function(ffi.Pointer<llama_model>)>()
external int llama_model_n_params(
  ffi.Pointer<llama_model> model,
);

/// Returns true if the model contains an encoder that requires llama_encode() call
@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_model>)>()
external bool llama_model_has_encoder(
  ffi.Pointer<llama_model> model,
);

/// Returns true if the model contains a decoder that requires llama_decode() call
@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_model>)>()
external bool llama_model_has_decoder(
  ffi.Pointer<llama_model> model,
);

/// For encoder-decoder models, this function returns id of the token that must be provided
/// to the decoder to start generating output sequence. For other models, it returns -1.
@ffi.Native<llama_token Function(ffi.Pointer<llama_model>)>()
external int llama_model_decoder_start_token(
  ffi.Pointer<llama_model> model,
);

/// Returns true if the model is recurrent (like Mamba, RWKV, etc.)
@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_model>)>()
external bool llama_model_is_recurrent(
  ffi.Pointer<llama_model> model,
);

/// Returns true if the model is hybrid (like Jamba, Granite, etc.)
@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_model>)>()
external bool llama_model_is_hybrid(
  ffi.Pointer<llama_model> model,
);

/// Returns true if the model is diffusion-based (like LLaDA, Dream, etc.)
@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_model>)>()
external bool llama_model_is_diffusion(
  ffi.Pointer<llama_model> model,
);

/// Returns 0 on success
@ffi.Native<
  ffi.Uint32 Function(
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<llama_model_quantize_params>,
  )
>()
external int llama_model_quantize(
  ffi.Pointer<ffi.Char> fname_inp,
  ffi.Pointer<ffi.Char> fname_out,
  ffi.Pointer<llama_model_quantize_params> params,
);

/// Load a LoRA adapter from file
@ffi.Native<
  ffi.Pointer<llama_adapter_lora> Function(
    ffi.Pointer<llama_model>,
    ffi.Pointer<ffi.Char>,
  )
>()
external ffi.Pointer<llama_adapter_lora> llama_adapter_lora_init(
  ffi.Pointer<llama_model> model,
  ffi.Pointer<ffi.Char> path_lora,
);

/// Get metadata value as a string by key name
@ffi.Native<
  ffi.Int32 Function(
    ffi.Pointer<llama_adapter_lora>,
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<ffi.Char>,
    ffi.Size,
  )
>()
external int llama_adapter_meta_val_str(
  ffi.Pointer<llama_adapter_lora> adapter,
  ffi.Pointer<ffi.Char> key,
  ffi.Pointer<ffi.Char> buf,
  int buf_size,
);

/// Get the number of metadata key/value pairs
@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_adapter_lora>)>()
external int llama_adapter_meta_count(
  ffi.Pointer<llama_adapter_lora> adapter,
);

/// Get metadata key name by index
@ffi.Native<
  ffi.Int32 Function(
    ffi.Pointer<llama_adapter_lora>,
    ffi.Int32,
    ffi.Pointer<ffi.Char>,
    ffi.Size,
  )
>()
external int llama_adapter_meta_key_by_index(
  ffi.Pointer<llama_adapter_lora> adapter,
  int i,
  ffi.Pointer<ffi.Char> buf,
  int buf_size,
);

/// Get metadata value as a string by index
@ffi.Native<
  ffi.Int32 Function(
    ffi.Pointer<llama_adapter_lora>,
    ffi.Int32,
    ffi.Pointer<ffi.Char>,
    ffi.Size,
  )
>()
external int llama_adapter_meta_val_str_by_index(
  ffi.Pointer<llama_adapter_lora> adapter,
  int i,
  ffi.Pointer<ffi.Char> buf,
  int buf_size,
);

/// Manually free a LoRA adapter
/// NOTE: loaded adapters will be free when the associated model is deleted
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_adapter_lora>)>()
external void llama_adapter_lora_free(
  ffi.Pointer<llama_adapter_lora> adapter,
);

/// Get the invocation tokens if the current lora is an alora
@ffi.Native<ffi.Uint64 Function(ffi.Pointer<llama_adapter_lora>)>()
external int llama_adapter_get_alora_n_invocation_tokens(
  ffi.Pointer<llama_adapter_lora> adapter,
);

@ffi.Native<
  ffi.Pointer<llama_token> Function(ffi.Pointer<llama_adapter_lora>)
>()
external ffi.Pointer<llama_token> llama_adapter_get_alora_invocation_tokens(
  ffi.Pointer<llama_adapter_lora> adapter,
);

/// Add a loaded LoRA adapter to given context
/// This will not modify model's weight
@ffi.Native<
  ffi.Int32 Function(
    ffi.Pointer<llama_context>,
    ffi.Pointer<llama_adapter_lora>,
    ffi.Float,
  )
>()
external int llama_set_adapter_lora(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<llama_adapter_lora> adapter,
  double scale,
);

/// Remove a specific LoRA adapter from given context
/// Return -1 if the adapter is not present in the context
@ffi.Native<
  ffi.Int32 Function(
    ffi.Pointer<llama_context>,
    ffi.Pointer<llama_adapter_lora>,
  )
>()
external int llama_rm_adapter_lora(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<llama_adapter_lora> adapter,
);

/// Remove all LoRA adapters from given context
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>()
external void llama_clear_adapter_lora(
  ffi.Pointer<llama_context> ctx,
);

/// Apply a loaded control vector to a llama_context, or if data is NULL, clear
/// the currently loaded vector.
/// n_embd should be the size of a single layer's control, and data should point
/// to an n_embd x n_layers buffer starting from layer 1.
/// il_start and il_end are the layer range the vector should apply to (both inclusive)
/// See llama_control_vector_load in common to load a control vector.
@ffi.Native<
  ffi.Int32 Function(
    ffi.Pointer<llama_context>,
    ffi.Pointer<ffi.Float>,
    ffi.Size,
    ffi.Int32,
    ffi.Int32,
    ffi.Int32,
  )
>()
external int llama_apply_adapter_cvec(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Float> data,
  int len,
  int n_embd,
  int il_start,
  int il_end,
);

/// Clear the memory contents
/// If data == true, the data buffers will also be cleared together with the metadata
@ffi.Native<ffi.Void Function(llama_memory_t, ffi.Bool)>()
external void llama_memory_clear(
  llama_memory_t mem,
  bool data,
);

/// Removes all tokens that belong to the specified sequence and have positions in [p0, p1)
/// Returns false if a partial sequence cannot be removed. Removing a whole sequence never fails
/// seq_id < 0 : match any sequence
/// p0 < 0     : [0,  p1]
/// p1 < 0     : [p0, inf)
@ffi.Native<
  ffi.Bool Function(llama_memory_t, llama_seq_id, llama_pos, llama_pos)
>()
external bool llama_memory_seq_rm(
  llama_memory_t mem,
  int seq_id,
  int p0,
  int p1,
);

/// Copy all tokens that belong to the specified sequence to another sequence
/// p0 < 0 : [0,  p1]
/// p1 < 0 : [p0, inf)
@ffi.Native<
  ffi.Void Function(
    llama_memory_t,
    llama_seq_id,
    llama_seq_id,
    llama_pos,
    llama_pos,
  )
>()
external void llama_memory_seq_cp(
  llama_memory_t mem,
  int seq_id_src,
  int seq_id_dst,
  int p0,
  int p1,
);

/// Removes all tokens that do not belong to the specified sequence
@ffi.Native<ffi.Void Function(llama_memory_t, llama_seq_id)>()
external void llama_memory_seq_keep(
  llama_memory_t mem,
  int seq_id,
);

/// Adds relative position "delta" to all tokens that belong to the specified sequence and have positions in [p0, p1)
/// p0 < 0 : [0,  p1]
/// p1 < 0 : [p0, inf)
@ffi.Native<
  ffi.Void Function(
    llama_memory_t,
    llama_seq_id,
    llama_pos,
    llama_pos,
    llama_pos,
  )
>()
external void llama_memory_seq_add(
  llama_memory_t mem,
  int seq_id,
  int p0,
  int p1,
  int delta,
);

/// Integer division of the positions by factor of `d > 1`
/// p0 < 0 : [0,  p1]
/// p1 < 0 : [p0, inf)
@ffi.Native<
  ffi.Void Function(llama_memory_t, llama_seq_id, llama_pos, llama_pos, ffi.Int)
>()
external void llama_memory_seq_div(
  llama_memory_t mem,
  int seq_id,
  int p0,
  int p1,
  int d,
);

/// Returns the smallest position present in the memory for the specified sequence
/// This is typically non-zero only for SWA caches
/// Note that all positions in the range [pos_min, pos_max] are guaranteed to be present in the memory
/// Return -1 if the sequence is empty
@ffi.Native<llama_pos Function(llama_memory_t, llama_seq_id)>()
external int llama_memory_seq_pos_min(
  llama_memory_t mem,
  int seq_id,
);

/// Returns the largest position present in the memory for the specified sequence
/// Note that all positions in the range [pos_min, pos_max] are guaranteed to be present in the memory
/// Return -1 if the sequence is empty
@ffi.Native<llama_pos Function(llama_memory_t, llama_seq_id)>()
external int llama_memory_seq_pos_max(
  llama_memory_t mem,
  int seq_id,
);

/// Check if the memory supports shifting
@ffi.Native<ffi.Bool Function(llama_memory_t)>()
external bool llama_memory_can_shift(
  llama_memory_t mem,
);

/// Returns the *actual* size in bytes of the state
/// (logits, embedding and memory)
/// Only use when saving the state, not when restoring it, otherwise the size may be too small.
@ffi.Native<ffi.Size Function(ffi.Pointer<llama_context>)>()
external int llama_state_get_size(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Size Function(ffi.Pointer<llama_context>)>()
external int llama_get_state_size(
  ffi.Pointer<llama_context> ctx,
);

/// Copies the state to the specified destination address.
/// Destination needs to have allocated enough memory.
/// Returns the number of bytes copied
@ffi.Native<
  ffi.Size Function(
    ffi.Pointer<llama_context>,
    ffi.Pointer<ffi.Uint8>,
    ffi.Size,
  )
>()
external int llama_state_get_data(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Uint8> dst,
  int size,
);

@ffi.Native<
  ffi.Size Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>)
>()
external int llama_copy_state_data(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Uint8> dst,
);

/// Set the state reading from the specified address
/// Returns the number of bytes read
@ffi.Native<
  ffi.Size Function(
    ffi.Pointer<llama_context>,
    ffi.Pointer<ffi.Uint8>,
    ffi.Size,
  )
>()
external int llama_state_set_data(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Uint8> src,
  int size,
);

@ffi.Native<
  ffi.Size Function(ffi.Pointer<llama_context>, ffi.Pointer<ffi.Uint8>)
>()
external int llama_set_state_data(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Uint8> src,
);

/// Save/load session file
@ffi.Native<
  ffi.Bool Function(
    ffi.Pointer<llama_context>,
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<llama_token>,
    ffi.Size,
    ffi.Pointer<ffi.Size>,
  )
>()
external bool llama_state_load_file(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Char> path_session,
  ffi.Pointer<llama_token> tokens_out,
  int n_token_capacity,
  ffi.Pointer<ffi.Size> n_token_count_out,
);

@ffi.Native<
  ffi.Bool Function(
    ffi.Pointer<llama_context>,
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<llama_token>,
    ffi.Size,
    ffi.Pointer<ffi.Size>,
  )
>()
external bool llama_load_session_file(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Char> path_session,
  ffi.Pointer<llama_token> tokens_out,
  int n_token_capacity,
  ffi.Pointer<ffi.Size> n_token_count_out,
);

@ffi.Native<
  ffi.Bool Function(
    ffi.Pointer<llama_context>,
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<llama_token>,
    ffi.Size,
  )
>()
external bool llama_state_save_file(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Char> path_session,
  ffi.Pointer<llama_token> tokens,
  int n_token_count,
);

@ffi.Native<
  ffi.Bool Function(
    ffi.Pointer<llama_context>,
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<llama_token>,
    ffi.Size,
  )
>()
external bool llama_save_session_file(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Char> path_session,
  ffi.Pointer<llama_token> tokens,
  int n_token_count,
);

/// Get the exact size needed to copy the state of a single sequence
@ffi.Native<ffi.Size Function(ffi.Pointer<llama_context>, llama_seq_id)>()
external int llama_state_seq_get_size(
  ffi.Pointer<llama_context> ctx,
  int seq_id,
);

/// Copy the state of a single sequence into the specified buffer
@ffi.Native<
  ffi.Size Function(
    ffi.Pointer<llama_context>,
    ffi.Pointer<ffi.Uint8>,
    ffi.Size,
    llama_seq_id,
  )
>()
external int llama_state_seq_get_data(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Uint8> dst,
  int size,
  int seq_id,
);

/// Copy the sequence data (originally copied with `llama_state_seq_get_data`) into the specified sequence
/// Returns:
/// - Positive: Ok
/// - Zero: Failed to load
@ffi.Native<
  ffi.Size Function(
    ffi.Pointer<llama_context>,
    ffi.Pointer<ffi.Uint8>,
    ffi.Size,
    llama_seq_id,
  )
>()
external int llama_state_seq_set_data(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Uint8> src,
  int size,
  int dest_seq_id,
);

@ffi.Native<
  ffi.Size Function(
    ffi.Pointer<llama_context>,
    ffi.Pointer<ffi.Char>,
    llama_seq_id,
    ffi.Pointer<llama_token>,
    ffi.Size,
  )
>()
external int llama_state_seq_save_file(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Char> filepath,
  int seq_id,
  ffi.Pointer<llama_token> tokens,
  int n_token_count,
);

@ffi.Native<
  ffi.Size Function(
    ffi.Pointer<llama_context>,
    ffi.Pointer<ffi.Char>,
    llama_seq_id,
    ffi.Pointer<llama_token>,
    ffi.Size,
    ffi.Pointer<ffi.Size>,
  )
>()
external int llama_state_seq_load_file(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Char> filepath,
  int dest_seq_id,
  ffi.Pointer<llama_token> tokens_out,
  int n_token_capacity,
  ffi.Pointer<ffi.Size> n_token_count_out,
);

@ffi.Native<
  ffi.Size Function(
    ffi.Pointer<llama_context>,
    llama_seq_id,
    llama_state_seq_flags,
  )
>()
external int llama_state_seq_get_size_ext(
  ffi.Pointer<llama_context> ctx,
  int seq_id,
  int flags,
);

@ffi.Native<
  ffi.Size Function(
    ffi.Pointer<llama_context>,
    ffi.Pointer<ffi.Uint8>,
    ffi.Size,
    llama_seq_id,
    llama_state_seq_flags,
  )
>()
external int llama_state_seq_get_data_ext(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Uint8> dst,
  int size,
  int seq_id,
  int flags,
);

@ffi.Native<
  ffi.Size Function(
    ffi.Pointer<llama_context>,
    ffi.Pointer<ffi.Uint8>,
    ffi.Size,
    llama_seq_id,
    llama_state_seq_flags,
  )
>()
external int llama_state_seq_set_data_ext(
  ffi.Pointer<llama_context> ctx,
  ffi.Pointer<ffi.Uint8> src,
  int size,
  int dest_seq_id,
  int flags,
);

/// Return batch for single sequence of tokens
/// The sequence ID will be fixed to 0
/// The position of the tokens will be tracked automatically by llama_decode
///
/// NOTE: this is a helper function to facilitate transition to the new batch API - avoid using it
@ffi.Native<llama_batch Function(ffi.Pointer<llama_token>, ffi.Int32)>()
external llama_batch llama_batch_get_one(
  ffi.Pointer<llama_token> tokens,
  int n_tokens,
);

/// Allocates a batch of tokens on the heap that can hold a maximum of n_tokens
/// Each token can be assigned up to n_seq_max sequence ids
/// The batch has to be freed with llama_batch_free()
/// If embd != 0, llama_batch.embd will be allocated with size of n_tokens * embd * sizeof(float)
/// Otherwise, llama_batch.token will be allocated to store n_tokens llama_token
/// The rest of the llama_batch members are allocated with size n_tokens
/// All members are left uninitialized
@ffi.Native<llama_batch Function(ffi.Int32, ffi.Int32, ffi.Int32)>()
external llama_batch llama_batch_init(
  int n_tokens,
  int embd,
  int n_seq_max,
);

/// Frees a batch of tokens allocated with llama_batch_init()
@ffi.Native<ffi.Void Function(llama_batch)>()
external void llama_batch_free(
  llama_batch batch,
);

/// Process a batch of tokens.
/// In contrast to llama_decode() - this call does not use KV cache.
/// For encode-decoder contexts, processes the batch using the encoder.
/// Can store the encoder output internally for later use by the decoder's cross-attention layers.
/// 0 - success
/// < 0 - error. the memory state is restored to the state before this call
@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_context>, llama_batch)>()
external int llama_encode(
  ffi.Pointer<llama_context> ctx,
  llama_batch batch,
);

/// Process a batch of tokens.
/// Requires the context to have a memory.
/// For encode-decoder contexts, processes the batch using the decoder.
/// Positive return values does not mean a fatal error, but rather a warning.
/// Upon fatal-error or abort, the ubatches that managed to be been processed will remain in the memory state of the context
/// To handle this correctly, query the memory state using llama_memory_seq_pos_min() and llama_memory_seq_pos_max()
/// Upon other return values, the memory state is restored to the state before this call
/// 0 - success
/// 1 - could not find a KV slot for the batch (try reducing the size of the batch or increase the context)
/// 2 - aborted     (processed ubatches will remain in the context's memory)
/// -1 - invalid input batch
/// < -1 - fatal error (processed ubatches will remain in the context's memory)
@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_context>, llama_batch)>()
external int llama_decode(
  ffi.Pointer<llama_context> ctx,
  llama_batch batch,
);

/// Set the number of threads used for decoding
/// n_threads is the number of threads used for generation (single token)
/// n_threads_batch is the number of threads used for prompt and batch processing (multiple tokens)
@ffi.Native<
  ffi.Void Function(ffi.Pointer<llama_context>, ffi.Int32, ffi.Int32)
>()
external void llama_set_n_threads(
  ffi.Pointer<llama_context> ctx,
  int n_threads,
  int n_threads_batch,
);

/// Get the number of threads used for generation of a single token.
@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_context>)>()
external int llama_n_threads(
  ffi.Pointer<llama_context> ctx,
);

/// Get the number of threads used for prompt and batch processing (multiple token).
@ffi.Native<ffi.Int32 Function(ffi.Pointer<llama_context>)>()
external int llama_n_threads_batch(
  ffi.Pointer<llama_context> ctx,
);

/// Set whether the context outputs embeddings or not
/// TODO: rename to avoid confusion with llama_get_embeddings()
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>, ffi.Bool)>()
external void llama_set_embeddings(
  ffi.Pointer<llama_context> ctx,
  bool embeddings,
);

/// Set whether to use causal attention or not
/// If set to true, the model will only attend to the past tokens
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>, ffi.Bool)>()
external void llama_set_causal_attn(
  ffi.Pointer<llama_context> ctx,
  bool causal_attn,
);

/// Set whether the model is in warmup mode or not
/// If true, all model tensors are activated during llama_decode() to load and cache their weights.
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>, ffi.Bool)>()
external void llama_set_warmup(
  ffi.Pointer<llama_context> ctx,
  bool warmup,
);

/// Set abort callback
@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<llama_context>,
    ggml_abort_callback,
    ffi.Pointer<ffi.Void>,
  )
>()
external void llama_set_abort_callback(
  ffi.Pointer<llama_context> ctx,
  ggml_abort_callback abort_callback,
  ffi.Pointer<ffi.Void> abort_callback_data,
);

/// Wait until all computations are finished
/// This is automatically done when using one of the functions below to obtain the computation results
/// and is not necessary to call it explicitly in most cases
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>()
external void llama_synchronize(
  ffi.Pointer<llama_context> ctx,
);

/// Token logits obtained from the last call to llama_decode()
/// The logits for which llama_batch.logits[i] != 0 are stored contiguously
/// in the order they have appeared in the batch.
/// Rows: number of tokens for which llama_batch.logits[i] != 0
/// Cols: n_vocab
/// TODO: deprecate in favor of llama_get_logits_ith() (ref: https://github.com/ggml-org/llama.cpp/pull/14853#issuecomment-3113143522)
@ffi.Native<ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>)>()
external ffi.Pointer<ffi.Float> llama_get_logits(
  ffi.Pointer<llama_context> ctx,
);

/// Logits for the ith token. For positive indices, Equivalent to:
/// llama_get_logits(ctx) + ctx->output_ids[i]*n_vocab
/// Negative indicies can be used to access logits in reverse order, -1 is the last logit.
/// returns NULL for invalid ids.
@ffi.Native<
  ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>, ffi.Int32)
>()
external ffi.Pointer<ffi.Float> llama_get_logits_ith(
  ffi.Pointer<llama_context> ctx,
  int i,
);

/// Get all output token embeddings.
/// when pooling_type == LLAMA_POOLING_TYPE_NONE or when using a generative model,
/// the embeddings for which llama_batch.logits[i] != 0 are stored contiguously
/// in the order they have appeared in the batch.
/// shape: [n_outputs*n_embd]
/// Otherwise, returns NULL.
/// TODO: deprecate in favor of llama_get_embeddings_ith() (ref: https://github.com/ggml-org/llama.cpp/pull/14853#issuecomment-3113143522)
@ffi.Native<ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>)>()
external ffi.Pointer<ffi.Float> llama_get_embeddings(
  ffi.Pointer<llama_context> ctx,
);

/// Get the embeddings for the ith token. For positive indices, Equivalent to:
/// llama_get_embeddings(ctx) + ctx->output_ids[i]*n_embd
/// Negative indicies can be used to access embeddings in reverse order, -1 is the last embedding.
/// shape: [n_embd] (1-dimensional)
/// returns NULL for invalid ids.
@ffi.Native<
  ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>, ffi.Int32)
>()
external ffi.Pointer<ffi.Float> llama_get_embeddings_ith(
  ffi.Pointer<llama_context> ctx,
  int i,
);

/// Get the embeddings for a sequence id
/// Returns NULL if pooling_type is LLAMA_POOLING_TYPE_NONE
/// when pooling_type == LLAMA_POOLING_TYPE_RANK, returns float[n_cls_out] with the rank(s) of the sequence
/// otherwise: float[n_embd] (1-dimensional)
@ffi.Native<
  ffi.Pointer<ffi.Float> Function(ffi.Pointer<llama_context>, llama_seq_id)
>()
external ffi.Pointer<ffi.Float> llama_get_embeddings_seq(
  ffi.Pointer<llama_context> ctx,
  int seq_id,
);

/// Vocab
@ffi.Native<
  ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_vocab>, llama_token)
>()
external ffi.Pointer<ffi.Char> llama_vocab_get_text(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

@ffi.Native<ffi.Float Function(ffi.Pointer<llama_vocab>, llama_token)>()
external double llama_vocab_get_score(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

@ffi.Native<ffi.UnsignedInt Function(ffi.Pointer<llama_vocab>, llama_token)>(
  symbol: 'llama_vocab_get_attr',
)
external int _llama_vocab_get_attr(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

llama_token_attr llama_vocab_get_attr(
  ffi.Pointer<llama_vocab> vocab,
  Dartllama_token token,
) => llama_token_attr.fromValue(
  _llama_vocab_get_attr(
    vocab,
    token,
  ),
);

/// Check if the token is supposed to end generation (end-of-generation, eg. EOS, EOT, etc.)
@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>, llama_token)>()
external bool llama_vocab_is_eog(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

/// Identify if Token Id is a control token or a render-able token
@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>, llama_token)>()
external bool llama_vocab_is_control(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

/// Special tokens
@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_bos(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_eos(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_eot(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_sep(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_nl(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_pad(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_mask(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>)>()
external bool llama_vocab_get_add_bos(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>)>()
external bool llama_vocab_get_add_eos(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>)>()
external bool llama_vocab_get_add_sep(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_fim_pre(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_fim_suf(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_fim_mid(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_fim_pad(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_fim_rep(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_fim_sep(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<
  ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_vocab>, llama_token)
>()
external ffi.Pointer<ffi.Char> llama_token_get_text(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

@ffi.Native<ffi.Float Function(ffi.Pointer<llama_vocab>, llama_token)>()
external double llama_token_get_score(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

@ffi.Native<ffi.UnsignedInt Function(ffi.Pointer<llama_vocab>, llama_token)>(
  symbol: 'llama_token_get_attr',
)
external int _llama_token_get_attr(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

llama_token_attr llama_token_get_attr(
  ffi.Pointer<llama_vocab> vocab,
  Dartllama_token token,
) => llama_token_attr.fromValue(
  _llama_token_get_attr(
    vocab,
    token,
  ),
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>, llama_token)>()
external bool llama_token_is_eog(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>, llama_token)>()
external bool llama_token_is_control(
  ffi.Pointer<llama_vocab> vocab,
  int token,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_bos(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_eos(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_eot(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_cls(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_sep(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_nl(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_pad(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>)>()
external bool llama_add_bos_token(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<llama_vocab>)>()
external bool llama_add_eos_token(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_fim_pre(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_fim_suf(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_fim_mid(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_fim_pad(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_fim_rep(
  ffi.Pointer<llama_vocab> vocab,
);

@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_token_fim_sep(
  ffi.Pointer<llama_vocab> vocab,
);

/// CLS is equivalent to BOS
@ffi.Native<llama_token Function(ffi.Pointer<llama_vocab>)>()
external int llama_vocab_cls(
  ffi.Pointer<llama_vocab> vocab,
);

/// @details Convert the provided text into tokens.
/// @param tokens The tokens pointer must be large enough to hold the resulting tokens.
/// @return Returns the number of tokens on success, no more than n_tokens_max
/// @return Returns a negative number on failure - the number of tokens that would have been returned
/// @return Returns INT32_MIN on overflow (e.g., tokenization result size exceeds int32_t limit)
/// @param add_special Allow to add BOS and EOS tokens if model is configured to do so.
/// @param parse_special Allow tokenizing special and/or control tokens which otherwise are not exposed and treated
/// as plaintext. Does not insert a leading space.
@ffi.Native<
  ffi.Int32 Function(
    ffi.Pointer<llama_vocab>,
    ffi.Pointer<ffi.Char>,
    ffi.Int32,
    ffi.Pointer<llama_token>,
    ffi.Int32,
    ffi.Bool,
    ffi.Bool,
  )
>()
external int llama_tokenize(
  ffi.Pointer<llama_vocab> vocab,
  ffi.Pointer<ffi.Char> text,
  int text_len,
  ffi.Pointer<llama_token> tokens,
  int n_tokens_max,
  bool add_special,
  bool parse_special,
);

/// Token Id -> Piece.
/// Uses the vocabulary in the provided context.
/// Does not write null terminator to the buffer.
/// User can skip up to 'lstrip' leading spaces before copying (useful when encoding/decoding multiple tokens with 'add_space_prefix')
/// @param special If true, special tokens are rendered in the output.
@ffi.Native<
  ffi.Int32 Function(
    ffi.Pointer<llama_vocab>,
    llama_token,
    ffi.Pointer<ffi.Char>,
    ffi.Int32,
    ffi.Int32,
    ffi.Bool,
  )
>()
external int llama_token_to_piece(
  ffi.Pointer<llama_vocab> vocab,
  int token,
  ffi.Pointer<ffi.Char> buf,
  int length,
  int lstrip,
  bool special,
);

/// @details Convert the provided tokens into text (inverse of llama_tokenize()).
/// @param text The char pointer must be large enough to hold the resulting text.
/// @return Returns the number of chars/bytes on success, no more than text_len_max.
/// @return Returns a negative number on failure - the number of chars/bytes that would have been returned.
/// @param remove_special Allow to remove BOS and EOS tokens if model is configured to do so.
/// @param unparse_special If true, special tokens are rendered in the output.
@ffi.Native<
  ffi.Int32 Function(
    ffi.Pointer<llama_vocab>,
    ffi.Pointer<llama_token>,
    ffi.Int32,
    ffi.Pointer<ffi.Char>,
    ffi.Int32,
    ffi.Bool,
    ffi.Bool,
  )
>()
external int llama_detokenize(
  ffi.Pointer<llama_vocab> vocab,
  ffi.Pointer<llama_token> tokens,
  int n_tokens,
  ffi.Pointer<ffi.Char> text,
  int text_len_max,
  bool remove_special,
  bool unparse_special,
);

/// Apply chat template. Inspired by hf apply_chat_template() on python.
/// Both "model" and "custom_template" are optional, but at least one is required. "custom_template" has higher precedence than "model"
/// NOTE: This function does not use a jinja parser. It only support a pre-defined list of template. See more: https://github.com/ggml-org/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template
/// @param tmpl A Jinja template to use for this chat. If this is nullptr, the models default chat template will be used instead.
/// @param chat Pointer to a list of multiple llama_chat_message
/// @param n_msg Number of llama_chat_message in this chat
/// @param add_ass Whether to end the prompt with the token(s) that indicate the start of an assistant message.
/// @param buf A buffer to hold the output formatted prompt. The recommended alloc size is 2 * (total number of characters of all messages)
/// @param length The size of the allocated buffer
/// @return The total number of bytes of the formatted prompt. If is it larger than the size of buffer, you may need to re-alloc it and then re-apply the template.
@ffi.Native<
  ffi.Int32 Function(
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<llama_chat_message>,
    ffi.Size,
    ffi.Bool,
    ffi.Pointer<ffi.Char>,
    ffi.Int32,
  )
>()
external int llama_chat_apply_template(
  ffi.Pointer<ffi.Char> tmpl,
  ffi.Pointer<llama_chat_message> chat,
  int n_msg,
  bool add_ass,
  ffi.Pointer<ffi.Char> buf,
  int length,
);

/// Get list of built-in chat templates
@ffi.Native<ffi.Int32 Function(ffi.Pointer<ffi.Pointer<ffi.Char>>, ffi.Size)>()
external int llama_chat_builtin_templates(
  ffi.Pointer<ffi.Pointer<ffi.Char>> output,
  int len,
);

/// mirror of llama_sampler_i:
@ffi.Native<
  ffi.Pointer<llama_sampler> Function(
    ffi.Pointer<llama_sampler_i>,
    llama_sampler_context_t,
  )
>()
external ffi.Pointer<llama_sampler> llama_sampler_init(
  ffi.Pointer<llama_sampler_i> iface,
  llama_sampler_context_t ctx,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_sampler>)>()
external ffi.Pointer<ffi.Char> llama_sampler_name(
  ffi.Pointer<llama_sampler> smpl,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_sampler>, llama_token)>()
external void llama_sampler_accept(
  ffi.Pointer<llama_sampler> smpl,
  int token,
);

@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<llama_sampler>,
    ffi.Pointer<llama_token_data_array>,
  )
>()
external void llama_sampler_apply(
  ffi.Pointer<llama_sampler> smpl,
  ffi.Pointer<llama_token_data_array> cur_p,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_sampler>)>()
external void llama_sampler_reset(
  ffi.Pointer<llama_sampler> smpl,
);

@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_sampler>)>()
external ffi.Pointer<llama_sampler> llama_sampler_clone(
  ffi.Pointer<llama_sampler> smpl,
);

/// important: do not free if the sampler has been added to a llama_sampler_chain (via llama_sampler_chain_add)
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_sampler>)>()
external void llama_sampler_free(
  ffi.Pointer<llama_sampler> smpl,
);

/// llama_sampler_chain
/// a type of llama_sampler that can chain multiple samplers one after another
@ffi.Native<ffi.Pointer<llama_sampler> Function(llama_sampler_chain_params)>()
external ffi.Pointer<llama_sampler> llama_sampler_chain_init(
  llama_sampler_chain_params params,
);

/// important: takes ownership of the sampler object and will free it when llama_sampler_free is called
@ffi.Native<
  ffi.Void Function(ffi.Pointer<llama_sampler>, ffi.Pointer<llama_sampler>)
>()
external void llama_sampler_chain_add(
  ffi.Pointer<llama_sampler> chain,
  ffi.Pointer<llama_sampler> smpl,
);

@ffi.Native<
  ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_sampler>, ffi.Int32)
>()
external ffi.Pointer<llama_sampler> llama_sampler_chain_get(
  ffi.Pointer<llama_sampler> chain,
  int i,
);

@ffi.Native<ffi.Int Function(ffi.Pointer<llama_sampler>)>()
external int llama_sampler_chain_n(
  ffi.Pointer<llama_sampler> chain,
);

/// after removing a sampler, the chain will no longer own it, and it will not be freed when the chain is freed
@ffi.Native<
  ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_sampler>, ffi.Int32)
>()
external ffi.Pointer<llama_sampler> llama_sampler_chain_remove(
  ffi.Pointer<llama_sampler> chain,
  int i,
);

/// available samplers:
@ffi.Native<ffi.Pointer<llama_sampler> Function()>()
external ffi.Pointer<llama_sampler> llama_sampler_init_greedy();

@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Uint32)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_dist(
  int seed,
);

/// @details Top-K sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
/// Setting k <= 0 makes this a noop
@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Int32)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_top_k(
  int k,
);

/// @details Nucleus sampling described in academic paper "The Curious Case of Neural Text Degeneration" https://arxiv.org/abs/1904.09751
@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Float, ffi.Size)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_top_p(
  double p,
  int min_keep,
);

/// @details Minimum P sampling as described in https://github.com/ggml-org/llama.cpp/pull/3841
@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Float, ffi.Size)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_min_p(
  double p,
  int min_keep,
);

/// @details Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.
@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Float, ffi.Size)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_typical(
  double p,
  int min_keep,
);

/// #details Updates the logits l_i` = l_i/t. When t <= 0.0f, the maximum logit is kept at it's original value, the rest are set to -inf
@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Float)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_temp(
  double t,
);

/// @details Dynamic temperature implementation (a.k.a. entropy) described in the paper https://arxiv.org/abs/2309.02772.
@ffi.Native<
  ffi.Pointer<llama_sampler> Function(ffi.Float, ffi.Float, ffi.Float)
>()
external ffi.Pointer<llama_sampler> llama_sampler_init_temp_ext(
  double t,
  double delta,
  double exponent,
);

/// @details XTC sampler as described in https://github.com/oobabooga/text-generation-webui/pull/6335
@ffi.Native<
  ffi.Pointer<llama_sampler> Function(
    ffi.Float,
    ffi.Float,
    ffi.Size,
    ffi.Uint32,
  )
>()
external ffi.Pointer<llama_sampler> llama_sampler_init_xtc(
  double p,
  double t,
  int min_keep,
  int seed,
);

/// @details Top n sigma sampling as described in academic paper "Top-n: Not All Logits Are You Need" https://arxiv.org/pdf/2411.07641
@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Float)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_top_n_sigma(
  double n,
);

/// @details Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
/// @param candidates A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
/// @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
/// @param eta The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.
/// @param m The number of tokens considered in the estimation of `s_hat`. This is an arbitrary value that is used to calculate `s_hat`, which in turn helps to calculate the value of `k`. In the paper, they use `m = 100`, but you can experiment with different values to see how it affects the performance of the algorithm.
/// @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.
@ffi.Native<
  ffi.Pointer<llama_sampler> Function(
    ffi.Int32,
    ffi.Uint32,
    ffi.Float,
    ffi.Float,
    ffi.Int32,
  )
>()
external ffi.Pointer<llama_sampler> llama_sampler_init_mirostat(
  int n_vocab,
  int seed,
  double tau,
  double eta,
  int m,
);

/// @details Mirostat 2.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.
/// @param candidates A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.
/// @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.
/// @param eta The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.
/// @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.
@ffi.Native<
  ffi.Pointer<llama_sampler> Function(ffi.Uint32, ffi.Float, ffi.Float)
>()
external ffi.Pointer<llama_sampler> llama_sampler_init_mirostat_v2(
  int seed,
  double tau,
  double eta,
);

/// @details Intializes a GBNF grammar, see grammars/README.md for details.
/// @param vocab The vocabulary that this grammar will be used with.
/// @param grammar_str The production rules for the grammar, encoded as a string. Returns an empty grammar if empty. Returns NULL if parsing of grammar_str fails.
/// @param grammar_root The name of the start symbol for the grammar.
@ffi.Native<
  ffi.Pointer<llama_sampler> Function(
    ffi.Pointer<llama_vocab>,
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<ffi.Char>,
  )
>()
external ffi.Pointer<llama_sampler> llama_sampler_init_grammar(
  ffi.Pointer<llama_vocab> vocab,
  ffi.Pointer<ffi.Char> grammar_str,
  ffi.Pointer<ffi.Char> grammar_root,
);

@ffi.Native<
  ffi.Pointer<llama_sampler> Function(
    ffi.Pointer<llama_vocab>,
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<ffi.Pointer<ffi.Char>>,
    ffi.Size,
    ffi.Pointer<llama_token>,
    ffi.Size,
  )
>()
external ffi.Pointer<llama_sampler> llama_sampler_init_grammar_lazy(
  ffi.Pointer<llama_vocab> vocab,
  ffi.Pointer<ffi.Char> grammar_str,
  ffi.Pointer<ffi.Char> grammar_root,
  ffi.Pointer<ffi.Pointer<ffi.Char>> trigger_words,
  int num_trigger_words,
  ffi.Pointer<llama_token> trigger_tokens,
  int num_trigger_tokens,
);

/// @details Lazy grammar sampler, introduced in https://github.com/ggml-org/llama.cpp/pull/9639
/// @param trigger_patterns A list of patterns that will trigger the grammar sampler. Pattern will be matched from the start of the generation output, and grammar sampler will be fed content starting from its first match group.
/// @param trigger_tokens A list of tokens that will trigger the grammar sampler. Grammar sampler will be fed content starting from the trigger token included.
@ffi.Native<
  ffi.Pointer<llama_sampler> Function(
    ffi.Pointer<llama_vocab>,
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<ffi.Pointer<ffi.Char>>,
    ffi.Size,
    ffi.Pointer<llama_token>,
    ffi.Size,
  )
>()
external ffi.Pointer<llama_sampler> llama_sampler_init_grammar_lazy_patterns(
  ffi.Pointer<llama_vocab> vocab,
  ffi.Pointer<ffi.Char> grammar_str,
  ffi.Pointer<ffi.Char> grammar_root,
  ffi.Pointer<ffi.Pointer<ffi.Char>> trigger_patterns,
  int num_trigger_patterns,
  ffi.Pointer<llama_token> trigger_tokens,
  int num_trigger_tokens,
);

/// NOTE: Avoid using on the full vocabulary as searching for repeated tokens can become slow. For example, apply top-k or top-p sampling first.
@ffi.Native<
  ffi.Pointer<llama_sampler> Function(
    ffi.Int32,
    ffi.Float,
    ffi.Float,
    ffi.Float,
  )
>()
external ffi.Pointer<llama_sampler> llama_sampler_init_penalties(
  int penalty_last_n,
  double penalty_repeat,
  double penalty_freq,
  double penalty_present,
);

/// @details DRY sampler, designed by p-e-w, as described in: https://github.com/oobabooga/text-generation-webui/pull/5677, porting Koboldcpp implementation authored by pi6am: https://github.com/LostRuins/koboldcpp/pull/982
@ffi.Native<
  ffi.Pointer<llama_sampler> Function(
    ffi.Pointer<llama_vocab>,
    ffi.Int32,
    ffi.Float,
    ffi.Float,
    ffi.Int32,
    ffi.Int32,
    ffi.Pointer<ffi.Pointer<ffi.Char>>,
    ffi.Size,
  )
>()
external ffi.Pointer<llama_sampler> llama_sampler_init_dry(
  ffi.Pointer<llama_vocab> vocab,
  int n_ctx_train,
  double dry_multiplier,
  double dry_base,
  int dry_allowed_length,
  int dry_penalty_last_n,
  ffi.Pointer<ffi.Pointer<ffi.Char>> seq_breakers,
  int num_breakers,
);

@ffi.Native<
  ffi.Pointer<llama_sampler> Function(
    ffi.Int32,
    ffi.Int32,
    ffi.Pointer<llama_logit_bias>,
  )
>()
external ffi.Pointer<llama_sampler> llama_sampler_init_logit_bias(
  int n_vocab,
  int n_logit_bias,
  ffi.Pointer<llama_logit_bias> logit_bias,
);

/// this sampler is meant to be used for fill-in-the-middle infilling
/// it's supposed to be used after top_k + top_p sampling
///
/// 1. if the sum of the EOG probs times the number of candidates is higher than the sum of the other probs -> pick EOG
/// 2. combine probs of tokens that have the same prefix
///
/// example:
///
/// - before:
/// "hel":   0.5
/// "hell":  0.2
/// "hello": 0.1
/// "dummy": 0.1
///
/// - after:
/// "hel":   0.8
/// "dummy": 0.1
///
/// 3. discard non-EOG tokens with low prob
/// 4. if no tokens are left -> pick EOT
@ffi.Native<ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_vocab>)>()
external ffi.Pointer<llama_sampler> llama_sampler_init_infill(
  ffi.Pointer<llama_vocab> vocab,
);

/// Returns the seed used by the sampler if applicable, LLAMA_DEFAULT_SEED otherwise
@ffi.Native<ffi.Uint32 Function(ffi.Pointer<llama_sampler>)>()
external int llama_sampler_get_seed(
  ffi.Pointer<llama_sampler> smpl,
);

/// @details Sample and accept a token from the idx-th output of the last evaluation
///
/// Shorthand for:
/// const auto * logits = llama_get_logits_ith(ctx, idx);
/// llama_token_data_array cur_p = { ... init from logits ... };
/// llama_sampler_apply(smpl, &cur_p);
/// auto token = cur_p.data[cur_p.selected].id;
/// llama_sampler_accept(smpl, token);
/// return token;
/// Returns the sampled token
@ffi.Native<
  llama_token Function(
    ffi.Pointer<llama_sampler>,
    ffi.Pointer<llama_context>,
    ffi.Int32,
  )
>()
external int llama_sampler_sample(
  ffi.Pointer<llama_sampler> smpl,
  ffi.Pointer<llama_context> ctx,
  int idx,
);

/// @details Build a split GGUF final path for this chunk.
/// llama_split_path(split_path, sizeof(split_path), "/models/ggml-model-q4_0", 2, 4) => split_path = "/models/ggml-model-q4_0-00002-of-00004.gguf"
/// Returns the split_path length.
@ffi.Native<
  ffi.Int Function(
    ffi.Pointer<ffi.Char>,
    ffi.Size,
    ffi.Pointer<ffi.Char>,
    ffi.Int,
    ffi.Int,
  )
>()
external int llama_split_path(
  ffi.Pointer<ffi.Char> split_path,
  int maxlen,
  ffi.Pointer<ffi.Char> path_prefix,
  int split_no,
  int split_count,
);

/// @details Extract the path prefix from the split_path if and only if the split_no and split_count match.
/// llama_split_prefix(split_prefix, 64, "/models/ggml-model-q4_0-00002-of-00004.gguf", 2, 4) => split_prefix = "/models/ggml-model-q4_0"
/// Returns the split_prefix length.
@ffi.Native<
  ffi.Int Function(
    ffi.Pointer<ffi.Char>,
    ffi.Size,
    ffi.Pointer<ffi.Char>,
    ffi.Int,
    ffi.Int,
  )
>()
external int llama_split_prefix(
  ffi.Pointer<ffi.Char> split_prefix,
  int maxlen,
  ffi.Pointer<ffi.Char> split_path,
  int split_no,
  int split_count,
);

/// Print system information
@ffi.Native<ffi.Pointer<ffi.Char> Function()>()
external ffi.Pointer<ffi.Char> llama_print_system_info();

/// Set callback for all future logging events.
/// If this is not called, or NULL is supplied, everything is output on stderr.
/// The logger state is global so these functions are NOT thread safe.
@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<ggml_log_callback>,
    ffi.Pointer<ffi.Pointer<ffi.Void>>,
  )
>()
external void llama_log_get(
  ffi.Pointer<ggml_log_callback> log_callback,
  ffi.Pointer<ffi.Pointer<ffi.Void>> user_data,
);

@ffi.Native<ffi.Void Function(ggml_log_callback, ffi.Pointer<ffi.Void>)>()
external void llama_log_set(
  ggml_log_callback log_callback,
  ffi.Pointer<ffi.Void> user_data,
);

@ffi.Native<llama_perf_context_data Function(ffi.Pointer<llama_context>)>()
external llama_perf_context_data llama_perf_context(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>()
external void llama_perf_context_print(
  ffi.Pointer<llama_context> ctx,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>()
external void llama_perf_context_reset(
  ffi.Pointer<llama_context> ctx,
);

/// NOTE: the following work only with samplers constructed via llama_sampler_chain_init
@ffi.Native<llama_perf_sampler_data Function(ffi.Pointer<llama_sampler>)>()
external llama_perf_sampler_data llama_perf_sampler(
  ffi.Pointer<llama_sampler> chain,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_sampler>)>()
external void llama_perf_sampler_print(
  ffi.Pointer<llama_sampler> chain,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<llama_sampler>)>()
external void llama_perf_sampler_reset(
  ffi.Pointer<llama_sampler> chain,
);

/// print a breakdown of per-device memory use via LLAMA_LOG:
@ffi.Native<ffi.Void Function(ffi.Pointer<llama_context>)>()
external void llama_memory_breakdown_print(
  ffi.Pointer<llama_context> ctx,
);

/// always returns true
@ffi.Native<
  ffi.Bool Function(ffi.Pointer<ggml_tensor>, ffi.Pointer<ffi.Void>)
>()
external bool llama_opt_param_filter_all(
  ffi.Pointer<ggml_tensor> tensor,
  ffi.Pointer<ffi.Void> userdata,
);

@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<llama_context>,
    ffi.Pointer<llama_model>,
    llama_opt_params,
  )
>()
external void llama_opt_init(
  ffi.Pointer<llama_context> lctx,
  ffi.Pointer<llama_model> model,
  llama_opt_params lopt_params,
);

@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<llama_context>,
    ggml_opt_dataset_t,
    ggml_opt_result_t,
    ggml_opt_result_t,
    ffi.Int64,
    ggml_opt_epoch_callback,
    ggml_opt_epoch_callback,
  )
>()
external void llama_opt_epoch(
  ffi.Pointer<llama_context> lctx,
  ggml_opt_dataset_t dataset,
  ggml_opt_result_t result_train,
  ggml_opt_result_t result_eval,
  int idata_split,
  ggml_opt_epoch_callback callback_train,
  ggml_opt_epoch_callback callback_eval,
);

@ffi.Native<
  ffi.Int Function(
    lcpp_sampling_params_t,
    ffi.Pointer<ffi.Pointer<lcpp_common_chat_msg_t>>,
    ffi.Int,
    ffi.Pointer<ffi.Pointer<lcpp_common_chat_tool_t>>,
    ffi.Int,
  )
>()
external int lcpp_prompt(
  lcpp_sampling_params_t sampling_params,
  ffi.Pointer<ffi.Pointer<lcpp_common_chat_msg_t>> messages,
  int n_messages,
  ffi.Pointer<ffi.Pointer<lcpp_common_chat_tool_t>> tools,
  int n_tools,
);

@ffi.Native<lcpp_params_t Function()>()
external lcpp_params_t lcpp_params_defaults();

@ffi.Native<lcpp_sampling_params_t Function()>()
external lcpp_sampling_params_t lcpp_sampling_params_defaults();

@ffi.Native<ffi.Void Function(llama_context_params_t, lcpp_params_t)>()
external void lcpp_reconfigure(
  llama_context_params_t context_params,
  lcpp_params_t lcpp_params$1,
);

@ffi.Native<ffi.Void Function(LppTokenStreamCallback)>()
external void lcpp_set_token_stream_callback(
  LppTokenStreamCallback newtoken_callback,
);

@ffi.Native<ffi.Void Function()>()
external void lcpp_unset_token_stream_callback();

@ffi.Native<ffi.Void Function(LppChatMessageCallback)>()
external void lcpp_set_chat_message_callback(
  LppChatMessageCallback chat_msg_callback,
);

@ffi.Native<ffi.Void Function()>()
external void lcpp_unset_chat_message_callback();

@ffi.Native<ffi.Void Function(LcppOnCancelCallback)>()
external void lcpp_set_on_cancel_callback(
  LcppOnCancelCallback on_cancel_callback,
);

@ffi.Native<ffi.Void Function()>()
external void lcpp_unset_on_cancel_callback();

@ffi.Native<ffi.Void Function(LcppOnAbortCallback)>()
external void lcpp_set_on_abort_callback(
  LcppOnAbortCallback on_abort_callback,
);

@ffi.Native<ffi.Void Function()>()
external void lcpp_unset_on_abort_callback();

@ffi.Native<ffi.Void Function(LppProgressCallback)>()
external void lcpp_set_model_load_progress_callback(
  LppProgressCallback model_loading_callback,
);

@ffi.Native<ffi.Void Function()>()
external void lcpp_unset_model_load_progress_callback();

@ffi.Native<
  ffi.Int32 Function(
    ffi.Pointer<ffi.Char>,
    ffi.Int,
    ffi.Bool,
    ffi.Bool,
    ffi.Pointer<ffi.Pointer<llama_token>>,
  )
>()
external int lcpp_tokenize(
  ffi.Pointer<ffi.Char> text,
  int n_text,
  bool add_special,
  bool parse_special,
  ffi.Pointer<ffi.Pointer<llama_token>> tokens,
);

@ffi.Native<ffi.Void Function(ffi.Bool)>()
external void lcpp_send_abort_signal(
  bool abort,
);

@ffi.Native<ffi.Void Function(ffi.Bool)>()
external void lcpp_send_cancel_signal(
  bool cancel,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<lcpp_common_chat_msg_t>)>()
external void lcpp_free_common_chat_msg(
  ffi.Pointer<lcpp_common_chat_msg_t> msg,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<LcppTextStruct_t>)>()
external void lcpp_free_text(
  ffi.Pointer<LcppTextStruct_t> ptr,
);

@ffi.Native<ffi.Void Function()>()
external void lcpp_reset();

@ffi.Native<ffi.Void Function()>()
external void lcpp_unload();

@ffi.Native<ffi.Void Function()>()
external void lcpp_destroy();

@ffi.Native<ffi.Void Function()>()
external void lcpp_initialize();

@ffi.Native<ffi.Pointer<lcpp_machine_info_t> Function()>()
external ffi.Pointer<lcpp_machine_info_t> lcpp_get_machine_info();

@ffi.Native<ffi.Void Function(ffi.Pointer<lcpp_machine_info_t>)>()
external void lcpp_free_machine_info(
  ffi.Pointer<lcpp_machine_info_t> mach_info,
);

@ffi.Native<ffi.Pointer<lcpp_model_rt_t> Function(ffi.Pointer<ffi.Char>)>()
external ffi.Pointer<lcpp_model_rt_t> lcpp_model_details(
  ffi.Pointer<ffi.Char> model_path,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<lcpp_model_mem_t>)>()
external void lcpp_free_model_mem(
  ffi.Pointer<lcpp_model_mem_t> model_mem,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<lcpp_model_rt_t>)>()
external void lcpp_free_model_rt(
  ffi.Pointer<lcpp_model_rt_t> model_rt,
);

@ffi.Native<ffi.Pointer<lcpp_model_info_t> Function(ffi.Pointer<ffi.Char>)>()
external ffi.Pointer<lcpp_model_info_t> lcpp_get_model_info(
  ffi.Pointer<ffi.Char> model_file,
);

@ffi.Native<ffi.Void Function(ffi.Pointer<lcpp_model_info_t>)>()
external void lcpp_free_model_info(
  ffi.Pointer<lcpp_model_info_t> model_info,
);

@ffi.Native<ffi.Pointer<gguf_context> Function()>()
external ffi.Pointer<gguf_context> gguf_init_empty();

@ffi.Native<
  ffi.Pointer<gguf_context> Function(ffi.Pointer<ffi.Char>, gguf_init_params)
>()
external ffi.Pointer<gguf_context> gguf_init_from_file(
  ffi.Pointer<ffi.Char> fname,
  gguf_init_params params,
);

/// GGML_API struct gguf_context * gguf_init_from_buffer(..);
@ffi.Native<ffi.Void Function(ffi.Pointer<gguf_context>)>()
external void gguf_free(
  ffi.Pointer<gguf_context> ctx,
);

@ffi.Native<ffi.Pointer<ffi.Char> Function(ffi.UnsignedInt)>(
  symbol: 'gguf_type_name',
)
external ffi.Pointer<ffi.Char> _gguf_type_name(
  int type,
);

ffi.Pointer<ffi.Char> gguf_type_name(
  gguf_type type,
) => _gguf_type_name(
  type.value,
);

@ffi.Native<ffi.Uint32 Function(ffi.Pointer<gguf_context>)>()
external int gguf_get_version(
  ffi.Pointer<gguf_context> ctx,
);

@ffi.Native<ffi.Size Function(ffi.Pointer<gguf_context>)>()
external int gguf_get_alignment(
  ffi.Pointer<gguf_context> ctx,
);

@ffi.Native<ffi.Size Function(ffi.Pointer<gguf_context>)>()
external int gguf_get_data_offset(
  ffi.Pointer<gguf_context> ctx,
);

@ffi.Native<ffi.Int64 Function(ffi.Pointer<gguf_context>)>()
external int gguf_get_n_kv(
  ffi.Pointer<gguf_context> ctx,
);

@ffi.Native<
  ffi.Int64 Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>)
>()
external int gguf_find_key(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> key,
);

@ffi.Native<
  ffi.Pointer<ffi.Char> Function(ffi.Pointer<gguf_context>, ffi.Int64)
>()
external ffi.Pointer<ffi.Char> gguf_get_key(
  ffi.Pointer<gguf_context> ctx,
  int key_id,
);

@ffi.Native<ffi.UnsignedInt Function(ffi.Pointer<gguf_context>, ffi.Int64)>(
  symbol: 'gguf_get_kv_type',
)
external int _gguf_get_kv_type(
  ffi.Pointer<gguf_context> ctx,
  int key_id,
);

gguf_type gguf_get_kv_type(
  ffi.Pointer<gguf_context> ctx,
  int key_id,
) => gguf_type.fromValue(
  _gguf_get_kv_type(
    ctx,
    key_id,
  ),
);

@ffi.Native<ffi.UnsignedInt Function(ffi.Pointer<gguf_context>, ffi.Int64)>(
  symbol: 'gguf_get_arr_type',
)
external int _gguf_get_arr_type(
  ffi.Pointer<gguf_context> ctx,
  int key_id,
);

gguf_type gguf_get_arr_type(
  ffi.Pointer<gguf_context> ctx,
  int key_id,
) => gguf_type.fromValue(
  _gguf_get_arr_type(
    ctx,
    key_id,
  ),
);

/// will abort if the wrong type is used for the key
@ffi.Native<ffi.Uint8 Function(ffi.Pointer<gguf_context>, ffi.Int64)>()
external int gguf_get_val_u8(
  ffi.Pointer<gguf_context> ctx,
  int key_id,
);

@ffi.Native<ffi.Int8 Function(ffi.Pointer<gguf_context>, ffi.Int64)>()
external int gguf_get_val_i8(
  ffi.Pointer<gguf_context> ctx,
  int key_id,
);

@ffi.Native<ffi.Uint16 Function(ffi.Pointer<gguf_context>, ffi.Int64)>()
external int gguf_get_val_u16(
  ffi.Pointer<gguf_context> ctx,
  int key_id,
);

@ffi.Native<ffi.Int16 Function(ffi.Pointer<gguf_context>, ffi.Int64)>()
external int gguf_get_val_i16(
  ffi.Pointer<gguf_context> ctx,
  int key_id,
);

@ffi.Native<ffi.Uint32 Function(ffi.Pointer<gguf_context>, ffi.Int64)>()
external int gguf_get_val_u32(
  ffi.Pointer<gguf_context> ctx,
  int key_id,
);

@ffi.Native<ffi.Int32 Function(ffi.Pointer<gguf_context>, ffi.Int64)>()
external int gguf_get_val_i32(
  ffi.Pointer<gguf_context> ctx,
  int key_id,
);

@ffi.Native<ffi.Float Function(ffi.Pointer<gguf_context>, ffi.Int64)>()
external double gguf_get_val_f32(
  ffi.Pointer<gguf_context> ctx,
  int key_id,
);

@ffi.Native<ffi.Uint64 Function(ffi.Pointer<gguf_context>, ffi.Int64)>()
external int gguf_get_val_u64(
  ffi.Pointer<gguf_context> ctx,
  int key_id,
);

@ffi.Native<ffi.Int64 Function(ffi.Pointer<gguf_context>, ffi.Int64)>()
external int gguf_get_val_i64(
  ffi.Pointer<gguf_context> ctx,
  int key_id,
);

@ffi.Native<ffi.Double Function(ffi.Pointer<gguf_context>, ffi.Int64)>()
external double gguf_get_val_f64(
  ffi.Pointer<gguf_context> ctx,
  int key_id,
);

@ffi.Native<ffi.Bool Function(ffi.Pointer<gguf_context>, ffi.Int64)>()
external bool gguf_get_val_bool(
  ffi.Pointer<gguf_context> ctx,
  int key_id,
);

@ffi.Native<
  ffi.Pointer<ffi.Char> Function(ffi.Pointer<gguf_context>, ffi.Int64)
>()
external ffi.Pointer<ffi.Char> gguf_get_val_str(
  ffi.Pointer<gguf_context> ctx,
  int key_id,
);

@ffi.Native<
  ffi.Pointer<ffi.Void> Function(ffi.Pointer<gguf_context>, ffi.Int64)
>()
external ffi.Pointer<ffi.Void> gguf_get_val_data(
  ffi.Pointer<gguf_context> ctx,
  int key_id,
);

@ffi.Native<ffi.Size Function(ffi.Pointer<gguf_context>, ffi.Int64)>()
external int gguf_get_arr_n(
  ffi.Pointer<gguf_context> ctx,
  int key_id,
);

/// get raw pointer to the first element of the array with the given key_id
/// for bool arrays, note that they are always stored as int8 on all platforms (usually this makes no difference)
@ffi.Native<
  ffi.Pointer<ffi.Void> Function(ffi.Pointer<gguf_context>, ffi.Int64)
>()
external ffi.Pointer<ffi.Void> gguf_get_arr_data(
  ffi.Pointer<gguf_context> ctx,
  int key_id,
);

/// get ith C string from array with given key_id
@ffi.Native<
  ffi.Pointer<ffi.Char> Function(ffi.Pointer<gguf_context>, ffi.Int64, ffi.Size)
>()
external ffi.Pointer<ffi.Char> gguf_get_arr_str(
  ffi.Pointer<gguf_context> ctx,
  int key_id,
  int i,
);

@ffi.Native<ffi.Int64 Function(ffi.Pointer<gguf_context>)>()
external int gguf_get_n_tensors(
  ffi.Pointer<gguf_context> ctx,
);

@ffi.Native<
  ffi.Int64 Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>)
>()
external int gguf_find_tensor(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> name,
);

@ffi.Native<ffi.Size Function(ffi.Pointer<gguf_context>, ffi.Int64)>()
external int gguf_get_tensor_offset(
  ffi.Pointer<gguf_context> ctx,
  int tensor_id,
);

@ffi.Native<
  ffi.Pointer<ffi.Char> Function(ffi.Pointer<gguf_context>, ffi.Int64)
>()
external ffi.Pointer<ffi.Char> gguf_get_tensor_name(
  ffi.Pointer<gguf_context> ctx,
  int tensor_id,
);

@ffi.Native<ffi.UnsignedInt Function(ffi.Pointer<gguf_context>, ffi.Int64)>(
  symbol: 'gguf_get_tensor_type',
)
external int _gguf_get_tensor_type(
  ffi.Pointer<gguf_context> ctx,
  int tensor_id,
);

ggml_type gguf_get_tensor_type(
  ffi.Pointer<gguf_context> ctx,
  int tensor_id,
) => ggml_type.fromValue(
  _gguf_get_tensor_type(
    ctx,
    tensor_id,
  ),
);

@ffi.Native<ffi.Size Function(ffi.Pointer<gguf_context>, ffi.Int64)>()
external int gguf_get_tensor_size(
  ffi.Pointer<gguf_context> ctx,
  int tensor_id,
);

/// removes key if it exists, returns id that the key had prior to removal (-1 if it didn't exist)
@ffi.Native<
  ffi.Int64 Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>)
>()
external int gguf_remove_key(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> key,
);

/// overrides an existing KV pair or adds a new one, the new KV pair is always at the back
@ffi.Native<
  ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, ffi.Uint8)
>()
external void gguf_set_val_u8(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> key,
  int val,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, ffi.Int8)
>()
external void gguf_set_val_i8(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> key,
  int val,
);

@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<gguf_context>,
    ffi.Pointer<ffi.Char>,
    ffi.Uint16,
  )
>()
external void gguf_set_val_u16(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> key,
  int val,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, ffi.Int16)
>()
external void gguf_set_val_i16(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> key,
  int val,
);

@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<gguf_context>,
    ffi.Pointer<ffi.Char>,
    ffi.Uint32,
  )
>()
external void gguf_set_val_u32(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> key,
  int val,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, ffi.Int32)
>()
external void gguf_set_val_i32(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> key,
  int val,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, ffi.Float)
>()
external void gguf_set_val_f32(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> key,
  double val,
);

@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<gguf_context>,
    ffi.Pointer<ffi.Char>,
    ffi.Uint64,
  )
>()
external void gguf_set_val_u64(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> key,
  int val,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, ffi.Int64)
>()
external void gguf_set_val_i64(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> key,
  int val,
);

@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<gguf_context>,
    ffi.Pointer<ffi.Char>,
    ffi.Double,
  )
>()
external void gguf_set_val_f64(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> key,
  double val,
);

@ffi.Native<
  ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, ffi.Bool)
>()
external void gguf_set_val_bool(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> key,
  bool val,
);

@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<gguf_context>,
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<ffi.Char>,
  )
>()
external void gguf_set_val_str(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> key,
  ffi.Pointer<ffi.Char> val,
);

/// creates a new array with n elements of the given type and copies the corresponding number of bytes from data
@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<gguf_context>,
    ffi.Pointer<ffi.Char>,
    ffi.UnsignedInt,
    ffi.Pointer<ffi.Void>,
    ffi.Size,
  )
>(symbol: 'gguf_set_arr_data')
external void _gguf_set_arr_data(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> key,
  int type,
  ffi.Pointer<ffi.Void> data,
  int n,
);

void gguf_set_arr_data(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> key,
  gguf_type type,
  ffi.Pointer<ffi.Void> data,
  int n,
) => _gguf_set_arr_data(
  ctx,
  key,
  type.value,
  data,
  n,
);

/// creates a new array with n strings and copies the corresponding strings from data
@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<gguf_context>,
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<ffi.Pointer<ffi.Char>>,
    ffi.Size,
  )
>()
external void gguf_set_arr_str(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> key,
  ffi.Pointer<ffi.Pointer<ffi.Char>> data,
  int n,
);

/// set or add KV pairs from another context
@ffi.Native<
  ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<gguf_context>)
>()
external void gguf_set_kv(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<gguf_context> src,
);

/// add tensor to GGUF context, tensor name must be unique
@ffi.Native<
  ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ggml_tensor>)
>()
external void gguf_add_tensor(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ggml_tensor> tensor,
);

/// after changing a tensor's type, the offsets of all tensors with higher indices are immediately recalculated
/// in such a way that the tensor data remains as one contiguous block (except for padding)
@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<gguf_context>,
    ffi.Pointer<ffi.Char>,
    ffi.UnsignedInt,
  )
>(symbol: 'gguf_set_tensor_type')
external void _gguf_set_tensor_type(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> name,
  int type,
);

void gguf_set_tensor_type(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> name,
  ggml_type type,
) => _gguf_set_tensor_type(
  ctx,
  name,
  type.value,
);

/// assumes that at least gguf_get_tensor_size bytes can be read from data
@ffi.Native<
  ffi.Void Function(
    ffi.Pointer<gguf_context>,
    ffi.Pointer<ffi.Char>,
    ffi.Pointer<ffi.Void>,
  )
>()
external void gguf_set_tensor_data(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> name,
  ffi.Pointer<ffi.Void> data,
);

/// write the entire context to a binary file
@ffi.Native<
  ffi.Bool Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Char>, ffi.Bool)
>()
external bool gguf_write_to_file(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Char> fname,
  bool only_meta,
);

/// get the size in bytes of the meta data (header, kv pairs, tensor info) including padding
@ffi.Native<ffi.Size Function(ffi.Pointer<gguf_context>)>()
external int gguf_get_meta_size(
  ffi.Pointer<gguf_context> ctx,
);

/// writes the meta data to pointer "data"
@ffi.Native<
  ffi.Void Function(ffi.Pointer<gguf_context>, ffi.Pointer<ffi.Void>)
>()
external void gguf_get_meta_data(
  ffi.Pointer<gguf_context> ctx,
  ffi.Pointer<ffi.Void> data,
);

typedef va_list = ffi.Pointer<ffi.Char>;
typedef ptrdiff_t = ffi.LongLong;
typedef Dartptrdiff_t = int;
typedef int_least8_t = ffi.SignedChar;
typedef Dartint_least8_t = int;
typedef int_least16_t = ffi.Short;
typedef Dartint_least16_t = int;
typedef int_least32_t = ffi.Int;
typedef Dartint_least32_t = int;
typedef int_least64_t = ffi.LongLong;
typedef Dartint_least64_t = int;
typedef uint_least8_t = ffi.UnsignedChar;
typedef Dartuint_least8_t = int;
typedef uint_least16_t = ffi.UnsignedShort;
typedef Dartuint_least16_t = int;
typedef uint_least32_t = ffi.UnsignedInt;
typedef Dartuint_least32_t = int;
typedef uint_least64_t = ffi.UnsignedLongLong;
typedef Dartuint_least64_t = int;
typedef int_fast8_t = ffi.SignedChar;
typedef Dartint_fast8_t = int;
typedef int_fast16_t = ffi.Int;
typedef Dartint_fast16_t = int;
typedef int_fast32_t = ffi.Int;
typedef Dartint_fast32_t = int;
typedef int_fast64_t = ffi.LongLong;
typedef Dartint_fast64_t = int;
typedef uint_fast8_t = ffi.UnsignedChar;
typedef Dartuint_fast8_t = int;
typedef uint_fast16_t = ffi.UnsignedInt;
typedef Dartuint_fast16_t = int;
typedef uint_fast32_t = ffi.UnsignedInt;
typedef Dartuint_fast32_t = int;
typedef uint_fast64_t = ffi.UnsignedLongLong;
typedef Dartuint_fast64_t = int;
typedef intmax_t = ffi.LongLong;
typedef Dartintmax_t = int;
typedef uintmax_t = ffi.UnsignedLongLong;
typedef Dartuintmax_t = int;
typedef errno_t = ffi.Int;
typedef Darterrno_t = int;
typedef wint_t = ffi.UnsignedShort;
typedef Dartwint_t = int;
typedef wctype_t = ffi.UnsignedShort;
typedef Dartwctype_t = int;
typedef __time32_t = ffi.Long;
typedef Dart__time32_t = int;
typedef __time64_t = ffi.LongLong;
typedef Dart__time64_t = int;

final class __crt_locale_data_public extends ffi.Struct {
  external ffi.Pointer<ffi.UnsignedShort> _locale_pctype;

  @ffi.Int()
  external int _locale_mb_cur_max;

  @ffi.UnsignedInt()
  external int _locale_lc_codepage;
}

final class __crt_locale_data extends ffi.Opaque {}

final class __crt_multibyte_data extends ffi.Opaque {}

final class __crt_locale_pointers extends ffi.Struct {
  external ffi.Pointer<__crt_locale_data> locinfo;

  external ffi.Pointer<__crt_multibyte_data> mbcinfo;
}

typedef _locale_t = ffi.Pointer<__crt_locale_pointers>;

final class _Mbstatet extends ffi.Struct {
  @ffi.UnsignedLong()
  external int _Wchar;

  @ffi.UnsignedShort()
  external int _Byte;

  @ffi.UnsignedShort()
  external int _State;
}

typedef mbstate_t = _Mbstatet;
typedef time_t = __time64_t;
typedef rsize_t = ffi.Size;
typedef Dartrsize_t = int;

final class _iobuf extends ffi.Struct {
  external ffi.Pointer<ffi.Void> _Placeholder;
}

typedef FILE = _iobuf;
typedef fpos_t = ffi.LongLong;
typedef Dartfpos_t = int;
typedef ggml_abort_callback_tFunction =
    ffi.Void Function(ffi.Pointer<ffi.Char> error_message);
typedef Dartggml_abort_callback_tFunction =
    void Function(ffi.Pointer<ffi.Char> error_message);

/// Function type used in fatal error callbacks
typedef ggml_abort_callback_t =
    ffi.Pointer<ffi.NativeFunction<ggml_abort_callback_tFunction>>;

enum ggml_status {
  GGML_STATUS_ALLOC_FAILED(-2),
  GGML_STATUS_FAILED(-1),
  GGML_STATUS_SUCCESS(0),
  GGML_STATUS_ABORTED(1);

  final int value;
  const ggml_status(this.value);

  static ggml_status fromValue(int value) => switch (value) {
    -2 => GGML_STATUS_ALLOC_FAILED,
    -1 => GGML_STATUS_FAILED,
    0 => GGML_STATUS_SUCCESS,
    1 => GGML_STATUS_ABORTED,
    _ => throw ArgumentError('Unknown value for ggml_status: $value'),
  };
}

/// ieee 754-2008 half-precision float16
/// todo: make this not an integral type
typedef ggml_fp16_t = ffi.Uint16;
typedef Dartggml_fp16_t = int;

/// google brain half-precision bfloat16
final class ggml_bf16_t extends ffi.Struct {
  @ffi.Uint16()
  external int bits;
}

final class ggml_object extends ffi.Opaque {}

final class ggml_context extends ffi.Opaque {}

final class ggml_cgraph extends ffi.Opaque {}

/// NOTE: always add types at the end of the enum to keep backward compatibility
enum ggml_type {
  GGML_TYPE_F32(0),
  GGML_TYPE_F16(1),
  GGML_TYPE_Q4_0(2),
  GGML_TYPE_Q4_1(3),

  /// GGML_TYPE_Q4_2 = 4, support has been removed
  /// GGML_TYPE_Q4_3 = 5, support has been removed
  GGML_TYPE_Q5_0(6),
  GGML_TYPE_Q5_1(7),
  GGML_TYPE_Q8_0(8),
  GGML_TYPE_Q8_1(9),
  GGML_TYPE_Q2_K(10),
  GGML_TYPE_Q3_K(11),
  GGML_TYPE_Q4_K(12),
  GGML_TYPE_Q5_K(13),
  GGML_TYPE_Q6_K(14),
  GGML_TYPE_Q8_K(15),
  GGML_TYPE_IQ2_XXS(16),
  GGML_TYPE_IQ2_XS(17),
  GGML_TYPE_IQ3_XXS(18),
  GGML_TYPE_IQ1_S(19),
  GGML_TYPE_IQ4_NL(20),
  GGML_TYPE_IQ3_S(21),
  GGML_TYPE_IQ2_S(22),
  GGML_TYPE_IQ4_XS(23),
  GGML_TYPE_I8(24),
  GGML_TYPE_I16(25),
  GGML_TYPE_I32(26),
  GGML_TYPE_I64(27),
  GGML_TYPE_F64(28),
  GGML_TYPE_IQ1_M(29),
  GGML_TYPE_BF16(30),

  /// GGML_TYPE_Q4_0_4_4 = 31, support has been removed from gguf files
  /// GGML_TYPE_Q4_0_4_8 = 32,
  /// GGML_TYPE_Q4_0_8_8 = 33,
  GGML_TYPE_TQ1_0(34),
  GGML_TYPE_TQ2_0(35),

  /// MXFP4 (1 block)
  GGML_TYPE_MXFP4(39),
  GGML_TYPE_COUNT(40);

  final int value;
  const ggml_type(this.value);

  static ggml_type fromValue(int value) => switch (value) {
    0 => GGML_TYPE_F32,
    1 => GGML_TYPE_F16,
    2 => GGML_TYPE_Q4_0,
    3 => GGML_TYPE_Q4_1,
    6 => GGML_TYPE_Q5_0,
    7 => GGML_TYPE_Q5_1,
    8 => GGML_TYPE_Q8_0,
    9 => GGML_TYPE_Q8_1,
    10 => GGML_TYPE_Q2_K,
    11 => GGML_TYPE_Q3_K,
    12 => GGML_TYPE_Q4_K,
    13 => GGML_TYPE_Q5_K,
    14 => GGML_TYPE_Q6_K,
    15 => GGML_TYPE_Q8_K,
    16 => GGML_TYPE_IQ2_XXS,
    17 => GGML_TYPE_IQ2_XS,
    18 => GGML_TYPE_IQ3_XXS,
    19 => GGML_TYPE_IQ1_S,
    20 => GGML_TYPE_IQ4_NL,
    21 => GGML_TYPE_IQ3_S,
    22 => GGML_TYPE_IQ2_S,
    23 => GGML_TYPE_IQ4_XS,
    24 => GGML_TYPE_I8,
    25 => GGML_TYPE_I16,
    26 => GGML_TYPE_I32,
    27 => GGML_TYPE_I64,
    28 => GGML_TYPE_F64,
    29 => GGML_TYPE_IQ1_M,
    30 => GGML_TYPE_BF16,
    34 => GGML_TYPE_TQ1_0,
    35 => GGML_TYPE_TQ2_0,
    39 => GGML_TYPE_MXFP4,
    40 => GGML_TYPE_COUNT,
    _ => throw ArgumentError('Unknown value for ggml_type: $value'),
  };
}

/// precision
enum ggml_prec {
  /// stored as ggml_tensor.op_params, 0 by default
  GGML_PREC_DEFAULT(0),
  GGML_PREC_F32(10);

  final int value;
  const ggml_prec(this.value);

  static ggml_prec fromValue(int value) => switch (value) {
    0 => GGML_PREC_DEFAULT,
    10 => GGML_PREC_F32,
    _ => throw ArgumentError('Unknown value for ggml_prec: $value'),
  };
}

/// model file types
enum ggml_ftype {
  GGML_FTYPE_UNKNOWN(-1),
  GGML_FTYPE_ALL_F32(0),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_F16(1),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q4_0(2),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q4_1(3),

  /// tok_embeddings.weight and output.weight are F16
  GGML_FTYPE_MOSTLY_Q4_1_SOME_F16(4),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q8_0(7),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q5_0(8),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q5_1(9),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q2_K(10),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q3_K(11),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q4_K(12),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q5_K(13),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_Q6_K(14),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ2_XXS(15),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ2_XS(16),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ3_XXS(17),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ1_S(18),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ4_NL(19),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ3_S(20),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ2_S(21),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ4_XS(22),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_IQ1_M(23),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_BF16(24),

  /// except 1d tensors
  GGML_FTYPE_MOSTLY_MXFP4(25);

  final int value;
  const ggml_ftype(this.value);

  static ggml_ftype fromValue(int value) => switch (value) {
    -1 => GGML_FTYPE_UNKNOWN,
    0 => GGML_FTYPE_ALL_F32,
    1 => GGML_FTYPE_MOSTLY_F16,
    2 => GGML_FTYPE_MOSTLY_Q4_0,
    3 => GGML_FTYPE_MOSTLY_Q4_1,
    4 => GGML_FTYPE_MOSTLY_Q4_1_SOME_F16,
    7 => GGML_FTYPE_MOSTLY_Q8_0,
    8 => GGML_FTYPE_MOSTLY_Q5_0,
    9 => GGML_FTYPE_MOSTLY_Q5_1,
    10 => GGML_FTYPE_MOSTLY_Q2_K,
    11 => GGML_FTYPE_MOSTLY_Q3_K,
    12 => GGML_FTYPE_MOSTLY_Q4_K,
    13 => GGML_FTYPE_MOSTLY_Q5_K,
    14 => GGML_FTYPE_MOSTLY_Q6_K,
    15 => GGML_FTYPE_MOSTLY_IQ2_XXS,
    16 => GGML_FTYPE_MOSTLY_IQ2_XS,
    17 => GGML_FTYPE_MOSTLY_IQ3_XXS,
    18 => GGML_FTYPE_MOSTLY_IQ1_S,
    19 => GGML_FTYPE_MOSTLY_IQ4_NL,
    20 => GGML_FTYPE_MOSTLY_IQ3_S,
    21 => GGML_FTYPE_MOSTLY_IQ2_S,
    22 => GGML_FTYPE_MOSTLY_IQ4_XS,
    23 => GGML_FTYPE_MOSTLY_IQ1_M,
    24 => GGML_FTYPE_MOSTLY_BF16,
    25 => GGML_FTYPE_MOSTLY_MXFP4,
    _ => throw ArgumentError('Unknown value for ggml_ftype: $value'),
  };
}

/// available tensor operations:
enum ggml_op {
  GGML_OP_NONE(0),
  GGML_OP_DUP(1),
  GGML_OP_ADD(2),
  GGML_OP_ADD_ID(3),
  GGML_OP_ADD1(4),
  GGML_OP_ACC(5),
  GGML_OP_SUB(6),
  GGML_OP_MUL(7),
  GGML_OP_DIV(8),
  GGML_OP_SQR(9),
  GGML_OP_SQRT(10),
  GGML_OP_LOG(11),
  GGML_OP_SIN(12),
  GGML_OP_COS(13),
  GGML_OP_SUM(14),
  GGML_OP_SUM_ROWS(15),
  GGML_OP_CUMSUM(16),
  GGML_OP_MEAN(17),
  GGML_OP_ARGMAX(18),
  GGML_OP_COUNT_EQUAL(19),
  GGML_OP_REPEAT(20),
  GGML_OP_REPEAT_BACK(21),
  GGML_OP_CONCAT(22),
  GGML_OP_SILU_BACK(23),

  /// normalize
  GGML_OP_NORM(24),
  GGML_OP_RMS_NORM(25),
  GGML_OP_RMS_NORM_BACK(26),
  GGML_OP_GROUP_NORM(27),
  GGML_OP_L2_NORM(28),
  GGML_OP_MUL_MAT(29),
  GGML_OP_MUL_MAT_ID(30),
  GGML_OP_OUT_PROD(31),
  GGML_OP_SCALE(32),
  GGML_OP_SET(33),
  GGML_OP_CPY(34),
  GGML_OP_CONT(35),
  GGML_OP_RESHAPE(36),
  GGML_OP_VIEW(37),
  GGML_OP_PERMUTE(38),
  GGML_OP_TRANSPOSE(39),
  GGML_OP_GET_ROWS(40),
  GGML_OP_GET_ROWS_BACK(41),
  GGML_OP_SET_ROWS(42),
  GGML_OP_DIAG(43),
  GGML_OP_DIAG_MASK_INF(44),
  GGML_OP_DIAG_MASK_ZERO(45),
  GGML_OP_SOFT_MAX(46),
  GGML_OP_SOFT_MAX_BACK(47),
  GGML_OP_ROPE(48),
  GGML_OP_ROPE_BACK(49),
  GGML_OP_CLAMP(50),
  GGML_OP_CONV_TRANSPOSE_1D(51),
  GGML_OP_IM2COL(52),
  GGML_OP_IM2COL_BACK(53),
  GGML_OP_IM2COL_3D(54),
  GGML_OP_CONV_2D(55),
  GGML_OP_CONV_3D(56),
  GGML_OP_CONV_2D_DW(57),
  GGML_OP_CONV_TRANSPOSE_2D(58),
  GGML_OP_POOL_1D(59),
  GGML_OP_POOL_2D(60),
  GGML_OP_POOL_2D_BACK(61),
  GGML_OP_UPSCALE(62),
  GGML_OP_PAD(63),
  GGML_OP_PAD_REFLECT_1D(64),
  GGML_OP_ROLL(65),
  GGML_OP_ARANGE(66),
  GGML_OP_TIMESTEP_EMBEDDING(67),
  GGML_OP_ARGSORT(68),
  GGML_OP_TOP_K(69),
  GGML_OP_LEAKY_RELU(70),
  GGML_OP_TRI(71),
  GGML_OP_FILL(72),
  GGML_OP_FLASH_ATTN_EXT(73),
  GGML_OP_FLASH_ATTN_BACK(74),
  GGML_OP_SSM_CONV(75),
  GGML_OP_SSM_SCAN(76),
  GGML_OP_WIN_PART(77),
  GGML_OP_WIN_UNPART(78),
  GGML_OP_GET_REL_POS(79),
  GGML_OP_ADD_REL_POS(80),
  GGML_OP_RWKV_WKV6(81),
  GGML_OP_GATED_LINEAR_ATTN(82),
  GGML_OP_RWKV_WKV7(83),
  GGML_OP_SOLVE_TRI(84),
  GGML_OP_UNARY(85),
  GGML_OP_MAP_CUSTOM1(86),
  GGML_OP_MAP_CUSTOM2(87),
  GGML_OP_MAP_CUSTOM3(88),
  GGML_OP_CUSTOM(89),
  GGML_OP_CROSS_ENTROPY_LOSS(90),
  GGML_OP_CROSS_ENTROPY_LOSS_BACK(91),
  GGML_OP_OPT_STEP_ADAMW(92),
  GGML_OP_OPT_STEP_SGD(93),
  GGML_OP_GLU(94),
  GGML_OP_COUNT(95);

  final int value;
  const ggml_op(this.value);

  static ggml_op fromValue(int value) => switch (value) {
    0 => GGML_OP_NONE,
    1 => GGML_OP_DUP,
    2 => GGML_OP_ADD,
    3 => GGML_OP_ADD_ID,
    4 => GGML_OP_ADD1,
    5 => GGML_OP_ACC,
    6 => GGML_OP_SUB,
    7 => GGML_OP_MUL,
    8 => GGML_OP_DIV,
    9 => GGML_OP_SQR,
    10 => GGML_OP_SQRT,
    11 => GGML_OP_LOG,
    12 => GGML_OP_SIN,
    13 => GGML_OP_COS,
    14 => GGML_OP_SUM,
    15 => GGML_OP_SUM_ROWS,
    16 => GGML_OP_CUMSUM,
    17 => GGML_OP_MEAN,
    18 => GGML_OP_ARGMAX,
    19 => GGML_OP_COUNT_EQUAL,
    20 => GGML_OP_REPEAT,
    21 => GGML_OP_REPEAT_BACK,
    22 => GGML_OP_CONCAT,
    23 => GGML_OP_SILU_BACK,
    24 => GGML_OP_NORM,
    25 => GGML_OP_RMS_NORM,
    26 => GGML_OP_RMS_NORM_BACK,
    27 => GGML_OP_GROUP_NORM,
    28 => GGML_OP_L2_NORM,
    29 => GGML_OP_MUL_MAT,
    30 => GGML_OP_MUL_MAT_ID,
    31 => GGML_OP_OUT_PROD,
    32 => GGML_OP_SCALE,
    33 => GGML_OP_SET,
    34 => GGML_OP_CPY,
    35 => GGML_OP_CONT,
    36 => GGML_OP_RESHAPE,
    37 => GGML_OP_VIEW,
    38 => GGML_OP_PERMUTE,
    39 => GGML_OP_TRANSPOSE,
    40 => GGML_OP_GET_ROWS,
    41 => GGML_OP_GET_ROWS_BACK,
    42 => GGML_OP_SET_ROWS,
    43 => GGML_OP_DIAG,
    44 => GGML_OP_DIAG_MASK_INF,
    45 => GGML_OP_DIAG_MASK_ZERO,
    46 => GGML_OP_SOFT_MAX,
    47 => GGML_OP_SOFT_MAX_BACK,
    48 => GGML_OP_ROPE,
    49 => GGML_OP_ROPE_BACK,
    50 => GGML_OP_CLAMP,
    51 => GGML_OP_CONV_TRANSPOSE_1D,
    52 => GGML_OP_IM2COL,
    53 => GGML_OP_IM2COL_BACK,
    54 => GGML_OP_IM2COL_3D,
    55 => GGML_OP_CONV_2D,
    56 => GGML_OP_CONV_3D,
    57 => GGML_OP_CONV_2D_DW,
    58 => GGML_OP_CONV_TRANSPOSE_2D,
    59 => GGML_OP_POOL_1D,
    60 => GGML_OP_POOL_2D,
    61 => GGML_OP_POOL_2D_BACK,
    62 => GGML_OP_UPSCALE,
    63 => GGML_OP_PAD,
    64 => GGML_OP_PAD_REFLECT_1D,
    65 => GGML_OP_ROLL,
    66 => GGML_OP_ARANGE,
    67 => GGML_OP_TIMESTEP_EMBEDDING,
    68 => GGML_OP_ARGSORT,
    69 => GGML_OP_TOP_K,
    70 => GGML_OP_LEAKY_RELU,
    71 => GGML_OP_TRI,
    72 => GGML_OP_FILL,
    73 => GGML_OP_FLASH_ATTN_EXT,
    74 => GGML_OP_FLASH_ATTN_BACK,
    75 => GGML_OP_SSM_CONV,
    76 => GGML_OP_SSM_SCAN,
    77 => GGML_OP_WIN_PART,
    78 => GGML_OP_WIN_UNPART,
    79 => GGML_OP_GET_REL_POS,
    80 => GGML_OP_ADD_REL_POS,
    81 => GGML_OP_RWKV_WKV6,
    82 => GGML_OP_GATED_LINEAR_ATTN,
    83 => GGML_OP_RWKV_WKV7,
    84 => GGML_OP_SOLVE_TRI,
    85 => GGML_OP_UNARY,
    86 => GGML_OP_MAP_CUSTOM1,
    87 => GGML_OP_MAP_CUSTOM2,
    88 => GGML_OP_MAP_CUSTOM3,
    89 => GGML_OP_CUSTOM,
    90 => GGML_OP_CROSS_ENTROPY_LOSS,
    91 => GGML_OP_CROSS_ENTROPY_LOSS_BACK,
    92 => GGML_OP_OPT_STEP_ADAMW,
    93 => GGML_OP_OPT_STEP_SGD,
    94 => GGML_OP_GLU,
    95 => GGML_OP_COUNT,
    _ => throw ArgumentError('Unknown value for ggml_op: $value'),
  };
}

enum ggml_unary_op {
  GGML_UNARY_OP_ABS(0),
  GGML_UNARY_OP_SGN(1),
  GGML_UNARY_OP_NEG(2),
  GGML_UNARY_OP_STEP(3),
  GGML_UNARY_OP_TANH(4),
  GGML_UNARY_OP_ELU(5),
  GGML_UNARY_OP_RELU(6),
  GGML_UNARY_OP_SIGMOID(7),
  GGML_UNARY_OP_GELU(8),
  GGML_UNARY_OP_GELU_QUICK(9),
  GGML_UNARY_OP_SILU(10),
  GGML_UNARY_OP_HARDSWISH(11),
  GGML_UNARY_OP_HARDSIGMOID(12),
  GGML_UNARY_OP_EXP(13),
  GGML_UNARY_OP_EXPM1(14),
  GGML_UNARY_OP_SOFTPLUS(15),
  GGML_UNARY_OP_GELU_ERF(16),
  GGML_UNARY_OP_XIELU(17),
  GGML_UNARY_OP_FLOOR(18),
  GGML_UNARY_OP_CEIL(19),
  GGML_UNARY_OP_ROUND(20),
  GGML_UNARY_OP_TRUNC(21),
  GGML_UNARY_OP_COUNT(22);

  final int value;
  const ggml_unary_op(this.value);

  static ggml_unary_op fromValue(int value) => switch (value) {
    0 => GGML_UNARY_OP_ABS,
    1 => GGML_UNARY_OP_SGN,
    2 => GGML_UNARY_OP_NEG,
    3 => GGML_UNARY_OP_STEP,
    4 => GGML_UNARY_OP_TANH,
    5 => GGML_UNARY_OP_ELU,
    6 => GGML_UNARY_OP_RELU,
    7 => GGML_UNARY_OP_SIGMOID,
    8 => GGML_UNARY_OP_GELU,
    9 => GGML_UNARY_OP_GELU_QUICK,
    10 => GGML_UNARY_OP_SILU,
    11 => GGML_UNARY_OP_HARDSWISH,
    12 => GGML_UNARY_OP_HARDSIGMOID,
    13 => GGML_UNARY_OP_EXP,
    14 => GGML_UNARY_OP_EXPM1,
    15 => GGML_UNARY_OP_SOFTPLUS,
    16 => GGML_UNARY_OP_GELU_ERF,
    17 => GGML_UNARY_OP_XIELU,
    18 => GGML_UNARY_OP_FLOOR,
    19 => GGML_UNARY_OP_CEIL,
    20 => GGML_UNARY_OP_ROUND,
    21 => GGML_UNARY_OP_TRUNC,
    22 => GGML_UNARY_OP_COUNT,
    _ => throw ArgumentError('Unknown value for ggml_unary_op: $value'),
  };
}

enum ggml_glu_op {
  GGML_GLU_OP_REGLU(0),
  GGML_GLU_OP_GEGLU(1),
  GGML_GLU_OP_SWIGLU(2),
  GGML_GLU_OP_SWIGLU_OAI(3),
  GGML_GLU_OP_GEGLU_ERF(4),
  GGML_GLU_OP_GEGLU_QUICK(5),
  GGML_GLU_OP_COUNT(6);

  final int value;
  const ggml_glu_op(this.value);

  static ggml_glu_op fromValue(int value) => switch (value) {
    0 => GGML_GLU_OP_REGLU,
    1 => GGML_GLU_OP_GEGLU,
    2 => GGML_GLU_OP_SWIGLU,
    3 => GGML_GLU_OP_SWIGLU_OAI,
    4 => GGML_GLU_OP_GEGLU_ERF,
    5 => GGML_GLU_OP_GEGLU_QUICK,
    6 => GGML_GLU_OP_COUNT,
    _ => throw ArgumentError('Unknown value for ggml_glu_op: $value'),
  };
}

enum ggml_object_type {
  GGML_OBJECT_TYPE_TENSOR(0),
  GGML_OBJECT_TYPE_GRAPH(1),
  GGML_OBJECT_TYPE_WORK_BUFFER(2);

  final int value;
  const ggml_object_type(this.value);

  static ggml_object_type fromValue(int value) => switch (value) {
    0 => GGML_OBJECT_TYPE_TENSOR,
    1 => GGML_OBJECT_TYPE_GRAPH,
    2 => GGML_OBJECT_TYPE_WORK_BUFFER,
    _ => throw ArgumentError('Unknown value for ggml_object_type: $value'),
  };
}

enum ggml_log_level {
  GGML_LOG_LEVEL_NONE(0),
  GGML_LOG_LEVEL_DEBUG(1),
  GGML_LOG_LEVEL_INFO(2),
  GGML_LOG_LEVEL_WARN(3),
  GGML_LOG_LEVEL_ERROR(4),

  /// continue previous log
  GGML_LOG_LEVEL_CONT(5);

  final int value;
  const ggml_log_level(this.value);

  static ggml_log_level fromValue(int value) => switch (value) {
    0 => GGML_LOG_LEVEL_NONE,
    1 => GGML_LOG_LEVEL_DEBUG,
    2 => GGML_LOG_LEVEL_INFO,
    3 => GGML_LOG_LEVEL_WARN,
    4 => GGML_LOG_LEVEL_ERROR,
    5 => GGML_LOG_LEVEL_CONT,
    _ => throw ArgumentError('Unknown value for ggml_log_level: $value'),
  };
}

/// this tensor...
enum ggml_tensor_flag {
  /// ...is an input for the GGML compute graph
  GGML_TENSOR_FLAG_INPUT(1),

  /// ...is an output for the GGML compute graph
  GGML_TENSOR_FLAG_OUTPUT(2),

  /// ...contains trainable parameters
  GGML_TENSOR_FLAG_PARAM(4),

  /// ...defines loss for numerical optimization (multiple loss tensors add up)
  GGML_TENSOR_FLAG_LOSS(8);

  final int value;
  const ggml_tensor_flag(this.value);

  static ggml_tensor_flag fromValue(int value) => switch (value) {
    1 => GGML_TENSOR_FLAG_INPUT,
    2 => GGML_TENSOR_FLAG_OUTPUT,
    4 => GGML_TENSOR_FLAG_PARAM,
    8 => GGML_TENSOR_FLAG_LOSS,
    _ => throw ArgumentError('Unknown value for ggml_tensor_flag: $value'),
  };
}

enum ggml_tri_type {
  GGML_TRI_TYPE_UPPER_DIAG(0),
  GGML_TRI_TYPE_UPPER(1),
  GGML_TRI_TYPE_LOWER_DIAG(2),
  GGML_TRI_TYPE_LOWER(3);

  final int value;
  const ggml_tri_type(this.value);

  static ggml_tri_type fromValue(int value) => switch (value) {
    0 => GGML_TRI_TYPE_UPPER_DIAG,
    1 => GGML_TRI_TYPE_UPPER,
    2 => GGML_TRI_TYPE_LOWER_DIAG,
    3 => GGML_TRI_TYPE_LOWER,
    _ => throw ArgumentError('Unknown value for ggml_tri_type: $value'),
  };
}

final class ggml_init_params extends ffi.Struct {
  /// bytes
  @ffi.Size()
  external int mem_size;

  /// if NULL, memory will be allocated internally
  external ffi.Pointer<ffi.Void> mem_buffer;

  /// don't allocate memory for the tensor data
  @ffi.Bool()
  external bool no_alloc;
}

final class ggml_backend_buffer extends ffi.Opaque {}

/// n-dimensional tensor
final class ggml_tensor extends ffi.Struct {
  @ffi.UnsignedInt()
  external int typeAsInt;

  ggml_type get type => ggml_type.fromValue(typeAsInt);

  external ffi.Pointer<ggml_backend_buffer> buffer;

  /// number of elements
  @ffi.Array.multi([4])
  external ffi.Array<ffi.Int64> ne;

  /// stride in bytes:
  /// nb[0] = ggml_type_size(type)
  /// nb[1] = nb[0]   * (ne[0] / ggml_blck_size(type)) + padding
  /// nb[i] = nb[i-1] * ne[i-1]
  @ffi.Array.multi([4])
  external ffi.Array<ffi.Size> nb;

  /// compute data
  @ffi.UnsignedInt()
  external int opAsInt;

  ggml_op get op => ggml_op.fromValue(opAsInt);

  /// op params - allocated as int32_t for alignment
  @ffi.Array.multi([16])
  external ffi.Array<ffi.Int32> op_params;

  @ffi.Int32()
  external int flags;

  @ffi.Array.multi([10])
  external ffi.Array<ffi.Pointer<ggml_tensor>> src;

  /// source tensor and offset for views
  external ffi.Pointer<ggml_tensor> view_src;

  @ffi.Size()
  external int view_offs;

  external ffi.Pointer<ffi.Void> data;

  @ffi.Array.multi([64])
  external ffi.Array<ffi.Char> name;

  /// extra things e.g. for ggml-cuda.cu
  external ffi.Pointer<ffi.Void> extra;

  @ffi.Array.multi([8])
  external ffi.Array<ffi.Char> padding;
}

typedef ggml_abort_callbackFunction =
    ffi.Bool Function(ffi.Pointer<ffi.Void> data);
typedef Dartggml_abort_callbackFunction =
    bool Function(ffi.Pointer<ffi.Void> data);

/// Abort callback
/// If not NULL, called before ggml computation
/// If it returns true, the computation is aborted
typedef ggml_abort_callback =
    ffi.Pointer<ffi.NativeFunction<ggml_abort_callbackFunction>>;
typedef ggml_guid_t = ffi.Pointer<ffi.Pointer<ffi.Uint8>>;

enum ggml_op_pool {
  GGML_OP_POOL_MAX(0),
  GGML_OP_POOL_AVG(1),
  GGML_OP_POOL_COUNT(2);

  final int value;
  const ggml_op_pool(this.value);

  static ggml_op_pool fromValue(int value) => switch (value) {
    0 => GGML_OP_POOL_MAX,
    1 => GGML_OP_POOL_AVG,
    2 => GGML_OP_POOL_COUNT,
    _ => throw ArgumentError('Unknown value for ggml_op_pool: $value'),
  };
}

enum ggml_scale_mode {
  GGML_SCALE_MODE_NEAREST(0),
  GGML_SCALE_MODE_BILINEAR(1),
  GGML_SCALE_MODE_BICUBIC(2),
  GGML_SCALE_MODE_COUNT(3);

  final int value;
  const ggml_scale_mode(this.value);

  static ggml_scale_mode fromValue(int value) => switch (value) {
    0 => GGML_SCALE_MODE_NEAREST,
    1 => GGML_SCALE_MODE_BILINEAR,
    2 => GGML_SCALE_MODE_BICUBIC,
    3 => GGML_SCALE_MODE_COUNT,
    _ => throw ArgumentError('Unknown value for ggml_scale_mode: $value'),
  };
}

enum ggml_scale_flag {
  GGML_SCALE_FLAG_ALIGN_CORNERS(256),
  GGML_SCALE_FLAG_ANTIALIAS(512);

  final int value;
  const ggml_scale_flag(this.value);

  static ggml_scale_flag fromValue(int value) => switch (value) {
    256 => GGML_SCALE_FLAG_ALIGN_CORNERS,
    512 => GGML_SCALE_FLAG_ANTIALIAS,
    _ => throw ArgumentError('Unknown value for ggml_scale_flag: $value'),
  };
}

/// sort rows
enum ggml_sort_order {
  GGML_SORT_ORDER_ASC(0),
  GGML_SORT_ORDER_DESC(1);

  final int value;
  const ggml_sort_order(this.value);

  static ggml_sort_order fromValue(int value) => switch (value) {
    0 => GGML_SORT_ORDER_ASC,
    1 => GGML_SORT_ORDER_DESC,
    _ => throw ArgumentError('Unknown value for ggml_sort_order: $value'),
  };
}

typedef ggml_custom1_op_tFunction =
    ffi.Void Function(
      ffi.Pointer<ggml_tensor> dst,
      ffi.Pointer<ggml_tensor> a,
      ffi.Int ith,
      ffi.Int nth,
      ffi.Pointer<ffi.Void> userdata,
    );
typedef Dartggml_custom1_op_tFunction =
    void Function(
      ffi.Pointer<ggml_tensor> dst,
      ffi.Pointer<ggml_tensor> a,
      int ith,
      int nth,
      ffi.Pointer<ffi.Void> userdata,
    );

/// custom operators
typedef ggml_custom1_op_t =
    ffi.Pointer<ffi.NativeFunction<ggml_custom1_op_tFunction>>;
typedef ggml_custom2_op_tFunction =
    ffi.Void Function(
      ffi.Pointer<ggml_tensor> dst,
      ffi.Pointer<ggml_tensor> a,
      ffi.Pointer<ggml_tensor> b,
      ffi.Int ith,
      ffi.Int nth,
      ffi.Pointer<ffi.Void> userdata,
    );
typedef Dartggml_custom2_op_tFunction =
    void Function(
      ffi.Pointer<ggml_tensor> dst,
      ffi.Pointer<ggml_tensor> a,
      ffi.Pointer<ggml_tensor> b,
      int ith,
      int nth,
      ffi.Pointer<ffi.Void> userdata,
    );
typedef ggml_custom2_op_t =
    ffi.Pointer<ffi.NativeFunction<ggml_custom2_op_tFunction>>;
typedef ggml_custom3_op_tFunction =
    ffi.Void Function(
      ffi.Pointer<ggml_tensor> dst,
      ffi.Pointer<ggml_tensor> a,
      ffi.Pointer<ggml_tensor> b,
      ffi.Pointer<ggml_tensor> c,
      ffi.Int ith,
      ffi.Int nth,
      ffi.Pointer<ffi.Void> userdata,
    );
typedef Dartggml_custom3_op_tFunction =
    void Function(
      ffi.Pointer<ggml_tensor> dst,
      ffi.Pointer<ggml_tensor> a,
      ffi.Pointer<ggml_tensor> b,
      ffi.Pointer<ggml_tensor> c,
      int ith,
      int nth,
      ffi.Pointer<ffi.Void> userdata,
    );
typedef ggml_custom3_op_t =
    ffi.Pointer<ffi.NativeFunction<ggml_custom3_op_tFunction>>;
typedef ggml_custom_op_tFunction =
    ffi.Void Function(
      ffi.Pointer<ggml_tensor> dst,
      ffi.Int ith,
      ffi.Int nth,
      ffi.Pointer<ffi.Void> userdata,
    );
typedef Dartggml_custom_op_tFunction =
    void Function(
      ffi.Pointer<ggml_tensor> dst,
      int ith,
      int nth,
      ffi.Pointer<ffi.Void> userdata,
    );
typedef ggml_custom_op_t =
    ffi.Pointer<ffi.NativeFunction<ggml_custom_op_tFunction>>;
typedef ggml_log_callbackFunction =
    ffi.Void Function(
      ffi.UnsignedInt level,
      ffi.Pointer<ffi.Char> text,
      ffi.Pointer<ffi.Void> user_data,
    );
typedef Dartggml_log_callbackFunction =
    void Function(
      ggml_log_level level,
      ffi.Pointer<ffi.Char> text,
      ffi.Pointer<ffi.Void> user_data,
    );

/// TODO these functions were sandwiched in the old optimization interface, is there a better place for them?
typedef ggml_log_callback =
    ffi.Pointer<ffi.NativeFunction<ggml_log_callbackFunction>>;
typedef ggml_to_float_tFunction =
    ffi.Void Function(
      ffi.Pointer<ffi.Void> x,
      ffi.Pointer<ffi.Float> y,
      ffi.Int64 k,
    );
typedef Dartggml_to_float_tFunction =
    void Function(ffi.Pointer<ffi.Void> x, ffi.Pointer<ffi.Float> y, int k);
typedef ggml_to_float_t =
    ffi.Pointer<ffi.NativeFunction<ggml_to_float_tFunction>>;
typedef ggml_from_float_tFunction =
    ffi.Void Function(
      ffi.Pointer<ffi.Float> x,
      ffi.Pointer<ffi.Void> y,
      ffi.Int64 k,
    );
typedef Dartggml_from_float_tFunction =
    void Function(ffi.Pointer<ffi.Float> x, ffi.Pointer<ffi.Void> y, int k);
typedef ggml_from_float_t =
    ffi.Pointer<ffi.NativeFunction<ggml_from_float_tFunction>>;

final class ggml_type_traits extends ffi.Struct {
  external ffi.Pointer<ffi.Char> type_name;

  @ffi.Int64()
  external int blck_size;

  /// interleave elements in blocks
  @ffi.Int64()
  external int blck_size_interleave;

  @ffi.Size()
  external int type_size;

  @ffi.Bool()
  external bool is_quantized;

  external ggml_to_float_t to_float;

  external ggml_from_float_t from_float_ref;
}

/// scheduling priorities
enum ggml_sched_priority {
  GGML_SCHED_PRIO_LOW(-1),
  GGML_SCHED_PRIO_NORMAL(0),
  GGML_SCHED_PRIO_MEDIUM(1),
  GGML_SCHED_PRIO_HIGH(2),
  GGML_SCHED_PRIO_REALTIME(3);

  final int value;
  const ggml_sched_priority(this.value);

  static ggml_sched_priority fromValue(int value) => switch (value) {
    -1 => GGML_SCHED_PRIO_LOW,
    0 => GGML_SCHED_PRIO_NORMAL,
    1 => GGML_SCHED_PRIO_MEDIUM,
    2 => GGML_SCHED_PRIO_HIGH,
    3 => GGML_SCHED_PRIO_REALTIME,
    _ => throw ArgumentError('Unknown value for ggml_sched_priority: $value'),
  };
}

/// threadpool params
/// Use ggml_threadpool_params_default() or ggml_threadpool_params_init() to populate the defaults
final class ggml_threadpool_params extends ffi.Struct {
  /// mask of cpu cores (all-zeros means use default affinity settings)
  @ffi.Array.multi([512])
  external ffi.Array<ffi.Bool> cpumask;

  /// number of threads
  @ffi.Int()
  external int n_threads;

  /// thread priority
  @ffi.Int()
  external int prioAsInt;

  ggml_sched_priority get prio => ggml_sched_priority.fromValue(prioAsInt);

  /// polling level (0 - no polling, 100 - aggressive polling)
  @ffi.Uint32()
  external int poll;

  /// strict cpu placement
  @ffi.Bool()
  external bool strict_cpu;

  /// start in paused state
  @ffi.Bool()
  external bool paused;
}

final class ggml_threadpool extends ffi.Opaque {}

typedef ggml_threadpool_t = ffi.Pointer<ggml_threadpool>;

final class ggml_backend_buffer_type extends ffi.Opaque {}

typedef ggml_backend_buffer_type_t = ffi.Pointer<ggml_backend_buffer_type>;
typedef ggml_backend_buffer_t = ffi.Pointer<ggml_backend_buffer>;

final class ggml_backend extends ffi.Opaque {}

typedef ggml_backend_t = ffi.Pointer<ggml_backend>;

/// Tensor allocator
final class ggml_tallocr extends ffi.Struct {
  external ggml_backend_buffer_t buffer;

  external ffi.Pointer<ffi.Void> base;

  @ffi.Size()
  external int alignment;

  @ffi.Size()
  external int offset;
}

final class ggml_gallocr extends ffi.Opaque {}

/// special tensor flags for use with the graph allocator:
/// ggml_set_input(): all input tensors are allocated at the beginning of the graph in non-overlapping addresses
/// ggml_set_output(): output tensors are never freed and never overwritten
typedef ggml_gallocr_t = ffi.Pointer<ggml_gallocr>;

final class ggml_backend_event extends ffi.Opaque {}

typedef ggml_backend_event_t = ffi.Pointer<ggml_backend_event>;
typedef ggml_backend_graph_plan_t = ffi.Pointer<ffi.Void>;

final class ggml_backend_reg extends ffi.Opaque {}

typedef ggml_backend_reg_t = ffi.Pointer<ggml_backend_reg>;

final class ggml_backend_device extends ffi.Opaque {}

typedef ggml_backend_dev_t = ffi.Pointer<ggml_backend_device>;

/// Backend buffer
enum ggml_backend_buffer_usage {
  GGML_BACKEND_BUFFER_USAGE_ANY(0),
  GGML_BACKEND_BUFFER_USAGE_WEIGHTS(1),
  GGML_BACKEND_BUFFER_USAGE_COMPUTE(2);

  final int value;
  const ggml_backend_buffer_usage(this.value);

  static ggml_backend_buffer_usage fromValue(int value) => switch (value) {
    0 => GGML_BACKEND_BUFFER_USAGE_ANY,
    1 => GGML_BACKEND_BUFFER_USAGE_WEIGHTS,
    2 => GGML_BACKEND_BUFFER_USAGE_COMPUTE,
    _ =>
      throw ArgumentError(
        'Unknown value for ggml_backend_buffer_usage: $value',
      ),
  };
}

/// Backend device
enum ggml_backend_dev_type {
  /// CPU device using system memory
  GGML_BACKEND_DEVICE_TYPE_CPU(0),

  /// GPU device using dedicated memory
  GGML_BACKEND_DEVICE_TYPE_GPU(1),

  /// integrated GPU device using host memory
  GGML_BACKEND_DEVICE_TYPE_IGPU(2),

  /// accelerator devices intended to be used together with the CPU backend (e.g. BLAS or AMX)
  GGML_BACKEND_DEVICE_TYPE_ACCEL(3);

  final int value;
  const ggml_backend_dev_type(this.value);

  static ggml_backend_dev_type fromValue(int value) => switch (value) {
    0 => GGML_BACKEND_DEVICE_TYPE_CPU,
    1 => GGML_BACKEND_DEVICE_TYPE_GPU,
    2 => GGML_BACKEND_DEVICE_TYPE_IGPU,
    3 => GGML_BACKEND_DEVICE_TYPE_ACCEL,
    _ => throw ArgumentError('Unknown value for ggml_backend_dev_type: $value'),
  };
}

/// functionality supported by the device
final class ggml_backend_dev_caps extends ffi.Struct {
  /// asynchronous operations
  @ffi.Bool()
  external bool async;

  /// pinned host buffer
  @ffi.Bool()
  external bool host_buffer;

  /// creating buffers from host ptr
  @ffi.Bool()
  external bool buffer_from_host_ptr;

  /// event synchronization
  @ffi.Bool()
  external bool events;
}

/// all the device properties
final class ggml_backend_dev_props extends ffi.Struct {
  /// device name
  external ffi.Pointer<ffi.Char> name;

  /// device description
  external ffi.Pointer<ffi.Char> description;

  /// device free memory in bytes
  @ffi.Size()
  external int memory_free;

  /// device total memory in bytes
  @ffi.Size()
  external int memory_total;

  /// device type
  @ffi.UnsignedInt()
  external int typeAsInt;

  ggml_backend_dev_type get type => ggml_backend_dev_type.fromValue(typeAsInt);

  /// device id
  /// for PCI devices, this should be the PCI bus id formatted as "domain:bus:device.function" (e.g. "0000:01:00.0")
  /// if the id is unknown, this should be NULL
  external ffi.Pointer<ffi.Char> device_id;

  /// device capabilities
  external ggml_backend_dev_caps caps;
}

typedef ggml_backend_split_buffer_type_tFunction =
    ggml_backend_buffer_type_t Function(
      ffi.Int main_device,
      ffi.Pointer<ffi.Float> tensor_split,
    );
typedef Dartggml_backend_split_buffer_type_tFunction =
    ggml_backend_buffer_type_t Function(
      int main_device,
      ffi.Pointer<ffi.Float> tensor_split,
    );

/// Split buffer type for tensor parallelism
typedef ggml_backend_split_buffer_type_t =
    ffi.Pointer<ffi.NativeFunction<ggml_backend_split_buffer_type_tFunction>>;
typedef ggml_backend_set_n_threads_tFunction =
    ffi.Void Function(ggml_backend_t backend, ffi.Int n_threads);
typedef Dartggml_backend_set_n_threads_tFunction =
    void Function(ggml_backend_t backend, int n_threads);

/// Set the number of threads for the backend
typedef ggml_backend_set_n_threads_t =
    ffi.Pointer<ffi.NativeFunction<ggml_backend_set_n_threads_tFunction>>;
typedef ggml_backend_dev_get_extra_bufts_tFunction =
    ffi.Pointer<ggml_backend_buffer_type_t> Function(ggml_backend_dev_t device);

/// Get additional buffer types provided by the device (returns a NULL-terminated array)
typedef ggml_backend_dev_get_extra_bufts_t =
    ffi.Pointer<ffi.NativeFunction<ggml_backend_dev_get_extra_bufts_tFunction>>;
typedef ggml_backend_set_abort_callback_tFunction =
    ffi.Void Function(
      ggml_backend_t backend,
      ggml_abort_callback abort_callback,
      ffi.Pointer<ffi.Void> abort_callback_data,
    );
typedef Dartggml_backend_set_abort_callback_tFunction =
    void Function(
      ggml_backend_t backend,
      ggml_abort_callback abort_callback,
      ffi.Pointer<ffi.Void> abort_callback_data,
    );

/// Set the abort callback for the backend
typedef ggml_backend_set_abort_callback_t =
    ffi.Pointer<ffi.NativeFunction<ggml_backend_set_abort_callback_tFunction>>;

/// Get a list of feature flags supported by the backend (returns a NULL-terminated array)
final class ggml_backend_feature extends ffi.Struct {
  external ffi.Pointer<ffi.Char> name;

  external ffi.Pointer<ffi.Char> value;
}

typedef ggml_backend_get_features_tFunction =
    ffi.Pointer<ggml_backend_feature> Function(ggml_backend_reg_t reg);
typedef ggml_backend_get_features_t =
    ffi.Pointer<ffi.NativeFunction<ggml_backend_get_features_tFunction>>;

final class ggml_backend_sched extends ffi.Opaque {}

/// The backend scheduler allows for multiple backend devices to be used together
/// Handles compute buffer allocation, assignment of tensors to backends, and copying of tensors between backends
/// The backends are selected based on:
/// - the backend that supports the operation
/// - the location of the pre-allocated tensors (e.g. the weights)
///     /*
///       Example usage:
///
/// operations that use tensors allocated in a buffer with USAGE_WEIGHTS will be assigned
/// preferrably to run on the same backend as the buffer
///         ggml_backend_buffer_set_usage(buf_weights, GGML_BACKEND_BUFFER_USAGE_WEIGHTS);
///
///         sched = ggml_backend_sched_new({backend_gpu, backend_gpu2, backend_cpu}, NULL, num_backends, GGML_DEFAULT_GRAPH_SIZE, false, true);
///
/// initialize buffers from a max size graph (optional)
///         reserve_graph = build_graph(sched, max_batch_size);
///
/// manually assign nodes to a backend (optional, should not be needed in most cases)
///         struct ggml_tensor * node = ggml_mul_mat(ctx, ...);
///         ggml_backend_sched_set_tensor_backend(sched, node, backend_gpu);
///
///         ggml_backend_sched_reserve(sched, reserve_graph);
///
/// compute
///         graph = build_graph(sched); // the graph and its tensors are single-use in terms of allocation, multi-use in terms of computation
///         for (int i = 0; i < 10; ++i) {
///             ggml_backend_sched_graph_compute(sched, graph); // on the first iteration the graph is allocated automatically
///         }
///
/// if there are graph inputs:
///         graph = build_graph(sched); // get a new graph that is not allocated (the metadata for the old graph is freed once ggml_free is called)
///         ggml_backend_sched_reset(sched); // clear the allocation of the previous graph
///         ggml_backend_sched_alloc_graph(sched, graph); // explicitly allocate the new graph but do not execute it
///         ggml_backend_tensor_set(input_tensor, ...); // copy data to the newly allocated graph tensors
///         ggml_backend_sched_graph_compute(sched, graph); // execute the graph
///
/// as an alternative to the above it is also possible to assign the inputs to a dedicated context and
/// allocate them statically via ggml_backend_alloc_ctx_tensors
///     }
///     */
typedef ggml_backend_sched_t = ffi.Pointer<ggml_backend_sched>;
typedef ggml_backend_sched_eval_callbackFunction =
    ffi.Bool Function(
      ffi.Pointer<ggml_tensor> t,
      ffi.Bool ask,
      ffi.Pointer<ffi.Void> user_data,
    );
typedef Dartggml_backend_sched_eval_callbackFunction =
    bool Function(
      ffi.Pointer<ggml_tensor> t,
      bool ask,
      ffi.Pointer<ffi.Void> user_data,
    );

/// Evaluation callback for each node in the graph (set with ggml_backend_sched_set_eval_callback)
/// when ask == true, the scheduler wants to know if the user wants to observe this node
/// this allows the scheduler to batch nodes together in order to evaluate them in a single call
///
/// when ask == false, the scheduler is passing the node tensor to the user for observation
/// if the user returns false, the scheduler will cancel the graph compute
typedef ggml_backend_sched_eval_callback =
    ffi.Pointer<ffi.NativeFunction<ggml_backend_sched_eval_callbackFunction>>;

/// Utils
final class ggml_backend_graph_copy$1 extends ffi.Struct {
  external ggml_backend_buffer_t buffer;

  external ffi.Pointer<ggml_context> ctx_allocated;

  external ffi.Pointer<ggml_context> ctx_unallocated;

  external ffi.Pointer<ggml_cgraph> graph;
}

typedef ggml_backend_eval_callbackFunction =
    ffi.Bool Function(
      ffi.Int node_index,
      ffi.Pointer<ggml_tensor> t1,
      ffi.Pointer<ggml_tensor> t2,
      ffi.Pointer<ffi.Void> user_data,
    );
typedef Dartggml_backend_eval_callbackFunction =
    bool Function(
      int node_index,
      ffi.Pointer<ggml_tensor> t1,
      ffi.Pointer<ggml_tensor> t2,
      ffi.Pointer<ffi.Void> user_data,
    );
typedef ggml_backend_eval_callback =
    ffi.Pointer<ffi.NativeFunction<ggml_backend_eval_callbackFunction>>;

/// the compute plan that needs to be prepared for ggml_graph_compute()
/// since https://github.com/ggml-org/ggml/issues/287
final class ggml_cplan extends ffi.Struct {
  /// size of work buffer, calculated by `ggml_graph_plan()`
  @ffi.Size()
  external int work_size;

  /// work buffer, to be allocated by caller before calling to `ggml_graph_compute()`
  external ffi.Pointer<ffi.Uint8> work_data;

  @ffi.Int()
  external int n_threads;

  external ffi.Pointer<ggml_threadpool> threadpool;

  /// abort ggml_graph_compute when true
  external ggml_abort_callback abort_callback;

  external ffi.Pointer<ffi.Void> abort_callback_data;
}

/// numa strategies
enum ggml_numa_strategy {
  GGML_NUMA_STRATEGY_DISABLED(0),
  GGML_NUMA_STRATEGY_DISTRIBUTE(1),
  GGML_NUMA_STRATEGY_ISOLATE(2),
  GGML_NUMA_STRATEGY_NUMACTL(3),
  GGML_NUMA_STRATEGY_MIRROR(4),
  GGML_NUMA_STRATEGY_COUNT(5);

  final int value;
  const ggml_numa_strategy(this.value);

  static ggml_numa_strategy fromValue(int value) => switch (value) {
    0 => GGML_NUMA_STRATEGY_DISABLED,
    1 => GGML_NUMA_STRATEGY_DISTRIBUTE,
    2 => GGML_NUMA_STRATEGY_ISOLATE,
    3 => GGML_NUMA_STRATEGY_NUMACTL,
    4 => GGML_NUMA_STRATEGY_MIRROR,
    5 => GGML_NUMA_STRATEGY_COUNT,
    _ => throw ArgumentError('Unknown value for ggml_numa_strategy: $value'),
  };
}

typedef ggml_vec_dot_tFunction =
    ffi.Void Function(
      ffi.Int n,
      ffi.Pointer<ffi.Float> s,
      ffi.Size bs,
      ffi.Pointer<ffi.Void> x,
      ffi.Size bx,
      ffi.Pointer<ffi.Void> y,
      ffi.Size by,
      ffi.Int nrc,
    );
typedef Dartggml_vec_dot_tFunction =
    void Function(
      int n,
      ffi.Pointer<ffi.Float> s,
      int bs,
      ffi.Pointer<ffi.Void> x,
      int bx,
      ffi.Pointer<ffi.Void> y,
      int by,
      int nrc,
    );

/// Internal types and functions exposed for tests and benchmarks
typedef ggml_vec_dot_t =
    ffi.Pointer<ffi.NativeFunction<ggml_vec_dot_tFunction>>;

final class ggml_type_traits_cpu extends ffi.Struct {
  external ggml_from_float_t from_float;

  external ggml_vec_dot_t vec_dot;

  @ffi.UnsignedInt()
  external int vec_dot_typeAsInt;

  ggml_type get vec_dot_type => ggml_type.fromValue(vec_dot_typeAsInt);

  /// number of rows to process simultaneously
  @ffi.Int64()
  external int nrows;
}

final class ggml_opt_dataset extends ffi.Opaque {}

final class ggml_opt_context extends ffi.Opaque {}

final class ggml_opt_result extends ffi.Opaque {}

typedef ggml_opt_dataset_t = ffi.Pointer<ggml_opt_dataset>;
typedef ggml_opt_context_t = ffi.Pointer<ggml_opt_context>;
typedef ggml_opt_result_t = ffi.Pointer<ggml_opt_result>;

/// built-in loss types, i.e. the built-in quantities minimized by the optimizer
/// custom loss types can be defined via mean or sum which simply reduce the outputs for all datapoints to a single value
enum ggml_opt_loss_type {
  GGML_OPT_LOSS_TYPE_MEAN(0),
  GGML_OPT_LOSS_TYPE_SUM(1),
  GGML_OPT_LOSS_TYPE_CROSS_ENTROPY(2),
  GGML_OPT_LOSS_TYPE_MEAN_SQUARED_ERROR(3);

  final int value;
  const ggml_opt_loss_type(this.value);

  static ggml_opt_loss_type fromValue(int value) => switch (value) {
    0 => GGML_OPT_LOSS_TYPE_MEAN,
    1 => GGML_OPT_LOSS_TYPE_SUM,
    2 => GGML_OPT_LOSS_TYPE_CROSS_ENTROPY,
    3 => GGML_OPT_LOSS_TYPE_MEAN_SQUARED_ERROR,
    _ => throw ArgumentError('Unknown value for ggml_opt_loss_type: $value'),
  };
}

/// ====== Model / Context ======
enum ggml_opt_build_type {
  GGML_OPT_BUILD_TYPE_FORWARD(10),
  GGML_OPT_BUILD_TYPE_GRAD(20),
  GGML_OPT_BUILD_TYPE_OPT(30);

  final int value;
  const ggml_opt_build_type(this.value);

  static ggml_opt_build_type fromValue(int value) => switch (value) {
    10 => GGML_OPT_BUILD_TYPE_FORWARD,
    20 => GGML_OPT_BUILD_TYPE_GRAD,
    30 => GGML_OPT_BUILD_TYPE_OPT,
    _ => throw ArgumentError('Unknown value for ggml_opt_build_type: $value'),
  };
}

enum ggml_opt_optimizer_type {
  GGML_OPT_OPTIMIZER_TYPE_ADAMW(0),
  GGML_OPT_OPTIMIZER_TYPE_SGD(1),
  GGML_OPT_OPTIMIZER_TYPE_COUNT(2);

  final int value;
  const ggml_opt_optimizer_type(this.value);

  static ggml_opt_optimizer_type fromValue(int value) => switch (value) {
    0 => GGML_OPT_OPTIMIZER_TYPE_ADAMW,
    1 => GGML_OPT_OPTIMIZER_TYPE_SGD,
    2 => GGML_OPT_OPTIMIZER_TYPE_COUNT,
    _ =>
      throw ArgumentError('Unknown value for ggml_opt_optimizer_type: $value'),
  };
}

final class UnnamedStruct extends ffi.Struct {
  /// learning rate
  @ffi.Float()
  external double alpha;

  /// first AdamW momentum
  @ffi.Float()
  external double beta1;

  /// second AdamW momentum
  @ffi.Float()
  external double beta2;

  /// epsilon for numerical stability
  @ffi.Float()
  external double eps;

  /// weight decay - 0.0f to disable
  @ffi.Float()
  external double wd;
}

final class UnnamedStruct$1 extends ffi.Struct {
  /// learning rate
  @ffi.Float()
  external double alpha;

  /// weight decay
  @ffi.Float()
  external double wd;
}

/// parameters that control which optimizer is used and how said optimizer tries to find the minimal loss
final class ggml_opt_optimizer_params extends ffi.Struct {
  external UnnamedStruct adamw;

  external UnnamedStruct$1 sgd;
}

typedef ggml_opt_get_optimizer_paramsFunction =
    ggml_opt_optimizer_params Function(ffi.Pointer<ffi.Void> userdata);

/// callback to calculate optimizer parameters prior to a backward pass
/// userdata can be used to pass arbitrary data
typedef ggml_opt_get_optimizer_params =
    ffi.Pointer<ffi.NativeFunction<ggml_opt_get_optimizer_paramsFunction>>;

/// parameters for initializing a new optimization context
final class ggml_opt_params extends ffi.Struct {
  /// defines which backends are used to construct the compute graphs
  external ggml_backend_sched_t backend_sched;

  /// by default the forward graph needs to be reconstructed for each eval
  /// if ctx_compute, inputs, and outputs are set the graphs are instead allocated statically
  external ffi.Pointer<ggml_context> ctx_compute;

  external ffi.Pointer<ggml_tensor> inputs;

  external ffi.Pointer<ggml_tensor> outputs;

  @ffi.UnsignedInt()
  external int loss_typeAsInt;

  ggml_opt_loss_type get loss_type =>
      ggml_opt_loss_type.fromValue(loss_typeAsInt);

  @ffi.UnsignedInt()
  external int build_typeAsInt;

  ggml_opt_build_type get build_type =>
      ggml_opt_build_type.fromValue(build_typeAsInt);

  /// after how many gradient accumulation steps an optimizer step should be done
  @ffi.Int32()
  external int opt_period;

  /// callback for calculating optimizer parameters
  external ggml_opt_get_optimizer_params get_opt_pars;

  /// userdata for calculating optimizer parameters
  external ffi.Pointer<ffi.Void> get_opt_pars_ud;

  /// only GGML_OPT_OPTIMIZER_TYPE_ADAMW needs m, v momenta per parameter tensor
  @ffi.UnsignedInt()
  external int optimizerAsInt;

  ggml_opt_optimizer_type get optimizer =>
      ggml_opt_optimizer_type.fromValue(optimizerAsInt);
}

typedef ggml_opt_epoch_callbackFunction =
    ffi.Void Function(
      ffi.Bool train,
      ggml_opt_context_t opt_ctx,
      ggml_opt_dataset_t dataset,
      ggml_opt_result_t result,
      ffi.Int64 ibatch,
      ffi.Int64 ibatch_max,
      ffi.Int64 t_start_us,
    );
typedef Dartggml_opt_epoch_callbackFunction =
    void Function(
      bool train,
      ggml_opt_context_t opt_ctx,
      ggml_opt_dataset_t dataset,
      ggml_opt_result_t result,
      int ibatch,
      int ibatch_max,
      int t_start_us,
    );

/// signature for a callback while evaluating opt_ctx on dataset, called after an evaluation
typedef ggml_opt_epoch_callback =
    ffi.Pointer<ffi.NativeFunction<ggml_opt_epoch_callbackFunction>>;

/// C interface
///
/// TODO: show sample usage
final class llama_vocab extends ffi.Opaque {}

final class llama_model extends ffi.Opaque {}

final class llama_context extends ffi.Opaque {}

typedef llama_token = ffi.Int32;
typedef Dartllama_token = int;

/// TODO: simplify (https://github.com/ggml-org/llama.cpp/pull/9294#pullrequestreview-2286561979)
final class llama_token_data extends ffi.Struct {
  /// token id
  @llama_token()
  external int id;

  /// log-odds of the token
  @ffi.Float()
  external double logit;

  /// probability of the token
  @ffi.Float()
  external double p;
}

final class llama_token_data_array extends ffi.Struct {
  /// TODO: consider SoA
  /// NOTE: this pointer can be modified by the samplers
  external ffi.Pointer<llama_token_data> data;

  @ffi.Size()
  external int size;

  /// this is the index in the data array (i.e. not the token id)
  @ffi.Int64()
  external int selected;

  /// note: do not assume the data is sorted - always check this flag
  @ffi.Bool()
  external bool sorted;
}

/// user code can implement the interface below in order to create custom llama_sampler
final class llama_sampler_i extends ffi.Struct {
  /// can be NULL
  external ffi.Pointer<
    ffi.NativeFunction<
      ffi.Pointer<ffi.Char> Function(ffi.Pointer<llama_sampler> smpl)
    >
  >
  name;

  /// can be NULL
  external ffi.Pointer<
    ffi.NativeFunction<
      ffi.Void Function(ffi.Pointer<llama_sampler> smpl, llama_token token)
    >
  >
  accept;

  /// required
  external ffi.Pointer<
    ffi.NativeFunction<
      ffi.Void Function(
        ffi.Pointer<llama_sampler> smpl,
        ffi.Pointer<llama_token_data_array> cur_p,
      )
    >
  >
  apply;

  /// can be NULL
  external ffi.Pointer<
    ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_sampler> smpl)>
  >
  reset;

  /// can be NULL if ctx is NULL
  external ffi.Pointer<
    ffi.NativeFunction<
      ffi.Pointer<llama_sampler> Function(ffi.Pointer<llama_sampler> smpl)
    >
  >
  clone;

  /// can be NULL if ctx is NULL
  external ffi.Pointer<
    ffi.NativeFunction<ffi.Void Function(ffi.Pointer<llama_sampler> smpl)>
  >
  free;
}

/// Sampling API
///
/// Sample usage:
///
/// // prepare the sampling chain at the start
/// auto sparams = llama_sampler_chain_default_params();
///
/// llama_sampler * smpl = llama_sampler_chain_init(sparams);
///
/// llama_sampler_chain_add(smpl, llama_sampler_init_top_k(50));
/// llama_sampler_chain_add(smpl, llama_sampler_init_top_p(0.9, 1));
/// llama_sampler_chain_add(smpl, llama_sampler_init_temp (0.8));
///
/// // typically, the chain should end with a sampler such as "greedy", "dist" or "mirostat"
/// // this sampler will be responsible to select the actual token
/// llama_sampler_chain_add(smpl, llama_sampler_init_dist(seed));
///
/// ...
///
/// // decoding loop:
/// while (...) {
/// ...
///
/// llama_decode(ctx, batch);
///
/// // sample from the logits of the last token in the batch
/// const llama_token id = llama_sampler_sample(smpl, ctx, -1);
///
/// ...
/// }
///
/// llama_sampler_free(smpl);
///
/// TODO: In the future, llama_sampler will be utilized to offload the sampling to the backends (e.g. GPU).
typedef llama_sampler_context_t = ffi.Pointer<ffi.Void>;

final class llama_sampler extends ffi.Struct {
  external ffi.Pointer<llama_sampler_i> iface;

  external llama_sampler_context_t ctx;
}

final class llama_memory_i extends ffi.Opaque {}

typedef llama_memory_t = ffi.Pointer<llama_memory_i>;
typedef llama_pos = ffi.Int32;
typedef Dartllama_pos = int;
typedef llama_seq_id = ffi.Int32;
typedef Dartllama_seq_id = int;

enum llama_vocab_type {
  /// For models without vocab
  LLAMA_VOCAB_TYPE_NONE(0),

  /// LLaMA tokenizer based on byte-level BPE with byte fallback
  LLAMA_VOCAB_TYPE_SPM(1),

  /// GPT-2 tokenizer based on byte-level BPE
  LLAMA_VOCAB_TYPE_BPE(2),

  /// BERT tokenizer based on WordPiece
  LLAMA_VOCAB_TYPE_WPM(3),

  /// T5 tokenizer based on Unigram
  LLAMA_VOCAB_TYPE_UGM(4),

  /// RWKV tokenizer based on greedy tokenization
  LLAMA_VOCAB_TYPE_RWKV(5),

  /// PLaMo-2 tokenizer based on Aho-Corasick with dynamic programming
  LLAMA_VOCAB_TYPE_PLAMO2(6);

  final int value;
  const llama_vocab_type(this.value);

  static llama_vocab_type fromValue(int value) => switch (value) {
    0 => LLAMA_VOCAB_TYPE_NONE,
    1 => LLAMA_VOCAB_TYPE_SPM,
    2 => LLAMA_VOCAB_TYPE_BPE,
    3 => LLAMA_VOCAB_TYPE_WPM,
    4 => LLAMA_VOCAB_TYPE_UGM,
    5 => LLAMA_VOCAB_TYPE_RWKV,
    6 => LLAMA_VOCAB_TYPE_PLAMO2,
    _ => throw ArgumentError('Unknown value for llama_vocab_type: $value'),
  };
}

enum llama_rope_type {
  LLAMA_ROPE_TYPE_NONE(-1),
  LLAMA_ROPE_TYPE_NORM(0),
  LLAMA_ROPE_TYPE_NEOX(2),
  LLAMA_ROPE_TYPE_MROPE(8),
  LLAMA_ROPE_TYPE_IMROPE(40),
  LLAMA_ROPE_TYPE_VISION(24);

  final int value;
  const llama_rope_type(this.value);

  static llama_rope_type fromValue(int value) => switch (value) {
    -1 => LLAMA_ROPE_TYPE_NONE,
    0 => LLAMA_ROPE_TYPE_NORM,
    2 => LLAMA_ROPE_TYPE_NEOX,
    8 => LLAMA_ROPE_TYPE_MROPE,
    40 => LLAMA_ROPE_TYPE_IMROPE,
    24 => LLAMA_ROPE_TYPE_VISION,
    _ => throw ArgumentError('Unknown value for llama_rope_type: $value'),
  };
}

enum llama_token_type {
  LLAMA_TOKEN_TYPE_UNDEFINED(0),
  LLAMA_TOKEN_TYPE_NORMAL(1),
  LLAMA_TOKEN_TYPE_UNKNOWN(2),
  LLAMA_TOKEN_TYPE_CONTROL(3),
  LLAMA_TOKEN_TYPE_USER_DEFINED(4),
  LLAMA_TOKEN_TYPE_UNUSED(5),
  LLAMA_TOKEN_TYPE_BYTE(6);

  final int value;
  const llama_token_type(this.value);

  static llama_token_type fromValue(int value) => switch (value) {
    0 => LLAMA_TOKEN_TYPE_UNDEFINED,
    1 => LLAMA_TOKEN_TYPE_NORMAL,
    2 => LLAMA_TOKEN_TYPE_UNKNOWN,
    3 => LLAMA_TOKEN_TYPE_CONTROL,
    4 => LLAMA_TOKEN_TYPE_USER_DEFINED,
    5 => LLAMA_TOKEN_TYPE_UNUSED,
    6 => LLAMA_TOKEN_TYPE_BYTE,
    _ => throw ArgumentError('Unknown value for llama_token_type: $value'),
  };
}

enum llama_token_attr {
  LLAMA_TOKEN_ATTR_UNDEFINED(0),
  LLAMA_TOKEN_ATTR_UNKNOWN(1),
  LLAMA_TOKEN_ATTR_UNUSED(2),
  LLAMA_TOKEN_ATTR_NORMAL(4),

  /// SPECIAL?
  LLAMA_TOKEN_ATTR_CONTROL(8),
  LLAMA_TOKEN_ATTR_USER_DEFINED(16),
  LLAMA_TOKEN_ATTR_BYTE(32),
  LLAMA_TOKEN_ATTR_NORMALIZED(64),
  LLAMA_TOKEN_ATTR_LSTRIP(128),
  LLAMA_TOKEN_ATTR_RSTRIP(256),
  LLAMA_TOKEN_ATTR_SINGLE_WORD(512);

  final int value;
  const llama_token_attr(this.value);

  static llama_token_attr fromValue(int value) => switch (value) {
    0 => LLAMA_TOKEN_ATTR_UNDEFINED,
    1 => LLAMA_TOKEN_ATTR_UNKNOWN,
    2 => LLAMA_TOKEN_ATTR_UNUSED,
    4 => LLAMA_TOKEN_ATTR_NORMAL,
    8 => LLAMA_TOKEN_ATTR_CONTROL,
    16 => LLAMA_TOKEN_ATTR_USER_DEFINED,
    32 => LLAMA_TOKEN_ATTR_BYTE,
    64 => LLAMA_TOKEN_ATTR_NORMALIZED,
    128 => LLAMA_TOKEN_ATTR_LSTRIP,
    256 => LLAMA_TOKEN_ATTR_RSTRIP,
    512 => LLAMA_TOKEN_ATTR_SINGLE_WORD,
    _ => throw ArgumentError('Unknown value for llama_token_attr: $value'),
  };
}

/// model file types
enum llama_ftype {
  LLAMA_FTYPE_ALL_F32(0),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_F16(1),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q4_0(2),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q4_1(3),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q8_0(7),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q5_0(8),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q5_1(9),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q2_K(10),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q3_K_S(11),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q3_K_M(12),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q3_K_L(13),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q4_K_S(14),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q4_K_M(15),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q5_K_S(16),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q5_K_M(17),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q6_K(18),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ2_XXS(19),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ2_XS(20),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_Q2_K_S(21),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ3_XS(22),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ3_XXS(23),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ1_S(24),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ4_NL(25),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ3_S(26),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ3_M(27),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ2_S(28),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ2_M(29),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ4_XS(30),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_IQ1_M(31),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_BF16(32),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_TQ1_0(36),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_TQ2_0(37),

  /// except 1d tensors
  LLAMA_FTYPE_MOSTLY_MXFP4_MOE(38),

  /// not specified in the model file
  LLAMA_FTYPE_GUESSED(1024);

  final int value;
  const llama_ftype(this.value);

  static llama_ftype fromValue(int value) => switch (value) {
    0 => LLAMA_FTYPE_ALL_F32,
    1 => LLAMA_FTYPE_MOSTLY_F16,
    2 => LLAMA_FTYPE_MOSTLY_Q4_0,
    3 => LLAMA_FTYPE_MOSTLY_Q4_1,
    7 => LLAMA_FTYPE_MOSTLY_Q8_0,
    8 => LLAMA_FTYPE_MOSTLY_Q5_0,
    9 => LLAMA_FTYPE_MOSTLY_Q5_1,
    10 => LLAMA_FTYPE_MOSTLY_Q2_K,
    11 => LLAMA_FTYPE_MOSTLY_Q3_K_S,
    12 => LLAMA_FTYPE_MOSTLY_Q3_K_M,
    13 => LLAMA_FTYPE_MOSTLY_Q3_K_L,
    14 => LLAMA_FTYPE_MOSTLY_Q4_K_S,
    15 => LLAMA_FTYPE_MOSTLY_Q4_K_M,
    16 => LLAMA_FTYPE_MOSTLY_Q5_K_S,
    17 => LLAMA_FTYPE_MOSTLY_Q5_K_M,
    18 => LLAMA_FTYPE_MOSTLY_Q6_K,
    19 => LLAMA_FTYPE_MOSTLY_IQ2_XXS,
    20 => LLAMA_FTYPE_MOSTLY_IQ2_XS,
    21 => LLAMA_FTYPE_MOSTLY_Q2_K_S,
    22 => LLAMA_FTYPE_MOSTLY_IQ3_XS,
    23 => LLAMA_FTYPE_MOSTLY_IQ3_XXS,
    24 => LLAMA_FTYPE_MOSTLY_IQ1_S,
    25 => LLAMA_FTYPE_MOSTLY_IQ4_NL,
    26 => LLAMA_FTYPE_MOSTLY_IQ3_S,
    27 => LLAMA_FTYPE_MOSTLY_IQ3_M,
    28 => LLAMA_FTYPE_MOSTLY_IQ2_S,
    29 => LLAMA_FTYPE_MOSTLY_IQ2_M,
    30 => LLAMA_FTYPE_MOSTLY_IQ4_XS,
    31 => LLAMA_FTYPE_MOSTLY_IQ1_M,
    32 => LLAMA_FTYPE_MOSTLY_BF16,
    36 => LLAMA_FTYPE_MOSTLY_TQ1_0,
    37 => LLAMA_FTYPE_MOSTLY_TQ2_0,
    38 => LLAMA_FTYPE_MOSTLY_MXFP4_MOE,
    1024 => LLAMA_FTYPE_GUESSED,
    _ => throw ArgumentError('Unknown value for llama_ftype: $value'),
  };
}

enum llama_rope_scaling_type {
  LLAMA_ROPE_SCALING_TYPE_UNSPECIFIED(-1),
  LLAMA_ROPE_SCALING_TYPE_NONE(0),
  LLAMA_ROPE_SCALING_TYPE_LINEAR(1),
  LLAMA_ROPE_SCALING_TYPE_YARN(2),
  LLAMA_ROPE_SCALING_TYPE_LONGROPE(3);

  static const LLAMA_ROPE_SCALING_TYPE_MAX_VALUE =
      LLAMA_ROPE_SCALING_TYPE_LONGROPE;

  final int value;
  const llama_rope_scaling_type(this.value);

  static llama_rope_scaling_type fromValue(int value) => switch (value) {
    -1 => LLAMA_ROPE_SCALING_TYPE_UNSPECIFIED,
    0 => LLAMA_ROPE_SCALING_TYPE_NONE,
    1 => LLAMA_ROPE_SCALING_TYPE_LINEAR,
    2 => LLAMA_ROPE_SCALING_TYPE_YARN,
    3 => LLAMA_ROPE_SCALING_TYPE_LONGROPE,
    _ =>
      throw ArgumentError('Unknown value for llama_rope_scaling_type: $value'),
  };

  @override
  String toString() {
    if (this == LLAMA_ROPE_SCALING_TYPE_LONGROPE)
      return "llama_rope_scaling_type.LLAMA_ROPE_SCALING_TYPE_LONGROPE, llama_rope_scaling_type.LLAMA_ROPE_SCALING_TYPE_MAX_VALUE";
    return super.toString();
  }
}

enum llama_pooling_type {
  LLAMA_POOLING_TYPE_UNSPECIFIED(-1),
  LLAMA_POOLING_TYPE_NONE(0),
  LLAMA_POOLING_TYPE_MEAN(1),
  LLAMA_POOLING_TYPE_CLS(2),
  LLAMA_POOLING_TYPE_LAST(3),

  /// used by reranking models to attach the classification head to the graph
  LLAMA_POOLING_TYPE_RANK(4);

  final int value;
  const llama_pooling_type(this.value);

  static llama_pooling_type fromValue(int value) => switch (value) {
    -1 => LLAMA_POOLING_TYPE_UNSPECIFIED,
    0 => LLAMA_POOLING_TYPE_NONE,
    1 => LLAMA_POOLING_TYPE_MEAN,
    2 => LLAMA_POOLING_TYPE_CLS,
    3 => LLAMA_POOLING_TYPE_LAST,
    4 => LLAMA_POOLING_TYPE_RANK,
    _ => throw ArgumentError('Unknown value for llama_pooling_type: $value'),
  };
}

enum llama_attention_type {
  LLAMA_ATTENTION_TYPE_UNSPECIFIED(-1),
  LLAMA_ATTENTION_TYPE_CAUSAL(0),
  LLAMA_ATTENTION_TYPE_NON_CAUSAL(1);

  final int value;
  const llama_attention_type(this.value);

  static llama_attention_type fromValue(int value) => switch (value) {
    -1 => LLAMA_ATTENTION_TYPE_UNSPECIFIED,
    0 => LLAMA_ATTENTION_TYPE_CAUSAL,
    1 => LLAMA_ATTENTION_TYPE_NON_CAUSAL,
    _ => throw ArgumentError('Unknown value for llama_attention_type: $value'),
  };
}

enum llama_flash_attn_type {
  LLAMA_FLASH_ATTN_TYPE_AUTO(-1),
  LLAMA_FLASH_ATTN_TYPE_DISABLED(0),
  LLAMA_FLASH_ATTN_TYPE_ENABLED(1);

  final int value;
  const llama_flash_attn_type(this.value);

  static llama_flash_attn_type fromValue(int value) => switch (value) {
    -1 => LLAMA_FLASH_ATTN_TYPE_AUTO,
    0 => LLAMA_FLASH_ATTN_TYPE_DISABLED,
    1 => LLAMA_FLASH_ATTN_TYPE_ENABLED,
    _ => throw ArgumentError('Unknown value for llama_flash_attn_type: $value'),
  };
}

enum llama_split_mode {
  /// single GPU
  LLAMA_SPLIT_MODE_NONE(0),

  /// split layers and KV across GPUs
  LLAMA_SPLIT_MODE_LAYER(1),

  /// split layers and KV across GPUs, use tensor parallelism if supported
  LLAMA_SPLIT_MODE_ROW(2);

  final int value;
  const llama_split_mode(this.value);

  static llama_split_mode fromValue(int value) => switch (value) {
    0 => LLAMA_SPLIT_MODE_NONE,
    1 => LLAMA_SPLIT_MODE_LAYER,
    2 => LLAMA_SPLIT_MODE_ROW,
    _ => throw ArgumentError('Unknown value for llama_split_mode: $value'),
  };
}

typedef llama_progress_callbackFunction =
    ffi.Bool Function(ffi.Float progress, ffi.Pointer<ffi.Void> user_data);
typedef Dartllama_progress_callbackFunction =
    bool Function(double progress, ffi.Pointer<ffi.Void> user_data);
typedef llama_progress_callback =
    ffi.Pointer<ffi.NativeFunction<llama_progress_callbackFunction>>;

/// Input data for llama_encode/llama_decode
/// A llama_batch object can contain input about one or many sequences
/// The provided arrays (i.e. token, embd, pos, etc.) must have size of n_tokens
///
/// - token  : the token ids of the input (used when embd is NULL)
/// - embd   : token embeddings (i.e. float vector of size n_embd) (used when token is NULL)
/// - pos    : the positions of the respective token in the sequence
/// (if set to NULL, the token position will be tracked automatically by llama_encode/llama_decode)
/// - seq_id : the sequence to which the respective token belongs
/// (if set to NULL, the sequence ID will be assumed to be 0)
/// - logits : if zero, the logits (and/or the embeddings) for the respective token will not be output
/// (if set to NULL:
/// - if embeddings: all tokens are output
/// - if not:        only the last token is output
/// )
final class llama_batch extends ffi.Struct {
  @ffi.Int32()
  external int n_tokens;

  external ffi.Pointer<llama_token> token;

  external ffi.Pointer<ffi.Float> embd;

  external ffi.Pointer<llama_pos> pos;

  external ffi.Pointer<ffi.Int32> n_seq_id;

  external ffi.Pointer<ffi.Pointer<llama_seq_id>> seq_id;

  /// TODO: rename this to "output"
  external ffi.Pointer<ffi.Int8> logits;
}

enum llama_model_kv_override_type {
  LLAMA_KV_OVERRIDE_TYPE_INT(0),
  LLAMA_KV_OVERRIDE_TYPE_FLOAT(1),
  LLAMA_KV_OVERRIDE_TYPE_BOOL(2),
  LLAMA_KV_OVERRIDE_TYPE_STR(3);

  final int value;
  const llama_model_kv_override_type(this.value);

  static llama_model_kv_override_type fromValue(int value) => switch (value) {
    0 => LLAMA_KV_OVERRIDE_TYPE_INT,
    1 => LLAMA_KV_OVERRIDE_TYPE_FLOAT,
    2 => LLAMA_KV_OVERRIDE_TYPE_BOOL,
    3 => LLAMA_KV_OVERRIDE_TYPE_STR,
    _ =>
      throw ArgumentError(
        'Unknown value for llama_model_kv_override_type: $value',
      ),
  };
}

enum llama_model_meta_key {
  LLAMA_MODEL_META_KEY_SAMPLING_SEQUENCE(0),
  LLAMA_MODEL_META_KEY_SAMPLING_TOP_K(1),
  LLAMA_MODEL_META_KEY_SAMPLING_TOP_P(2),
  LLAMA_MODEL_META_KEY_SAMPLING_MIN_P(3),
  LLAMA_MODEL_META_KEY_SAMPLING_XTC_PROBABILITY(4),
  LLAMA_MODEL_META_KEY_SAMPLING_XTC_THRESHOLD(5),
  LLAMA_MODEL_META_KEY_SAMPLING_TEMP(6),
  LLAMA_MODEL_META_KEY_SAMPLING_PENALTY_LAST_N(7),
  LLAMA_MODEL_META_KEY_SAMPLING_PENALTY_REPEAT(8),
  LLAMA_MODEL_META_KEY_SAMPLING_MIROSTAT(9),
  LLAMA_MODEL_META_KEY_SAMPLING_MIROSTAT_TAU(10),
  LLAMA_MODEL_META_KEY_SAMPLING_MIROSTAT_ETA(11);

  final int value;
  const llama_model_meta_key(this.value);

  static llama_model_meta_key fromValue(int value) => switch (value) {
    0 => LLAMA_MODEL_META_KEY_SAMPLING_SEQUENCE,
    1 => LLAMA_MODEL_META_KEY_SAMPLING_TOP_K,
    2 => LLAMA_MODEL_META_KEY_SAMPLING_TOP_P,
    3 => LLAMA_MODEL_META_KEY_SAMPLING_MIN_P,
    4 => LLAMA_MODEL_META_KEY_SAMPLING_XTC_PROBABILITY,
    5 => LLAMA_MODEL_META_KEY_SAMPLING_XTC_THRESHOLD,
    6 => LLAMA_MODEL_META_KEY_SAMPLING_TEMP,
    7 => LLAMA_MODEL_META_KEY_SAMPLING_PENALTY_LAST_N,
    8 => LLAMA_MODEL_META_KEY_SAMPLING_PENALTY_REPEAT,
    9 => LLAMA_MODEL_META_KEY_SAMPLING_MIROSTAT,
    10 => LLAMA_MODEL_META_KEY_SAMPLING_MIROSTAT_TAU,
    11 => LLAMA_MODEL_META_KEY_SAMPLING_MIROSTAT_ETA,
    _ => throw ArgumentError('Unknown value for llama_model_meta_key: $value'),
  };
}

final class UnnamedUnion extends ffi.Union {
  @ffi.Int64()
  external int val_i64;

  @ffi.Double()
  external double val_f64;

  @ffi.Bool()
  external bool val_bool;

  @ffi.Array.multi([128])
  external ffi.Array<ffi.Char> val_str;
}

final class llama_model_kv_override extends ffi.Struct {
  @ffi.UnsignedInt()
  external int tagAsInt;

  llama_model_kv_override_type get tag =>
      llama_model_kv_override_type.fromValue(tagAsInt);

  @ffi.Array.multi([128])
  external ffi.Array<ffi.Char> key;

  external UnnamedUnion unnamed;
}

final class llama_model_tensor_buft_override extends ffi.Struct {
  external ffi.Pointer<ffi.Char> pattern;

  external ggml_backend_buffer_type_t buft;
}

final class llama_model_params extends ffi.Struct {
  /// NULL-terminated list of devices to use for offloading (if NULL, all available devices are used)
  external ffi.Pointer<ggml_backend_dev_t> devices;

  /// NULL-terminated list of buffer types to use for tensors that match a pattern
  external ffi.Pointer<llama_model_tensor_buft_override> tensor_buft_overrides;

  /// number of layers to store in VRAM
  @ffi.Int32()
  external int n_gpu_layers;

  /// how to split the model across multiple GPUs
  @ffi.UnsignedInt()
  external int split_modeAsInt;

  llama_split_mode get split_mode =>
      llama_split_mode.fromValue(split_modeAsInt);

  /// the GPU that is used for the entire model when split_mode is LLAMA_SPLIT_MODE_NONE
  @ffi.Int32()
  external int main_gpu;

  /// proportion of the model (layers or rows) to offload to each GPU, size: llama_max_devices()
  external ffi.Pointer<ffi.Float> tensor_split;

  /// Called with a progress value between 0.0 and 1.0. Pass NULL to disable.
  /// If the provided progress_callback returns true, model loading continues.
  /// If it returns false, model loading is immediately aborted.
  external llama_progress_callback progress_callback;

  /// context pointer passed to the progress callback
  external ffi.Pointer<ffi.Void> progress_callback_user_data;

  /// override key-value pairs of the model meta data
  external ffi.Pointer<llama_model_kv_override> kv_overrides;

  /// only load the vocabulary, no weights
  @ffi.Bool()
  external bool vocab_only;

  /// use mmap if possible
  @ffi.Bool()
  external bool use_mmap;

  /// force system to keep model in RAM
  @ffi.Bool()
  external bool use_mlock;

  /// validate model tensor data
  @ffi.Bool()
  external bool check_tensors;

  /// use extra buffer types (used for weight repacking)
  @ffi.Bool()
  external bool use_extra_bufts;

  /// bypass host buffer allowing extra buffers to be used
  @ffi.Bool()
  external bool no_host;

  /// only load metadata and simulate memory allocations
  @ffi.Bool()
  external bool no_alloc;
}

/// NOTE: changing the default values of parameters marked as [EXPERIMENTAL] may cause crashes or incorrect results in certain configurations
/// https://github.com/ggml-org/llama.cpp/pull/7544
final class llama_context_params extends ffi.Struct {
  /// text context, 0 = from model
  @ffi.Uint32()
  external int n_ctx;

  /// logical maximum batch size that can be submitted to llama_decode
  @ffi.Uint32()
  external int n_batch;

  /// physical maximum batch size
  @ffi.Uint32()
  external int n_ubatch;

  /// max number of sequences (i.e. distinct states for recurrent models)
  @ffi.Uint32()
  external int n_seq_max;

  /// number of threads to use for generation
  @ffi.Int32()
  external int n_threads;

  /// number of threads to use for batch processing
  @ffi.Int32()
  external int n_threads_batch;

  /// RoPE scaling type, from `enum llama_rope_scaling_type`
  @ffi.Int()
  external int rope_scaling_typeAsInt;

  llama_rope_scaling_type get rope_scaling_type =>
      llama_rope_scaling_type.fromValue(rope_scaling_typeAsInt);

  /// whether to pool (sum) embedding results by sequence id
  @ffi.Int()
  external int pooling_typeAsInt;

  llama_pooling_type get pooling_type =>
      llama_pooling_type.fromValue(pooling_typeAsInt);

  /// attention type to use for embeddings
  @ffi.Int()
  external int attention_typeAsInt;

  llama_attention_type get attention_type =>
      llama_attention_type.fromValue(attention_typeAsInt);

  /// when to enable Flash Attention
  @ffi.Int()
  external int flash_attn_typeAsInt;

  llama_flash_attn_type get flash_attn_type =>
      llama_flash_attn_type.fromValue(flash_attn_typeAsInt);

  /// RoPE base frequency, 0 = from model
  @ffi.Float()
  external double rope_freq_base;

  /// RoPE frequency scaling factor, 0 = from model
  @ffi.Float()
  external double rope_freq_scale;

  /// YaRN extrapolation mix factor, negative = from model
  @ffi.Float()
  external double yarn_ext_factor;

  /// YaRN magnitude scaling factor
  @ffi.Float()
  external double yarn_attn_factor;

  /// YaRN low correction dim
  @ffi.Float()
  external double yarn_beta_fast;

  /// YaRN high correction dim
  @ffi.Float()
  external double yarn_beta_slow;

  /// YaRN original context size
  @ffi.Uint32()
  external int yarn_orig_ctx;

  /// [DEPRECATED] defragment the KV cache if holes/size > thold, <= 0 disabled (default)
  @ffi.Float()
  external double defrag_thold;

  external ggml_backend_sched_eval_callback cb_eval;

  external ffi.Pointer<ffi.Void> cb_eval_user_data;

  /// data type for K cache [EXPERIMENTAL]
  @ffi.UnsignedInt()
  external int type_kAsInt;

  ggml_type get type_k => ggml_type.fromValue(type_kAsInt);

  /// data type for V cache [EXPERIMENTAL]
  @ffi.UnsignedInt()
  external int type_vAsInt;

  ggml_type get type_v => ggml_type.fromValue(type_vAsInt);

  /// Abort callback
  /// if it returns true, execution of llama_decode() will be aborted
  /// currently works only with CPU execution
  external ggml_abort_callback abort_callback;

  external ffi.Pointer<ffi.Void> abort_callback_data;

  /// if true, extract embeddings (together with logits)
  @ffi.Bool()
  external bool embeddings;

  /// offload the KQV ops (including the KV cache) to GPU
  @ffi.Bool()
  external bool offload_kqv;

  /// measure performance timings
  @ffi.Bool()
  external bool no_perf;

  /// offload host tensor operations to device
  @ffi.Bool()
  external bool op_offload;

  /// use full-size SWA cache (https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
  /// NOTE: setting to false when n_seq_max > 1 can cause bad performance in some cases
  /// ref: https://github.com/ggml-org/llama.cpp/pull/13845#issuecomment-2924800573
  @ffi.Bool()
  external bool swa_full;

  /// use a unified buffer across the input sequences when computing the attention
  /// try to disable when n_seq_max > 1 for improved performance when the sequences do not share a large prefix
  /// ref: https://github.com/ggml-org/llama.cpp/pull/14363
  @ffi.Bool()
  external bool kv_unified;
}

/// model quantization parameters
final class llama_model_quantize_params extends ffi.Struct {
  /// number of threads to use for quantizing, if <=0 will use std::thread::hardware_concurrency()
  @ffi.Int32()
  external int nthread;

  /// quantize to this llama_ftype
  @ffi.UnsignedInt()
  external int ftypeAsInt;

  llama_ftype get ftype => llama_ftype.fromValue(ftypeAsInt);

  /// output tensor type
  @ffi.UnsignedInt()
  external int output_tensor_typeAsInt;

  ggml_type get output_tensor_type =>
      ggml_type.fromValue(output_tensor_typeAsInt);

  /// token embeddings tensor type
  @ffi.UnsignedInt()
  external int token_embedding_typeAsInt;

  ggml_type get token_embedding_type =>
      ggml_type.fromValue(token_embedding_typeAsInt);

  /// allow quantizing non-f32/f16 tensors
  @ffi.Bool()
  external bool allow_requantize;

  /// quantize output.weight
  @ffi.Bool()
  external bool quantize_output_tensor;

  /// only copy tensors - ftype, allow_requantize and quantize_output_tensor are ignored
  @ffi.Bool()
  external bool only_copy;

  /// quantize all tensors to the default type
  @ffi.Bool()
  external bool pure;

  /// quantize to the same number of shards
  @ffi.Bool()
  external bool keep_split;

  /// pointer to importance matrix data
  external ffi.Pointer<ffi.Void> imatrix;

  /// pointer to vector containing overrides
  external ffi.Pointer<ffi.Void> kv_overrides;

  /// pointer to vector containing tensor types
  external ffi.Pointer<ffi.Void> tensor_types;

  /// pointer to vector containing layer indices to prune
  external ffi.Pointer<ffi.Void> prune_layers;
}

final class llama_logit_bias extends ffi.Struct {
  @llama_token()
  external int token;

  @ffi.Float()
  external double bias;
}

final class llama_sampler_chain_params extends ffi.Struct {
  /// whether to measure performance timings
  @ffi.Bool()
  external bool no_perf;
}

/// used in chat template
final class llama_chat_message extends ffi.Struct {
  external ffi.Pointer<ffi.Char> role;

  external ffi.Pointer<ffi.Char> content;
}

/// lora adapter
final class llama_adapter_lora extends ffi.Opaque {}

typedef llama_state_seq_flags = ffi.Uint32;
typedef Dartllama_state_seq_flags = int;

/// Performance utils
///
/// NOTE: Used by llama.cpp examples/tools, avoid using in third-party apps. Instead, do your own performance measurements.
final class llama_perf_context_data extends ffi.Struct {
  /// absolute start time
  @ffi.Double()
  external double t_start_ms;

  /// time needed for loading the model
  @ffi.Double()
  external double t_load_ms;

  /// time needed for processing the prompt
  @ffi.Double()
  external double t_p_eval_ms;

  /// time needed for generating tokens
  @ffi.Double()
  external double t_eval_ms;

  /// number of prompt tokens
  @ffi.Int32()
  external int n_p_eval;

  /// number of generated tokens
  @ffi.Int32()
  external int n_eval;

  /// number of times a ggml compute graph had been reused
  @ffi.Int32()
  external int n_reused;
}

final class llama_perf_sampler_data extends ffi.Struct {
  /// time needed for sampling in ms
  @ffi.Double()
  external double t_sample_ms;

  /// number of sampled tokens
  @ffi.Int32()
  external int n_sample;
}

typedef llama_opt_param_filterFunction =
    ffi.Bool Function(
      ffi.Pointer<ggml_tensor> tensor,
      ffi.Pointer<ffi.Void> userdata,
    );
typedef Dartllama_opt_param_filterFunction =
    bool Function(
      ffi.Pointer<ggml_tensor> tensor,
      ffi.Pointer<ffi.Void> userdata,
    );

/// function that returns whether or not a given tensor contains trainable parameters
typedef llama_opt_param_filter =
    ffi.Pointer<ffi.NativeFunction<llama_opt_param_filterFunction>>;

final class llama_opt_params extends ffi.Struct {
  /// assumed context size post training, use context size specified in llama_context if 0
  @ffi.Uint32()
  external int n_ctx_train;

  /// callback for determining which tensors contain trainable parameters
  external llama_opt_param_filter param_filter;

  /// userdata for determining which tensors contain trainable parameters
  external ffi.Pointer<ffi.Void> param_filter_ud;

  /// callback for calculating optimizer parameters
  external ggml_opt_get_optimizer_params get_opt_pars;

  /// userdata for calculating optimizer parameters
  external ffi.Pointer<ffi.Void> get_opt_pars_ud;

  @ffi.UnsignedInt()
  external int optimizer_typeAsInt;

  ggml_opt_optimizer_type get optimizer_type =>
      ggml_opt_optimizer_type.fromValue(optimizer_typeAsInt);
}

final class lcpp_data_pvalue extends ffi.Struct {
  external ffi.Pointer<ffi.Char> value;

  @ffi.Int32()
  external int length;

  @ffi.Bool()
  external bool found;
}

typedef lcpp_data_pvalue_t = lcpp_data_pvalue;

enum lcpp_numa_strategy {
  LCPP_NUMA_STRATEGY_DISABLED(0),
  LCPP_NUMA_STRATEGY_DISTRIBUTE(1),
  LCPP_NUMA_STRATEGY_ISOLATE(2),
  LCPP_NUMA_STRATEGY_NUMACTL(3),
  LCPP_NUMA_STRATEGY_MIRROR(4);

  final int value;
  const lcpp_numa_strategy(this.value);

  static lcpp_numa_strategy fromValue(int value) => switch (value) {
    0 => LCPP_NUMA_STRATEGY_DISABLED,
    1 => LCPP_NUMA_STRATEGY_DISTRIBUTE,
    2 => LCPP_NUMA_STRATEGY_ISOLATE,
    3 => LCPP_NUMA_STRATEGY_NUMACTL,
    4 => LCPP_NUMA_STRATEGY_MIRROR,
    _ => throw ArgumentError('Unknown value for lcpp_numa_strategy: $value'),
  };
}

enum lcpp_split_mode {
  /// single GPU
  LCPP_SPLIT_MODE_NONE(0),

  /// split layers and KV across GPUs
  LCPP_SPLIT_MODE_LAYER(1),

  /// split layers and KV across GPUs, use tensor parallelism if supported
  LCPP_SPLIT_MODE_ROW(2);

  final int value;
  const lcpp_split_mode(this.value);

  static lcpp_split_mode fromValue(int value) => switch (value) {
    0 => LCPP_SPLIT_MODE_NONE,
    1 => LCPP_SPLIT_MODE_LAYER,
    2 => LCPP_SPLIT_MODE_ROW,
    _ => throw ArgumentError('Unknown value for lcpp_split_mode: $value'),
  };
}

enum lcpp_finish_reason {
  /// normal completed
  LCPP_FINISH_REASON_STOP(0),

  /// exceeded max length
  LCPP_FINISH_REASON_LENGTH(1),

  /// generated EOS token
  LCPP_FINISH_REASON_EOS(2),

  /// generation cancelled
  LCPP_FINISH_REASON_CANCELLED(3),

  /// generation aborted due to error
  LCPP_FINISH_REASON_ABORTED(4),

  /// generation stopped due to tool call
  LCPP_FINISH_REASON_TOOL_CALLS(5),

  /// generation stopped due for content filter reasons
  LCPP_FINISH_REASON_CONTENT_FILTER(6),

  /// generation stopped due to recitation stopping criteria
  LCPP_FINISH_REASON_RECITATION(7),

  /// generation stopped due to tokenization error
  LCPP_FINISH_REASON_ERROR_TOKENIZE(8),

  /// generation stopped due to tokenization error
  LCPP_FINISH_REASON_ERROR_DETOKENIZE(9),

  /// generation stopped due to decoding error
  LCPP_FINISH_REASON_ERROR_DECODE(10),

  /// generation stopped due to encoding error
  LCPP_FINISH_REASON_ERROR_ENCODE(11),

  /// generation stopped due to timeout
  LCPP_FINISH_REASON_TIMEOUT(13),

  /// generation stopped invalid input batch
  LCPP_FINISH_REASON_INVALID_BATCH_INPUT(14),

  /// generation stopped due to encoding error
  LCPP_FINISH_REASON_UNHANDLED_EXCEPTION(32),

  /// generation stopped due to encoding error
  LCPP_FINISH_REASON_FATAL_ERROR(64),

  /// unspecified reason
  LCPP_FINISH_REASON_UNSPECIFIED(127);

  /// generation stopped could not find a KV slot for the batch (try reducing the size of the batch or increase the context)
  static const LCPP_FINISH_REASON_NO_KV_SLOT_AVAILABLE =
      LCPP_FINISH_REASON_TIMEOUT;

  final int value;
  const lcpp_finish_reason(this.value);

  static lcpp_finish_reason fromValue(int value) => switch (value) {
    0 => LCPP_FINISH_REASON_STOP,
    1 => LCPP_FINISH_REASON_LENGTH,
    2 => LCPP_FINISH_REASON_EOS,
    3 => LCPP_FINISH_REASON_CANCELLED,
    4 => LCPP_FINISH_REASON_ABORTED,
    5 => LCPP_FINISH_REASON_TOOL_CALLS,
    6 => LCPP_FINISH_REASON_CONTENT_FILTER,
    7 => LCPP_FINISH_REASON_RECITATION,
    8 => LCPP_FINISH_REASON_ERROR_TOKENIZE,
    9 => LCPP_FINISH_REASON_ERROR_DETOKENIZE,
    10 => LCPP_FINISH_REASON_ERROR_DECODE,
    11 => LCPP_FINISH_REASON_ERROR_ENCODE,
    13 => LCPP_FINISH_REASON_TIMEOUT,
    14 => LCPP_FINISH_REASON_INVALID_BATCH_INPUT,
    32 => LCPP_FINISH_REASON_UNHANDLED_EXCEPTION,
    64 => LCPP_FINISH_REASON_FATAL_ERROR,
    127 => LCPP_FINISH_REASON_UNSPECIFIED,
    _ => throw ArgumentError('Unknown value for lcpp_finish_reason: $value'),
  };

  @override
  String toString() {
    if (this == LCPP_FINISH_REASON_TIMEOUT)
      return "lcpp_finish_reason.LCPP_FINISH_REASON_TIMEOUT, lcpp_finish_reason.LCPP_FINISH_REASON_NO_KV_SLOT_AVAILABLE";
    return super.toString();
  }
}

enum lcpp_mirostat_type {
  /// disabled,
  LCPP_MIROSTAT_NONE(0),

  /// mirostat 1.0
  LCPP_MIROSTAT_V1(1),

  /// mirostat 2.0
  LCPP_MIROSTAT_V2(2);

  final int value;
  const lcpp_mirostat_type(this.value);

  static lcpp_mirostat_type fromValue(int value) => switch (value) {
    0 => LCPP_MIROSTAT_NONE,
    1 => LCPP_MIROSTAT_V1,
    2 => LCPP_MIROSTAT_V2,
    _ => throw ArgumentError('Unknown value for lcpp_mirostat_type: $value'),
  };
}

enum lcpp_model_family {
  LCPP_MODEL_FAMILY_LLAMA(0),
  LCPP_MODEL_FAMILY_QWEN(1),
  LCPP_MODEL_FAMILY_PHI(2),
  LCPP_MODEL_FAMILY_GEMMA(3),
  LCPP_MODEL_FAMILY_GRANITE(4),
  LCPP_MODEL_FAMILY_DEEPSEEK(5),
  LCPP_MODEL_FAMILY_MISTRAL(6),
  LCPP_MODEL_FAMILY_GPT_OSS(7),
  LCPP_MODEL_FAMILY_SEED_OSS(8),
  LCPP_MODEL_FAMILY_APERTUS(9),
  LCPP_MODEL_FAMILY_NEMOTRON(10),
  LCPP_MODEL_FAMILY_LIQUID(11),
  LCPP_MODEL_FAMILY_GLM(12),
  LCPP_MODEL_FAMILY_MINIMAX(13),
  LCPP_MODEL_FAMILY_COHERE(14),
  LCPP_MODEL_FAMILY_GENERIC(29),
  LCPP_MODEL_FAMILY_COUNT(30),
  LCPP_MODEL_FAMILY_UNKNOWN(31);

  static const LCPP_MODEL_FAMILY_UNSPECIFIED = LCPP_MODEL_FAMILY_COUNT;

  final int value;
  const lcpp_model_family(this.value);

  static lcpp_model_family fromValue(int value) => switch (value) {
    0 => LCPP_MODEL_FAMILY_LLAMA,
    1 => LCPP_MODEL_FAMILY_QWEN,
    2 => LCPP_MODEL_FAMILY_PHI,
    3 => LCPP_MODEL_FAMILY_GEMMA,
    4 => LCPP_MODEL_FAMILY_GRANITE,
    5 => LCPP_MODEL_FAMILY_DEEPSEEK,
    6 => LCPP_MODEL_FAMILY_MISTRAL,
    7 => LCPP_MODEL_FAMILY_GPT_OSS,
    8 => LCPP_MODEL_FAMILY_SEED_OSS,
    9 => LCPP_MODEL_FAMILY_APERTUS,
    10 => LCPP_MODEL_FAMILY_NEMOTRON,
    11 => LCPP_MODEL_FAMILY_LIQUID,
    12 => LCPP_MODEL_FAMILY_GLM,
    13 => LCPP_MODEL_FAMILY_MINIMAX,
    14 => LCPP_MODEL_FAMILY_COHERE,
    29 => LCPP_MODEL_FAMILY_GENERIC,
    30 => LCPP_MODEL_FAMILY_COUNT,
    31 => LCPP_MODEL_FAMILY_UNKNOWN,
    _ => throw ArgumentError('Unknown value for lcpp_model_family: $value'),
  };

  @override
  String toString() {
    if (this == LCPP_MODEL_FAMILY_COUNT)
      return "lcpp_model_family.LCPP_MODEL_FAMILY_COUNT, lcpp_model_family.LCPP_MODEL_FAMILY_UNSPECIFIED";
    return super.toString();
  }
}

/// from common.h
enum lcpp_common_sampler_type {
  LCPP_COMMON_SAMPLER_TYPE_NONE(0),
  LCPP_COMMON_SAMPLER_TYPE_DRY(1),
  LCPP_COMMON_SAMPLER_TYPE_TOP_K(2),
  LCPP_COMMON_SAMPLER_TYPE_TOP_P(3),
  LCPP_COMMON_SAMPLER_TYPE_MIN_P(4),

  /// LCPP_COMMON_SAMPLER_TYPE_TFS_Z       = 5,
  LCPP_COMMON_SAMPLER_TYPE_TYPICAL_P(6),
  LCPP_COMMON_SAMPLER_TYPE_TEMPERATURE(7),
  LCPP_COMMON_SAMPLER_TYPE_XTC(8),
  LCPP_COMMON_SAMPLER_TYPE_INFILL(9),
  LCPP_COMMON_SAMPLER_TYPE_PENALTIES(10),
  LCPP_COMMON_SAMPLER_TYPE_TOP_N_SIGMA(11);

  final int value;
  const lcpp_common_sampler_type(this.value);

  static lcpp_common_sampler_type fromValue(int value) => switch (value) {
    0 => LCPP_COMMON_SAMPLER_TYPE_NONE,
    1 => LCPP_COMMON_SAMPLER_TYPE_DRY,
    2 => LCPP_COMMON_SAMPLER_TYPE_TOP_K,
    3 => LCPP_COMMON_SAMPLER_TYPE_TOP_P,
    4 => LCPP_COMMON_SAMPLER_TYPE_MIN_P,
    6 => LCPP_COMMON_SAMPLER_TYPE_TYPICAL_P,
    7 => LCPP_COMMON_SAMPLER_TYPE_TEMPERATURE,
    8 => LCPP_COMMON_SAMPLER_TYPE_XTC,
    9 => LCPP_COMMON_SAMPLER_TYPE_INFILL,
    10 => LCPP_COMMON_SAMPLER_TYPE_PENALTIES,
    11 => LCPP_COMMON_SAMPLER_TYPE_TOP_N_SIGMA,
    _ =>
      throw ArgumentError('Unknown value for lcpp_common_sampler_type: $value'),
  };
}

final class lcpp_sampling_params extends ffi.Struct {
  /// <= 0.0 to sample greedily, 0.0 to not output probabilities
  @ffi.Float()
  external double temp;

  /// 0.0 = disabled
  @ffi.Float()
  external double dynatemp_range;

  /// controls how entropy maps to temperature in dynamic temperature sampler
  @ffi.Float()
  external double dynatemp_exponent;

  /// 1.0 = disabled
  @ffi.Float()
  external double top_p;

  /// 0.0 = disabled
  @ffi.Float()
  external double min_p;

  /// 0.0 = disabled
  @ffi.Float()
  external double xtc_probability;

  /// > 0.5 disables XTC
  @ffi.Float()
  external double xtc_threshold;

  /// typical_p, 1.0 = disabled
  @ffi.Float()
  external double typ_p;

  /// 1.0 = disabled
  @ffi.Float()
  external double penalty_repeat;

  /// 0.0 = disabled
  @ffi.Float()
  external double penalty_freq;

  /// 0.0 = disabled
  @ffi.Float()
  external double penalty_present;

  /// 0.0 = disabled;      DRY repetition penalty for tokens extending repetition:
  @ffi.Float()
  external double dry_multiplier;

  /// 0.0 = disabled;      multiplier * base ^ (length of sequence before token - allowed length)
  @ffi.Float()
  external double dry_base;

  /// -1.0 = disabled
  @ffi.Float()
  external double top_n_sigma;

  /// target entropy
  @ffi.Float()
  external double mirostat_tau;

  /// learning rate
  @ffi.Float()
  external double mirostat_eta;

  /// the seed used to initialize llama_sampler
  @ffi.Uint32()
  external int seed;

  /// number of previous tokens to remember
  @ffi.Int32()
  external int n_prev;

  /// if greater than 0, output the probabilities of top n_probs tokens.
  @ffi.Int32()
  external int n_probs;

  /// 0 = disabled, otherwise samplers should return at least min_keep tokens
  @ffi.Int32()
  external int min_keep;

  /// <= 0 to use vocab size
  @ffi.Int32()
  external int top_k;

  /// last n tokens to penalize (0 = disable penalty, -1 = context size)
  @ffi.Int32()
  external int penalty_last_n;

  /// tokens extending repetitions beyond this receive penalty
  @ffi.Int32()
  external int dry_allowed_length;

  /// how many tokens to scan for repetitions (0 = disable penalty, -1 = context size)
  @ffi.Int32()
  external int dry_penalty_last_n;

  @ffi.Int32()
  external int n_samplers;

  @ffi.Int32()
  external int n_grammar_length;

  /// 0 = disabled, 1 = mirostat, 2 = mirostat 2.0
  @ffi.Uint8()
  external int mirostatAsInt;

  lcpp_mirostat_type get mirostat =>
      lcpp_mirostat_type.fromValue(mirostatAsInt);

  @ffi.Bool()
  external bool ignore_eos;

  /// disable performance metrics
  @ffi.Bool()
  external bool no_perf;

  @ffi.Bool()
  external bool timing_per_token;

  @ffi.Bool()
  external bool grammar_lazy;

  external ffi.Pointer<ffi.Uint8> samplers;

  /// optional BNF-like grammar to constrain sampling
  external ffi.Pointer<ffi.Char> grammar;
}

typedef lcpp_sampling_params_t = lcpp_sampling_params;

/// sampling parameters
final class lcpp_params extends ffi.Struct {
  /// number of layers to store in VRAM
  @ffi.Int32()
  external int n_gpu_layers;

  /// the GPU that is used for the entire model when split_mode is LLAMA_SPLIT_MODE_NONE
  @ffi.Int32()
  external int main_gpu;

  @ffi.Int32()
  external int n_model_path_length;

  /// model family e.g. deepseek phi
  @ffi.Uint8()
  external int model_familyAsInt;

  lcpp_model_family get model_family =>
      lcpp_model_family.fromValue(model_familyAsInt);

  /// how to split the model across multiple GPUs
  @ffi.Uint8()
  external int split_modeAsInt;

  lcpp_split_mode get split_mode => lcpp_split_mode.fromValue(split_modeAsInt);

  /// NUMA strategy for multi-CPU systems
  @ffi.Uint8()
  external int numaAsInt;

  lcpp_numa_strategy get numa => lcpp_numa_strategy.fromValue(numaAsInt);

  /// only load the vocabulary, no weights
  @ffi.Bool()
  external bool vocab_only;

  /// use mmap if possible
  @ffi.Bool()
  external bool use_mmap;

  /// force system to keep model in RAM
  @ffi.Bool()
  external bool use_mlock;

  /// validate model tensor data
  @ffi.Bool()
  external bool check_tensors;

  /// escape "\n", "\r", "\t", "\'", "\"", and "\\"
  @ffi.Bool()
  external bool escape;

  /// reverse the usage of `\`
  @ffi.Bool()
  external bool multiline_input;

  /// loading reasoning model
  @ffi.Bool()
  external bool is_reasoning;

  /// mixture of experts offload to cpu
  @ffi.Bool()
  external bool offload_experts;

  /// path to GGUF model file
  external ffi.Pointer<ffi.Char> model_path;
}

typedef lcpp_params_t = lcpp_params;

final class lcpp_common_chat_tool_call extends ffi.Struct {
  external ffi.Pointer<ffi.Char> name;

  external ffi.Pointer<ffi.Char> arguments;

  external ffi.Pointer<ffi.Char> id;

  @ffi.Uint32()
  external int n_name;

  @ffi.Uint32()
  external int n_arguments;

  @ffi.Uint32()
  external int n_id;
}

typedef lcpp_common_chat_tool_call_t = lcpp_common_chat_tool_call;

final class lcpp_common_chat_msg_content_part extends ffi.Struct {
  external ffi.Pointer<ffi.Char> type;

  external ffi.Pointer<ffi.Char> text;

  @ffi.Uint32()
  external int n_type;

  @ffi.Uint32()
  external int n_text;
}

typedef lcpp_common_chat_msg_content_part_t = lcpp_common_chat_msg_content_part;

final class lcpp_common_chat_msg extends ffi.Struct {
  external ffi.Pointer<ffi.Char> role;

  external ffi.Pointer<ffi.Char> content;

  @ffi.Uint32()
  external int n_role;

  @ffi.Uint32()
  external int n_content;

  external ffi.Pointer<ffi.Pointer<lcpp_common_chat_msg_content_part_t>>
  content_parts;

  @ffi.Int32()
  external int n_content_parts;

  external ffi.Pointer<ffi.Pointer<lcpp_common_chat_tool_call_t>> tool_calls;

  @ffi.Int32()
  external int n_tool_calls;

  external ffi.Pointer<ffi.Char> reasoning_content;

  @ffi.Uint32()
  external int n_reasoning_content;

  external ffi.Pointer<ffi.Char> tool_name;

  @ffi.Uint32()
  external int n_tool_name;

  external ffi.Pointer<ffi.Char> tool_call_id;

  @ffi.Uint32()
  external int n_tool_call_id;
}

typedef lcpp_common_chat_msg_t = lcpp_common_chat_msg;

final class lcpp_common_chat_tool extends ffi.Struct {
  external ffi.Pointer<ffi.Char> name;

  external ffi.Pointer<ffi.Char> description;

  external ffi.Pointer<ffi.Char> paramaeters_schema;

  @ffi.Uint32()
  external int n_name;

  @ffi.Uint32()
  external int n_description;

  @ffi.Uint32()
  external int n_paramaeters_schema;
}

typedef lcpp_common_chat_tool_t = lcpp_common_chat_tool;

final class LcppTextStruct extends ffi.Struct {
  external ffi.Pointer<ffi.Char> text;

  @ffi.Int32()
  external int length;
}

typedef LcppTextStruct_t = LcppTextStruct;
typedef llama_context_params_t = llama_context_params;
typedef LppTokenStreamCallbackFunction =
    ffi.Void Function(ffi.Pointer<LcppTextStruct_t>);
typedef DartLppTokenStreamCallbackFunction =
    void Function(ffi.Pointer<LcppTextStruct_t>);
typedef LppTokenStreamCallback =
    ffi.Pointer<ffi.NativeFunction<LppTokenStreamCallbackFunction>>;
typedef LppChatMessageCallbackFunction =
    ffi.Void Function(ffi.Pointer<lcpp_common_chat_msg_t>);
typedef DartLppChatMessageCallbackFunction =
    void Function(ffi.Pointer<lcpp_common_chat_msg_t>);
typedef LppChatMessageCallback =
    ffi.Pointer<ffi.NativeFunction<LppChatMessageCallbackFunction>>;
typedef LppProgressCallbackFunction = ffi.Void Function(ffi.Double);
typedef DartLppProgressCallbackFunction = void Function(double);
typedef LppProgressCallback =
    ffi.Pointer<ffi.NativeFunction<LppProgressCallbackFunction>>;
typedef LcppOnCancelCallbackFunction = ffi.Void Function(ffi.Int32);
typedef DartLcppOnCancelCallbackFunction = void Function(int);
typedef LcppOnCancelCallback =
    ffi.Pointer<ffi.NativeFunction<LcppOnCancelCallbackFunction>>;
typedef LcppOnAbortCallbackFunction = ffi.Void Function(ffi.Int32);
typedef DartLcppOnAbortCallbackFunction = void Function(int);
typedef LcppOnAbortCallback =
    ffi.Pointer<ffi.NativeFunction<LcppOnAbortCallbackFunction>>;

final class lcpp_model_info extends ffi.Struct {
  /// required
  external ffi.Pointer<ffi.Char> architecture;

  @ffi.Uint32()
  external int n_architecture;

  @ffi.Uint32()
  external int quantization_version;

  @ffi.Uint32()
  external int alignment;

  @ffi.Uint32()
  external int gguf_version;

  @ffi.Int32()
  external int file_type;

  /// metadata
  external ffi.Pointer<ffi.Char> name;

  @ffi.Uint32()
  external int n_name;

  external ffi.Pointer<ffi.Char> author;

  @ffi.Uint32()
  external int n_author;

  external ffi.Pointer<ffi.Char> version;

  @ffi.Uint32()
  external int n_version;

  external ffi.Pointer<ffi.Char> organization;

  @ffi.Uint32()
  external int n_organization;

  external ffi.Pointer<ffi.Char> basename;

  @ffi.Uint32()
  external int n_basename;

  external ffi.Pointer<ffi.Char> finetune;

  @ffi.Uint32()
  external int n_finetune;

  external ffi.Pointer<ffi.Char> description;

  @ffi.Uint32()
  external int n_description;

  external ffi.Pointer<ffi.Char> size_label;

  @ffi.Uint32()
  external int n_size_label;

  external ffi.Pointer<ffi.Char> license;

  @ffi.Uint32()
  external int n_license;

  external ffi.Pointer<ffi.Char> license_link;

  @ffi.Uint32()
  external int n_license_link;

  external ffi.Pointer<ffi.Char> url;

  @ffi.Uint32()
  external int n_url;

  external ffi.Pointer<ffi.Char> doi;

  @ffi.Uint32()
  external int n_doi;

  external ffi.Pointer<ffi.Char> uuid;

  @ffi.Uint32()
  external int n_uuid;

  external ffi.Pointer<ffi.Char> repo_url;

  @ffi.Uint32()
  external int n_repo_url;

  /// n_ctx
  @ffi.Uint64()
  external int context_length;

  /// n_embd
  @ffi.Uint64()
  external int embedding_length;

  /// n_gpu_layers
  @ffi.Uint64()
  external int block_count;

  /// n_ff
  @ffi.Uint64()
  external int feed_forward_length;

  @ffi.Uint8()
  external int use_parallel_residual;

  @ffi.Uint32()
  external int expert_count;

  @ffi.Uint32()
  external int expert_used_count;

  /// n_head
  @ffi.Uint64()
  external int attention_head_count;

  /// set equal to n_head if model does not use GQA
  @ffi.Uint64()
  external int attention_head_count_kv;

  @ffi.Double()
  external double attention_max_alibi_bias;

  @ffi.Double()
  external double attention_clamp_kqv;

  @ffi.Double()
  external double attention_layer_norm_epsilon;

  @ffi.Double()
  external double attention_layer_norm_rms_epsilon;

  @ffi.Uint32()
  external int attention_key_length;

  @ffi.Uint32()
  external int attention_value_length;

  /// ROPE
  @ffi.Uint64()
  external int rope_dimension_count;

  @ffi.Double()
  external double rope_freq_base;

  external ffi.Pointer<ffi.Char> rope_scaling_type;

  @ffi.Uint32()
  external int n_rope_scaling_type;

  @ffi.Double()
  external double rope_scaling_factor;

  @ffi.Uint32()
  external int rope_original_context_length;

  @ffi.Uint8()
  external int rope_scaling_finetuned;

  /// Split
  @ffi.Uint64()
  external int split_count;

  @ffi.Uint64()
  external int split_tensor_count;
}

typedef lcpp_model_info_t = lcpp_model_info;

enum lcpp_cpu_endianess {
  LCPP_CPU_ENDIANESS_UNSPECIFIED(0),
  LCPP_CPU_ENDIANESS_BIG(1),
  LCPP_CPU_ENDIANESS_LITTLE(2),
  LCPP_CPU_ENDIANESS_UNKNOWN(3);

  final int value;
  const lcpp_cpu_endianess(this.value);

  static lcpp_cpu_endianess fromValue(int value) => switch (value) {
    0 => LCPP_CPU_ENDIANESS_UNSPECIFIED,
    1 => LCPP_CPU_ENDIANESS_BIG,
    2 => LCPP_CPU_ENDIANESS_LITTLE,
    3 => LCPP_CPU_ENDIANESS_UNKNOWN,
    _ => throw ArgumentError('Unknown value for lcpp_cpu_endianess: $value'),
  };
}

final class lcpp_cpu_info extends ffi.Struct {
  external ffi.Pointer<ffi.Char> vendor_id;

  @ffi.Int32()
  external int n_vendor_id;

  external ffi.Pointer<ffi.Char> processor_name;

  @ffi.Int32()
  external int n_processor_name;

  external ffi.Pointer<ffi.Char> chipset_vendor;

  @ffi.Int32()
  external int n_chipset_vendor;

  external ffi.Pointer<ffi.Char> uarch;

  @ffi.Int32()
  external int n_uarch;

  @ffi.Uint8()
  external int endianessAsInt;

  lcpp_cpu_endianess get endianess =>
      lcpp_cpu_endianess.fromValue(endianessAsInt);

  @ffi.Uint64()
  external int frequency;

  @ffi.Uint32()
  external int num_cores;

  @ffi.Uint32()
  external int num_processors;

  @ffi.Uint32()
  external int num_clusters;
}

typedef lcpp_cpu_info_t = lcpp_cpu_info;

final class lcpp_memory_info extends ffi.Struct {
  @ffi.Uint64()
  external int physical_mem;

  @ffi.Uint64()
  external int virtual_mem;
}

typedef lcpp_memory_info_t = lcpp_memory_info;

final class lcpp_gpu_info extends ffi.Struct {
  external ffi.Pointer<ffi.Char> vendor;

  @ffi.Int32()
  external int n_vendor;

  external ffi.Pointer<ffi.Char> device_name;

  @ffi.Int32()
  external int n_device_name;

  @ffi.Int32()
  external int index;

  @ffi.Uint8()
  external int type;

  @ffi.Uint64()
  external int memory;

  @ffi.Int32()
  external int frequency;
}

typedef lcpp_gpu_info_t = lcpp_gpu_info;

final class lcpp_system_info extends ffi.Struct {
  external ffi.Pointer<ffi.Char> os_name;

  @ffi.Int32()
  external int n_os_name;

  external ffi.Pointer<ffi.Char> os_version;

  @ffi.Int32()
  external int n_os_version;

  external ffi.Pointer<ffi.Char> full_name;

  @ffi.Int32()
  external int n_full_name;
}

typedef lcpp_system_info_t = lcpp_system_info;

final class lcpp_machine_info extends ffi.Struct {
  external ffi.Pointer<lcpp_system_info_t> sysinfo;

  external ffi.Pointer<lcpp_cpu_info_t> cpuinfo;

  external ffi.Pointer<lcpp_memory_info_t> meminfo;

  external ffi.Pointer<ffi.Pointer<lcpp_gpu_info_t>> gpuinfo;

  @ffi.Int32()
  external int n_gpuinfo;

  @ffi.Uint64()
  external int total_vram;

  @ffi.Uint64()
  external int blkmax_vram;
}

typedef lcpp_machine_info_t = lcpp_machine_info;

final class lcpp_model_filepath extends ffi.Struct {
  external ffi.Pointer<ffi.Char> directory;

  @ffi.Int32()
  external int n_directory;

  external ffi.Pointer<ffi.Char> basename;

  @ffi.Int32()
  external int n_basename;

  external ffi.Pointer<ffi.Char> file_ext;

  @ffi.Int32()
  external int n_file_ext;

  @ffi.Uint8()
  external int is_sharded;

  @ffi.Int32()
  external int n_shards;
}

typedef lcpp_model_filepath_t = lcpp_model_filepath;

final class lcpp_model_mem extends ffi.Struct {
  @ffi.Size()
  external int mem_model;

  @ffi.Size()
  external int tensor_mem;

  @ffi.Size()
  external int mem_experts;

  @ffi.Size()
  external int mem_context;

  @ffi.Size()
  external int mem_attention;

  @ffi.Size()
  external int mem_kv_cache;
}

typedef lcpp_model_mem_t = lcpp_model_mem;

final class lcpp_model_rt extends ffi.Struct {
  external ffi.Pointer<lcpp_model_mem_t> memory;

  external ffi.Pointer<lcpp_model_info_t> info;
}

typedef lcpp_model_rt_t = lcpp_model_rt;

/// types that can be stored as GGUF KV data
enum gguf_type {
  GGUF_TYPE_UINT8(0),
  GGUF_TYPE_INT8(1),
  GGUF_TYPE_UINT16(2),
  GGUF_TYPE_INT16(3),
  GGUF_TYPE_UINT32(4),
  GGUF_TYPE_INT32(5),
  GGUF_TYPE_FLOAT32(6),
  GGUF_TYPE_BOOL(7),
  GGUF_TYPE_STRING(8),
  GGUF_TYPE_ARRAY(9),
  GGUF_TYPE_UINT64(10),
  GGUF_TYPE_INT64(11),
  GGUF_TYPE_FLOAT64(12),

  /// marks the end of the enum
  GGUF_TYPE_COUNT(13);

  final int value;
  const gguf_type(this.value);

  static gguf_type fromValue(int value) => switch (value) {
    0 => GGUF_TYPE_UINT8,
    1 => GGUF_TYPE_INT8,
    2 => GGUF_TYPE_UINT16,
    3 => GGUF_TYPE_INT16,
    4 => GGUF_TYPE_UINT32,
    5 => GGUF_TYPE_INT32,
    6 => GGUF_TYPE_FLOAT32,
    7 => GGUF_TYPE_BOOL,
    8 => GGUF_TYPE_STRING,
    9 => GGUF_TYPE_ARRAY,
    10 => GGUF_TYPE_UINT64,
    11 => GGUF_TYPE_INT64,
    12 => GGUF_TYPE_FLOAT64,
    13 => GGUF_TYPE_COUNT,
    _ => throw ArgumentError('Unknown value for gguf_type: $value'),
  };
}

final class gguf_context extends ffi.Opaque {}

final class gguf_init_params extends ffi.Struct {
  @ffi.Bool()
  external bool no_alloc;

  /// if not NULL, create a ggml_context and allocate the tensor data in it
  external ffi.Pointer<ffi.Pointer<ggml_context>> ctx;
}

const int _VCRT_COMPILER_PREPROCESSOR = 1;

const int _SAL_VERSION = 20;

const int __SAL_H_VERSION = 180000000;

const int _USE_DECLSPECS_FOR_SAL = 0;

const int _USE_ATTRIBUTES_FOR_SAL = 0;

const int _CRT_PACKING = 8;

const int _VCRUNTIME_DISABLED_WARNINGS = 4514;

const int _HAS_EXCEPTIONS = 1;

const int _WCHAR_T_DEFINED = 1;

const int NULL = 0;

const int _HAS_CXX17 = 0;

const int _HAS_CXX20 = 0;

const int _HAS_CXX23 = 0;

const int _HAS_CXX26 = 0;

const int _HAS_NODISCARD = 1;

const int INT8_MIN = -128;

const int INT16_MIN = -32768;

const int INT32_MIN = -2147483648;

const int INT64_MIN = -9223372036854775808;

const int INT8_MAX = 127;

const int INT16_MAX = 32767;

const int INT32_MAX = 2147483647;

const int INT64_MAX = 9223372036854775807;

const int UINT8_MAX = 255;

const int UINT16_MAX = 65535;

const int UINT32_MAX = 4294967295;

const int UINT64_MAX = -1;

const int INT_LEAST8_MIN = -128;

const int INT_LEAST16_MIN = -32768;

const int INT_LEAST32_MIN = -2147483648;

const int INT_LEAST64_MIN = -9223372036854775808;

const int INT_LEAST8_MAX = 127;

const int INT_LEAST16_MAX = 32767;

const int INT_LEAST32_MAX = 2147483647;

const int INT_LEAST64_MAX = 9223372036854775807;

const int UINT_LEAST8_MAX = 255;

const int UINT_LEAST16_MAX = 65535;

const int UINT_LEAST32_MAX = 4294967295;

const int UINT_LEAST64_MAX = -1;

const int INT_FAST8_MIN = -128;

const int INT_FAST16_MIN = -2147483648;

const int INT_FAST32_MIN = -2147483648;

const int INT_FAST64_MIN = -9223372036854775808;

const int INT_FAST8_MAX = 127;

const int INT_FAST16_MAX = 2147483647;

const int INT_FAST32_MAX = 2147483647;

const int INT_FAST64_MAX = 9223372036854775807;

const int UINT_FAST8_MAX = 255;

const int UINT_FAST16_MAX = 4294967295;

const int UINT_FAST32_MAX = 4294967295;

const int UINT_FAST64_MAX = -1;

const int INTPTR_MIN = -9223372036854775808;

const int INTPTR_MAX = 9223372036854775807;

const int UINTPTR_MAX = -1;

const int INTMAX_MIN = -9223372036854775808;

const int INTMAX_MAX = 9223372036854775807;

const int UINTMAX_MAX = -1;

const int PTRDIFF_MIN = -9223372036854775808;

const int PTRDIFF_MAX = 9223372036854775807;

const int SIZE_MAX = -1;

const int SIG_ATOMIC_MIN = -2147483648;

const int SIG_ATOMIC_MAX = 2147483647;

const int WCHAR_MIN = 0;

const int WCHAR_MAX = 65535;

const int WINT_MIN = 0;

const int WINT_MAX = 65535;

const int __bool_true_false_are_defined = 1;

const int false$ = 0;

const int true$ = 1;

const int _WIN32_WINNT = 2560;

const int _ARM_WINAPI_PARTITION_DESKTOP_SDK_AVAILABLE = 1;

const int _CRT_BUILD_DESKTOP_APP = 1;

const int _UCRT_DISABLED_WARNINGS = 4324;

const int _ARGMAX = 100;

const int _TRUNCATE = -1;

const int _CRT_INT_MAX = 2147483647;

const int _CRT_SIZE_MAX = -1;

const String __FILEW__ = 'C';

const int _CRT_FUNCTIONS_REQUIRED = 1;

const int _CRT_HAS_CXX17 = 0;

const int _CRT_HAS_C11 = 0;

const int _CRT_INTERNAL_NONSTDC_NAMES = 1;

const int __STDC_SECURE_LIB__ = 200411;

const int __GOT_SECURE_LIB__ = 200411;

const int __STDC_WANT_SECURE_LIB__ = 1;

const int _SECURECRT_FILL_BUFFER_PATTERN = 254;

const int _CRT_SECURE_CPP_OVERLOAD_STANDARD_NAMES = 0;

const int _CRT_SECURE_CPP_OVERLOAD_STANDARD_NAMES_COUNT = 0;

const int _CRT_SECURE_CPP_OVERLOAD_SECURE_NAMES = 1;

const int _CRT_SECURE_CPP_OVERLOAD_STANDARD_NAMES_MEMORY = 0;

const int _CRT_SECURE_CPP_OVERLOAD_SECURE_NAMES_MEMORY = 0;

const int _STATIC_INLINE_UCRT_FUNCTIONS = 1;

const String _CRT_INTERNAL_STDIO_SYMBOL_PREFIX = '';

const int _CRT_INTERNAL_PRINTF_LEGACY_VSPRINTF_NULL_TERMINATION = 1;

const int _CRT_INTERNAL_PRINTF_STANDARD_SNPRINTF_BEHAVIOR = 2;

const int _CRT_INTERNAL_PRINTF_LEGACY_WIDE_SPECIFIERS = 4;

const int _CRT_INTERNAL_PRINTF_LEGACY_MSVCRT_COMPATIBILITY = 8;

const int _CRT_INTERNAL_PRINTF_LEGACY_THREE_DIGIT_EXPONENTS = 16;

const int _CRT_INTERNAL_PRINTF_STANDARD_ROUNDING = 32;

const int _CRT_INTERNAL_SCANF_SECURECRT = 1;

const int _CRT_INTERNAL_SCANF_LEGACY_WIDE_SPECIFIERS = 2;

const int _CRT_INTERNAL_SCANF_LEGACY_MSVCRT_COMPATIBILITY = 4;

const int WEOF = 65535;

const int BUFSIZ = 512;

const int _NFILE = 512;

const int _NSTREAM_ = 512;

const int _IOB_ENTRIES = 3;

const int EOF = -1;

const int _IOFBF = 0;

const int _IOLBF = 64;

const int _IONBF = 4;

const int L_tmpnam = 260;

const int L_tmpnam_s = 260;

const int SEEK_CUR = 1;

const int SEEK_END = 2;

const int SEEK_SET = 0;

const int FILENAME_MAX = 260;

const int FOPEN_MAX = 20;

const int _SYS_OPEN = 20;

const int TMP_MAX = 2147483647;

const int TMP_MAX_S = 2147483647;

const int _TMP_MAX_S = 2147483647;

const int SYS_OPEN = 20;

const int GGML_FILE_MAGIC = 1734831468;

const int GGML_FILE_VERSION = 2;

const int GGML_QNT_VERSION = 2;

const int GGML_QNT_VERSION_FACTOR = 1000;

const int GGML_MAX_DIMS = 4;

const int GGML_MAX_PARAMS = 2048;

const int GGML_MAX_SRC = 10;

const int GGML_MAX_N_THREADS = 512;

const int GGML_MAX_OP_PARAMS = 64;

const int GGML_MAX_NAME = 64;

const int GGML_DEFAULT_N_THREADS = 4;

const int GGML_DEFAULT_GRAPH_SIZE = 2048;

const int GGML_MEM_ALIGN = 16;

const int GGML_EXIT_SUCCESS = 0;

const int GGML_EXIT_ABORTED = 1;

const int GGML_ROPE_TYPE_NORMAL = 0;

const int GGML_ROPE_TYPE_NEOX = 2;

const int GGML_ROPE_TYPE_MROPE = 8;

const int GGML_ROPE_TYPE_VISION = 24;

const int GGML_ROPE_TYPE_IMROPE = 40;

const int GGML_MROPE_SECTIONS = 4;

const int GGML_N_TASKS_MAX = -1;

const int LLAMA_DEFAULT_SEED = 4294967295;

const int LLAMA_TOKEN_NULL = -1;

const int LLAMA_FILE_MAGIC_GGLA = 1734831201;

const int LLAMA_FILE_MAGIC_GGSN = 1734833006;

const int LLAMA_FILE_MAGIC_GGSQ = 1734833009;

const int LLAMA_SESSION_MAGIC = 1734833006;

const int LLAMA_SESSION_VERSION = 9;

const int LLAMA_STATE_SEQ_MAGIC = 1734833009;

const int LLAMA_STATE_SEQ_VERSION = 2;

const int LLAMA_STATE_SEQ_FLAGS_SWA_ONLY = 1;

const int LLAMA_STATE_SEQ_FLAGS_PARTIAL_ONLY = 1;

const String GGUF_MAGIC = 'GGUF';

const int GGUF_VERSION = 3;

const String GGUF_KEY_GENERAL_ALIGNMENT = 'general.alignment';

const int GGUF_DEFAULT_ALIGNMENT = 32;
